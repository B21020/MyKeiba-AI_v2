{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. モジュールインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from modules.constants import Master\n",
    "from modules.constants import LocalPaths\n",
    "from modules.constants import HorseResultsCols\n",
    "from modules.constants import ResultsCols\n",
    "from modules import preparing\n",
    "from modules import preprocessing\n",
    "from modules import training\n",
    "from modules import simulation\n",
    "from modules import policies\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エンコーディング設定の確認\n",
    "import sys\n",
    "import locale\n",
    "\n",
    "print(\"=== エンコーディング設定確認 ===\")\n",
    "print(f\"sys.getdefaultencoding(): {sys.getdefaultencoding()}\")\n",
    "print(f\"sys.getfilesystemencoding(): {sys.getfilesystemencoding()}\")\n",
    "print(f\"locale.getpreferredencoding(): {locale.getpreferredencoding()}\")\n",
    "print(f\"PYTHONUTF8 環境変数: {sys.flags.utf8_mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "標準的な土日競馬開催時の運用スケジュールを以下の表の通り。\n",
    "\n",
    "|曜日|時刻|内容|実行する main.ipynb の項番|備考|\n",
    "|:-:|:--|:--|:--|:--|\n",
    "|月|||||\n",
    "|火|||||\n",
    "|水|16:30過ぎ|先週土日の馬の過去成績ページ確定<BR>（netkeiba.comﾌﾟﾚﾐｱｻｰﾋﾞｽのﾀｲﾑ指数・ﾚｰｽ分析・注目馬 ﾚｰｽ後の短評情報確定）|2. データ取得 ～ 5. シミュレーション|3日間開催の場合も、水曜日|\n",
    "|木|||||\n",
    "|金|10:05過ぎ<BR>19:25過ぎ|土曜の出馬表確定<BR>土曜の天候・馬場状態更新|6.1. 前日準備 ～ 6.2. 前日全レース予想（天候・馬場状態は手動設定）<BR>6.1. 前日準備 ～ 6.2. 前日全レース予想||\n",
    "|土|09:00～17:00<BR>10:05過ぎ<BR>19:25過ぎ| レース時刻<BR>日曜の出馬表確定<BR>日曜の天候・馬場状態更新|6.3. レース直前データ処理（当日レース予想）<BR>6.1. 前日準備 ～ 6.2. 前日全レース予想（天候・馬場状態は手動設定）<BR>6.1. 前日準備 ～ 6.2. 前日全レース予想||\n",
    "|日|09:00～17:00|レース時刻|6.3. レース直前データ処理（当日レース予想）||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. データ取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. レースID取得\n",
    "例として、2020年のレースデータを取得する場合を考える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#開催日取得。to_の月は含まないので注意。\n",
    "kaisai_date_2025 = preparing.scrape_kaisai_date(from_=\"2025-12-01\", to_=\"2026-01-01\")\n",
    "len(kaisai_date_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開催日からレースIDの取得\n",
    "race_id_list = preparing.scrape_race_id_list(kaisai_date_2025)\n",
    "len(race_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. /race/ディレクトリのデータ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://db.netkeiba.com/race/のhtml(binファイル)をスクレイピングして保存\n",
    "html_files_race = preparing.scrape_html_race(race_id_list, skip=False)\n",
    "html_files_race[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/html/race/に保存されているhtml(binファイル)をリストにする\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# LocalPathsからHTMLレースディレクトリを取得\n",
    "race_html_dir = LocalPaths.HTML_RACE_DIR\n",
    "print(f\"レースHTMLディレクトリ: {race_html_dir}\")\n",
    "\n",
    "# globでbinファイルを検索\n",
    "html_files_race = glob.glob(os.path.join(race_html_dir, \"*.bin\"))\n",
    "print(f\"見つかったHTMLファイル数: {len(html_files_race)}\")\n",
    "\n",
    "# 最初の5ファイルを表示\n",
    "html_files_race[:5]\n",
    "html_files_race[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数確認\n",
    "print(f\"html_files_race変数の状態:\")\n",
    "print(f\"  タイプ: {type(html_files_race)}\")\n",
    "print(f\"  サイズ: {len(html_files_race)}\")\n",
    "print(f\"  範囲: {html_files_race[0].split('\\\\')[-1]} ～ {html_files_race[-1].split('\\\\')[-1]}\")\n",
    "\n",
    "# これで次のセルでget_rawdata_results関数を正常に実行できます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_new = preparing.get_rawdata_results(html_files_race) #レース結果テーブルの作成\n",
    "race_info_new = preparing.get_rawdata_info(html_files_race) #レース情報テーブルの作成\n",
    "return_tables_new = preparing.get_rawdata_return(html_files_race) #払戻テーブルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルの更新。元々のテーブルが存在しない場合は、新たに作成される。\n",
    "preparing.update_rawdata(filepath=LocalPaths.RAW_RESULTS_PATH, new_df=results_new)\n",
    "preparing.update_rawdata(filepath=LocalPaths.RAW_RACE_INFO_PATH, new_df=race_info_new)\n",
    "preparing.update_rawdata(filepath=LocalPaths.RAW_RETURN_TABLES_PATH, new_df=return_tables_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.x. 生成済み raw テーブル確認\n",
    "`data/raw` に保存された各pickleの基本情報を表示します。存在しない場合はスキップします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/raw 配下の pickle テーブル概要確認 + null率集計\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from modules.constants import LocalPaths\n",
    "\n",
    "RAW_DIR = pathlib.Path('data/raw')\n",
    "\n",
    "if not RAW_DIR.exists():\n",
    "    print(f'ディレクトリが存在しません: {RAW_DIR.resolve()}')\n",
    "else:\n",
    "    pickle_files = sorted(RAW_DIR.glob('*.pickle'))\n",
    "    if not pickle_files:\n",
    "        print('pickleファイルが見つかりません。先に取得処理を実行してください。')\n",
    "    else:\n",
    "        summaries = []\n",
    "        null_detail_rows = []  # 列単位 null 率詳細\n",
    "        for p in pickle_files:\n",
    "            info = {\n",
    "                'file': p.name,\n",
    "                'size_MB': round(p.stat().st_size / 1_000_000, 3)\n",
    "            }\n",
    "            try:\n",
    "                df = pd.read_pickle(p)\n",
    "                info['rows'] = len(df)\n",
    "                info['cols'] = df.shape[1]\n",
    "                info['memory_MB'] = round(df.memory_usage(deep=True).sum() / 1_000_000, 3)\n",
    "                # 代表的なカラムサンプル（最大5件）\n",
    "                info['sample_cols'] = ', '.join(list(df.columns[:5]))\n",
    "                # 日付らしき列から範囲を取得\n",
    "                date_cols = [c for c in df.columns if 'date' in c.lower()]\n",
    "                date_range = ''\n",
    "                for dc in date_cols:\n",
    "                    try:\n",
    "                        s = pd.to_datetime(df[dc], errors='coerce')\n",
    "                        if s.notna().any():\n",
    "                            date_range = f\"{dc}:{s.min().date()}→{s.max().date()}\"\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                info['date_range'] = date_range\n",
    "                # 全体 null 率（セル全体）\n",
    "                total_cells = df.shape[0] * (df.shape[1] if df.shape[0] else 0)\n",
    "                info['overall_null_pct'] = round((df.isna().sum().sum() / total_cells) * 100, 2) if total_cells else 0.0\n",
    "                # 列ごとの null 率\n",
    "                col_null_pct = (df.isna().mean() * 100).sort_values(ascending=False)\n",
    "                # 上位10列を詳細に保存（列が10未満なら全て）\n",
    "                for col, pct in col_null_pct.head(10).items():\n",
    "                    null_detail_rows.append({\n",
    "                        'file': p.name,\n",
    "                        'column': col,\n",
    "                        'null_pct': round(pct, 2)\n",
    "                    })\n",
    "                # 列単位統計（最大値/平均値/中央値）\n",
    "                info['max_col_null_pct'] = round(col_null_pct.iloc[0], 2) if not col_null_pct.empty else 0.0\n",
    "                info['mean_col_null_pct'] = round(col_null_pct.mean(), 2) if not col_null_pct.empty else 0.0\n",
    "                info['median_col_null_pct'] = round(col_null_pct.median(), 2) if not col_null_pct.empty else 0.0\n",
    "            except Exception as e:\n",
    "                info['rows'] = 'ERR'\n",
    "                info['cols'] = 'ERR'\n",
    "                info['memory_MB'] = 'ERR'\n",
    "                info['sample_cols'] = f'load error: {e.__class__.__name__}'\n",
    "                info['date_range'] = ''\n",
    "                info['overall_null_pct'] = 'ERR'\n",
    "                info['max_col_null_pct'] = 'ERR'\n",
    "                info['mean_col_null_pct'] = 'ERR'\n",
    "                info['median_col_null_pct'] = 'ERR'\n",
    "            summaries.append(info)\n",
    "        summary_df = pd.DataFrame(summaries)\n",
    "        # 表示順を調整\n",
    "        summary_cols_order = [\n",
    "            'file','rows','cols','size_MB','memory_MB','overall_null_pct',\n",
    "            'max_col_null_pct','mean_col_null_pct','median_col_null_pct',\n",
    "            'sample_cols','date_range'\n",
    "        ]\n",
    "        summary_df = summary_df[summary_cols_order]\n",
    "        display(summary_df)\n",
    "\n",
    "        if null_detail_rows:\n",
    "            null_detail_df = pd.DataFrame(null_detail_rows)\n",
    "            # ファイル毎に null の高い列を横持ち要約（pivot）するオプション（必要であれば）\n",
    "            display(null_detail_df)\n",
    "\n",
    "        # 主要パスが指すファイルの存在と行数確認（存在しない場合も出力）\n",
    "        main_paths = {\n",
    "            'RAW_RESULTS_PATH': getattr(LocalPaths, 'RAW_RESULTS_PATH', None),\n",
    "            'RAW_RACE_INFO_PATH': getattr(LocalPaths, 'RAW_RACE_INFO_PATH', None),\n",
    "            'RAW_RETURN_TABLES_PATH': getattr(LocalPaths, 'RAW_RETURN_TABLES_PATH', None),\n",
    "            'RAW_HORSE_INFO_PATH': getattr(LocalPaths, 'RAW_HORSE_INFO_PATH', None),\n",
    "            'RAW_HORSE_RESULTS_PATH': getattr(LocalPaths, 'RAW_HORSE_RESULTS_PATH', None),\n",
    "            'RAW_PEDS_PATH': getattr(LocalPaths, 'RAW_PEDS_PATH', None)\n",
    "        }\n",
    "        path_rows = []\n",
    "        for key, path in main_paths.items():\n",
    "            if path is None:\n",
    "                path_rows.append({'name': key, 'path': None, 'exists': False, 'rows': None})\n",
    "                continue\n",
    "            exists = os.path.isfile(path)\n",
    "            rows = None\n",
    "            if exists:\n",
    "                try:\n",
    "                    rows = len(pd.read_pickle(path))\n",
    "                except Exception:\n",
    "                    rows = 'ERR'\n",
    "            path_rows.append({'name': key, 'path': path, 'exists': exists, 'rows': rows})\n",
    "        display(pd.DataFrame(path_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 既存のresultsデータを読み込んでテスト用horse_idリストを取得\n",
    "results_new = pd.read_pickle(LocalPaths.RAW_RESULTS_PATH)\n",
    "print(f\"results_new loaded: {results_new.shape}\")\n",
    "\n",
    "# 先頭10頭のテスト用リスト作成\n",
    "horse_id_list = results_new['horse_id'].unique()\n",
    "horse_id_test_list = horse_id_list[:10]\n",
    "print(f\"テスト用horse_id: {horse_id_test_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. /horse/ディレクトリのデータ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# マスターファイルにNaN値が含まれているものだけ再スクレイピングする\n",
    "import pandas as pd\n",
    "import os\n",
    "from modules.constants import LocalPaths\n",
    "\n",
    "# マスターファイルからNaN値を持つhorse_idを特定\n",
    "master_files = {\n",
    "    'horse_id': 'horse_id.csv',\n",
    "    'jockey_id': 'jockey_id.csv', \n",
    "    'trainer_id': 'trainer_id.csv',\n",
    "    'owner_id': 'owner_id.csv',\n",
    "    'breeder_id': 'breeder_id.csv'\n",
    "}\n",
    "\n",
    "nan_horse_ids = set()\n",
    "print(\"=== マスターファイルのNaN値チェック ===\")\n",
    "\n",
    "# horse_info.pickleを読み込み\n",
    "try:\n",
    "    horse_info = pd.read_pickle(os.path.join(LocalPaths.RAW_DIR, 'horse_info.pickle'))\n",
    "    print(f\"horse_info.pickle読み込み完了: {len(horse_info)}頭の馬データ\")\n",
    "except Exception as e:\n",
    "    print(f\"horse_info.pickleの読み込みエラー: {e}\")\n",
    "    horse_info = None\n",
    "\n",
    "# 各マスターファイルをチェック\n",
    "for master_type, filename in master_files.items():\n",
    "    filepath = os.path.join(LocalPaths.MASTER_DIR, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "        id_col = df.columns[0]\n",
    "        \n",
    "        # NaN値を持つ行を特定\n",
    "        nan_rows = df[df[id_col].isna()]\n",
    "        if len(nan_rows) > 0:\n",
    "            print(f\"\\n{master_type}: {len(nan_rows)}個のNaN値を発見\")\n",
    "            \n",
    "            if horse_info is not None:\n",
    "                if master_type == 'horse_id':\n",
    "                    # horse_idが直接NaNの場合（これは通常起こらない）\n",
    "                    print(f\"  -> horse_idが直接NaNになっている行: {len(nan_rows)}\")\n",
    "                else:\n",
    "                    # 他のIDがNaNの馬を特定\n",
    "                    col_mapping = {\n",
    "                        'jockey_id': '騎手',\n",
    "                        'trainer_id': '調教師', \n",
    "                        'owner_id': '馬主',\n",
    "                        'breeder_id': '生産者'\n",
    "                    }\n",
    "                    if master_type in col_mapping:\n",
    "                        target_col = col_mapping[master_type]\n",
    "                        if target_col in horse_info.columns:\n",
    "                            # 該当する列がNaNまたは空文字の馬を特定\n",
    "                            nan_horses = horse_info[\n",
    "                                (horse_info[target_col].isna()) | \n",
    "                                (horse_info[target_col] == '') |\n",
    "                                (horse_info[target_col] == 'nan')\n",
    "                            ]\n",
    "                            for horse_id in nan_horses.index:\n",
    "                                nan_horse_ids.add(horse_id)\n",
    "                            print(f\"  -> {target_col}がNaN/空の馬: {len(nan_horses)}頭\")\n",
    "                            if len(nan_horses) > 0:\n",
    "                                print(f\"      例: {list(nan_horses.index)[:5]}\")\n",
    "    else:\n",
    "        print(f\"{master_type}: ファイルが存在しません\")\n",
    "\n",
    "print(f\"\\n=== 再スクレイピング対象の特定結果 ===\")\n",
    "print(f\"再スクレイピングが必要な馬ID数: {len(nan_horse_ids)}\")\n",
    "\n",
    "if len(nan_horse_ids) > 0:\n",
    "    nan_horse_ids_list = sorted(list(nan_horse_ids))\n",
    "    print(f\"対象馬ID例: {nan_horse_ids_list[:10]}{'...' if len(nan_horse_ids_list) > 10 else ''}\")\n",
    "    \n",
    "    print(f\"\\n=== 再スクレイピング実行オプション ===\")\n",
    "    print(\"以下の変数を設定して次のセルで実行してください：\")\n",
    "    print(\"re_scrape_horses = True  # この行のコメントアウトを外して実行\")\n",
    "    print(\"target_horse_ids = nan_horse_ids_list  # 対象馬IDリスト\")\n",
    "    \n",
    "    # 変数を保存（次のセルで使用）\n",
    "    globals()['nan_horse_ids_list'] = nan_horse_ids_list\n",
    "    globals()['re_scrape_needed'] = True\n",
    "else:\n",
    "    print(\"再スクレイピングが必要な馬は見つかりませんでした。\")\n",
    "    globals()['re_scrape_needed'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horse_id_listのうち先頭10頭の馬のリストを作成し、スクレイピングテストする\n",
    "horse_id_list = results_new['horse_id'].unique()\n",
    "horse_id_test_list = horse_id_list[:10]  # 先頭10頭でテスト\n",
    "\n",
    "print(f\"全体の馬数: {len(horse_id_list)}\")\n",
    "print(f\"テスト対象の馬数: {len(horse_id_test_list)}\")\n",
    "print(f\"テスト対象馬ID: {horse_id_test_list}\")\n",
    "\n",
    "#htmlをスクレイピング\n",
    "#すでにスクレイピングしてある馬をスキップしたい場合はskip=Trueにする\n",
    "#すでにスクレイピングしてある馬でも、新たに出走した成績を更新したい場合はskip=Falseにする\n",
    "html_files_horse = preparing.scrape_html_horse_with_master(\n",
    "    horse_id_test_list, skip=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_id_list = results_new['horse_id'].unique()\n",
    "#htmlをスクレイピング\n",
    "#すでにスクレイピングしてある馬をスキップしたい場合はskip=Trueにする\n",
    "#すでにスクレイピングしてある馬でも、新たに出走した成績を更新したい場合はskip=Falseにする\n",
    "html_files_horse = preparing.scrape_html_horse_with_master(\n",
    "    horse_id_list, skip=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#追加で新たにスクレイピングされた数\n",
    "len(html_files_horse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### scrape関数を実行せずに、保存してあるhtmlのパスを取得する場合、以下を実行 ###\n",
    "\n",
    "target_date = '2025-09-20' #スクレイピングした日付を指定\n",
    "# マスタの読み込み\n",
    "update_master = pd.read_csv(\n",
    "    LocalPaths.MASTER_RAW_HORSE_RESULTS_PATH,\n",
    "    dtype=object\n",
    "    )\n",
    "# target_dateにスクレイピングしたhorse_idに絞り込む\n",
    "filter = pd.to_datetime(update_master['updated_at']).dt.strftime('%Y-%m-%d') == target_date\n",
    "horse_id_list = update_master[filter]['horse_id']\n",
    "\n",
    "# binファイルのパスを取得\n",
    "html_files_horse = []\n",
    "for horse_id in tqdm(horse_id_list):\n",
    "    file = glob.glob(os.path.join(LocalPaths.HTML_HORSE_DIR, horse_id+'*.bin'))[0]\n",
    "    html_files_horse.append(file)\n",
    "html_files_horse[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# マスターファイルにNaN値が含まれているものだけ再スクレイピングする\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再スクレイピング実行（上のセルで特定されたNaN馬のみ）\n",
    "# 実行する場合は下の行のコメントアウトを外してください\n",
    "# re_scrape_horses = True\n",
    "\n",
    "if 're_scrape_horses' in locals() and re_scrape_horses and 're_scrape_needed' in globals() and re_scrape_needed:\n",
    "    print(\"=== NaN値を持つ馬の再スクレイピング開始 ===\")\n",
    "    print(f\"対象馬数: {len(nan_horse_ids_list)}\")\n",
    "    \n",
    "    # 進捗表示用\n",
    "    from tqdm import tqdm\n",
    "    import time\n",
    "    \n",
    "    success_count = 0\n",
    "    failed_ids = []\n",
    "    \n",
    "    for i, horse_id in enumerate(tqdm(nan_horse_ids_list, desc=\"再スクレイピング\")):\n",
    "        try:\n",
    "            # 馬情報をスクレイピング\n",
    "            scrape_horse_html(horse_id)\n",
    "            success_count += 1\n",
    "            \n",
    "            # サーバー負荷軽減のため少し待機\n",
    "            if i % 10 == 0 and i > 0:\n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"エラー - 馬ID {horse_id}: {e}\")\n",
    "            failed_ids.append(horse_id)\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n=== 再スクレイピング完了 ===\")\n",
    "    print(f\"成功: {success_count}頭\")\n",
    "    print(f\"失敗: {len(failed_ids)}頭\")\n",
    "    \n",
    "    if failed_ids:\n",
    "        print(f\"失敗した馬ID: {failed_ids[:10]}{'...' if len(failed_ids) > 10 else ''}\")\n",
    "    \n",
    "    print(\"\\n再スクレイピング完了後は、該当セクションのデータ処理を再実行してください。\")\n",
    "    \n",
    "elif 're_scrape_needed' in globals() and not re_scrape_needed:\n",
    "    print(\"再スクレイピングが必要な馬は見つかりませんでした。\")\n",
    "    \n",
    "else:\n",
    "    print(\"再スクレイピングを実行するには以下を設定してください：\")\n",
    "    print(\"1. 上のセルを実行してNaN馬を特定\")\n",
    "    print(\"2. 're_scrape_horses = True' のコメントアウトを外す\")\n",
    "    print(\"3. このセルを再実行\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再スクレイピング後のデータ再処理\n",
    "# 上の再スクレイピングが完了した後に実行してください\n",
    "# reprocess_data = True  # コメントアウトを外して実行\n",
    "\n",
    "if 'reprocess_data' in locals() and reprocess_data:\n",
    "    print(\"=== 再スクレイピング後のデータ再処理開始 ===\")\n",
    "    \n",
    "    # 1. 新しくスクレイピングしたHTMLから馬情報を再抽出\n",
    "    print(\"1. 馬情報の再抽出...\")\n",
    "    from modules.preprocessing._horse_info_processor import HorseInfoProcessor\n",
    "    \n",
    "    # 新しいHTMLファイルのみ処理\n",
    "    html_files_horse_new = []\n",
    "    for horse_id in nan_horse_ids_list:\n",
    "        html_file = os.path.join(LocalPaths.HTML_HORSE_DIR, f\"{horse_id}.bin\")\n",
    "        if os.path.exists(html_file):\n",
    "            html_files_horse_new.append(html_file)\n",
    "    \n",
    "    print(f\"再処理対象HTMLファイル数: {len(html_files_horse_new)}\")\n",
    "    \n",
    "    if len(html_files_horse_new) > 0:\n",
    "        # 馬情報を再処理\n",
    "        horse_info_processor = HorseInfoProcessor(html_files_horse_new)\n",
    "        horse_info_new = horse_info_processor.scrape_horse_info()\n",
    "        \n",
    "        # 既存の馬情報に新しい情報をマージ\n",
    "        try:\n",
    "            horse_info_existing = pd.read_pickle(os.path.join(LocalPaths.RAW_DIR, 'horse_info.pickle'))\n",
    "            # 新しい情報で既存の情報を更新\n",
    "            horse_info_updated = horse_info_existing.copy()\n",
    "            for horse_id in horse_info_new.index:\n",
    "                horse_info_updated.loc[horse_id] = horse_info_new.loc[horse_id]\n",
    "            \n",
    "            # バックアップ作成\n",
    "            horse_info_existing.to_pickle(os.path.join(LocalPaths.RAW_DIR, 'horse_info.pickle.bak'))\n",
    "            \n",
    "            # 更新されたデータを保存\n",
    "            horse_info_updated.to_pickle(os.path.join(LocalPaths.RAW_DIR, 'horse_info.pickle'))\n",
    "            \n",
    "            print(f\"馬情報更新完了: {len(horse_info_new)}頭の情報を更新\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"馬情報の更新エラー: {e}\")\n",
    "    \n",
    "    # 2. マスターファイルの再生成\n",
    "    print(\"\\n2. マスターファイルの再生成...\")\n",
    "    \n",
    "    # horse_info.pickleから各種IDを抽出してマスターファイルを更新\n",
    "    try:\n",
    "        horse_info_updated = pd.read_pickle(os.path.join(LocalPaths.RAW_DIR, 'horse_info.pickle'))\n",
    "        \n",
    "        # 各マスターファイルを更新\n",
    "        id_columns = {\n",
    "            'horse_id.csv': ('index', 'horse_id'),  # インデックスがhorse_id\n",
    "            'jockey_id.csv': ('騎手', 'jockey_id'),\n",
    "            'trainer_id.csv': ('調教師', 'trainer_id'),\n",
    "            'owner_id.csv': ('馬主', 'owner_id'),\n",
    "            'breeder_id.csv': ('生産者', 'breeder_id')\n",
    "        }\n",
    "        \n",
    "        for master_file, (col_name, id_type) in id_columns.items():\n",
    "            master_path = os.path.join(LocalPaths.MASTER_DIR, master_file)\n",
    "            \n",
    "            if col_name == 'index':\n",
    "                # horse_idの場合\n",
    "                unique_ids = horse_info_updated.index.dropna().unique()\n",
    "            else:\n",
    "                # その他のIDの場合\n",
    "                if col_name in horse_info_updated.columns:\n",
    "                    unique_ids = horse_info_updated[col_name].dropna().unique()\n",
    "                    unique_ids = [str(x) for x in unique_ids if str(x) not in ['nan', 'NaN', '']]\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            # 既存のマスターファイルを読み込み\n",
    "            if os.path.exists(master_path):\n",
    "                existing_master = pd.read_csv(master_path)\n",
    "                existing_ids = set(existing_master.iloc[:, 0].dropna().astype(str))\n",
    "            else:\n",
    "                existing_ids = set()\n",
    "                existing_master = pd.DataFrame(columns=[id_type, 'encoded_id'])\n",
    "            \n",
    "            # 新しいIDを追加\n",
    "            new_ids = [id for id in unique_ids if str(id) not in existing_ids]\n",
    "            \n",
    "            if new_ids:\n",
    "                # 新しいエンコードIDを生成\n",
    "                max_encoded = existing_master['encoded_id'].max() if len(existing_master) > 0 else -1\n",
    "                new_encoded = list(range(max_encoded + 1, max_encoded + 1 + len(new_ids)))\n",
    "                \n",
    "                # 新しいエントリを作成\n",
    "                new_entries = pd.DataFrame({\n",
    "                    id_type: new_ids,\n",
    "                    'encoded_id': new_encoded\n",
    "                })\n",
    "                \n",
    "                # マスターファイルを更新\n",
    "                updated_master = pd.concat([existing_master, new_entries], ignore_index=True)\n",
    "                updated_master.to_csv(master_path, index=False)\n",
    "                \n",
    "                print(f\"{master_file}: {len(new_ids)}個の新しいIDを追加\")\n",
    "        \n",
    "        print(\"\\nマスターファイル再生成完了！\")\n",
    "        print(\"これで特徴量エンジニアリングを再実行できます。\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"マスターファイル再生成エラー: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"データ再処理を実行するには 'reprocess_data = True' を設定してください\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 再スクレイピング機能の使い方\n",
    "\n",
    "### 手順:\n",
    "1. **NaN値検出**: セル24を実行してマスターファイル内のNaN値を特定\n",
    "2. **再スクレイピング実行**: セル25で `re_scrape_horses = True` のコメントアウトを外して実行\n",
    "3. **データ再処理**: セル26で `reprocess_data = True` のコメントアウトを外して実行\n",
    "\n",
    "### 注意事項:\n",
    "- 再スクレイピングには時間がかかる場合があります\n",
    "- サーバー負荷軽減のため、適度な間隔で実行されます\n",
    "- バックアップファイルが自動生成されます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/html/horse/に保存されているhtml(binファイル)をリストにする\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "horse_html_dir = LocalPaths.HTML_HORSE_DIR\n",
    "print(f\"馬HTMLディレクトリ: {horse_html_dir}\")\n",
    "\n",
    "html_files_horse = glob.glob(os.path.join(horse_html_dir, \"*.bin\"))\n",
    "print(f\"見つかったHTMLファイル数: {len(html_files_horse)}\")\n",
    "\n",
    "# 最初の5ファイルを表示\n",
    "html_files_horse[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 馬の基本情報テーブルの作成（修正済み関数使用）\n",
    "%autoreload\n",
    "horse_info_new = preparing.get_rawdata_horse_info(html_files_horse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_rawdata_horse_info関数のデバッグ: 1つのHTMLファイルで詳細確認\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy import nan as NaN\n",
    "\n",
    "# テスト用に1つのHTMLファイルを選択\n",
    "test_html_file = html_files_horse[0]\n",
    "print(f\"テスト対象ファイル: {test_html_file}\")\n",
    "\n",
    "# ファイルを読み込み\n",
    "with open(test_html_file, 'rb') as f:\n",
    "    html = f.read()\n",
    "\n",
    "# horse_idを取得\n",
    "horse_id = re.findall(r'horse\\W(\\d+)\\.bin', test_html_file)[0]\n",
    "print(f\"horse_id: {horse_id}\")\n",
    "\n",
    "# HTMLファイルサイズとデコード確認\n",
    "print(f\"HTMLファイルサイズ: {len(html)} bytes\")\n",
    "\n",
    "# EUC-JPでデコード試行\n",
    "for enc in ('euc-jp', 'cp932', 'utf-8'):\n",
    "    try:\n",
    "        text = html.decode(enc)\n",
    "        print(f\"デコード成功: {enc}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"デコード失敗 {enc}: {e}\")\n",
    "        text = html.decode(enc, errors='ignore')\n",
    "        print(f\"エラー無視でデコード: {enc}\")\n",
    "        break\n",
    "\n",
    "# BeautifulSoupで解析\n",
    "soup = BeautifulSoup(text, 'lxml')\n",
    "print(f\"BeautifulSoup解析完了\")\n",
    "\n",
    "# プロフィールテーブルの確認\n",
    "prof_table = (\n",
    "    soup.select_one('table.db_prof_table[summary*=\"プロフィール\"]')\n",
    "    or soup.find('table', attrs={'summary': re.compile('プロフィール')})\n",
    ")\n",
    "\n",
    "if prof_table:\n",
    "    print(\"プロフィールテーブル発見!\")\n",
    "    print(f\"テーブルHTML（最初の500文字）: {str(prof_table)[:500]}\")\n",
    "    \n",
    "    # テーブルを読み込む\n",
    "    try:\n",
    "        df = pd.read_html(str(prof_table), flavor='lxml')[0]\n",
    "        print(f\"DataFrame形状: {df.shape}\")\n",
    "        print(f\"DataFrame列名: {list(df.columns)}\")\n",
    "        print(\"DataFrame内容:\")\n",
    "        display(df)\n",
    "        \n",
    "        # 2列形式の確認\n",
    "        if df.shape[1] >= 2:\n",
    "            df = df.iloc[:, :2]\n",
    "            df.columns = ['項目', '値']\n",
    "            df_info = df.set_index('項目').T\n",
    "            print(\"転置後:\")\n",
    "            display(df_info)\n",
    "        else:\n",
    "            print(f\"プロフィール表の列数が想定外: {df.shape[1]}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"pd.read_htmlエラー: {e}\")\n",
    "else:\n",
    "    print(\"プロフィールテーブルが見つかりません\")\n",
    "    # 代替手段: すべてのテーブルを確認\n",
    "    print(\"すべてのテーブルを確認:\")\n",
    "    tables = soup.find_all('table')\n",
    "    for i, table in enumerate(tables):\n",
    "        attrs = table.attrs\n",
    "        print(f\"テーブル {i+1}: {attrs}\")\n",
    "        if i < 3:  # 最初の3テーブルの内容を確認\n",
    "            try:\n",
    "                df_temp = pd.read_html(str(table))[0]\n",
    "                print(f\"  形状: {df_temp.shape}, 列名: {list(df_temp.columns[:3])}\")\n",
    "                if len(df_temp) > 0:\n",
    "                    print(f\"  最初の行: {df_temp.iloc[0].values[:3]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  読み込みエラー: {e}\")\n",
    "\n",
    "# ID抽出の確認\n",
    "def extract_id(selector, pattern):\n",
    "    a = soup.select_one(selector)\n",
    "    if a and a.has_attr('href'):\n",
    "        m = re.search(pattern, a['href'])\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "    return NaN\n",
    "\n",
    "trainer_id = extract_id('a[href^=\"/trainer/\"]', r'/trainer/([^/]+)/')\n",
    "owner_id   = extract_id('a[href^=\"/owner/\"]',   r'/owner/([^/]+)/')\n",
    "breeder_id = extract_id('a[href^=\"/breeder/\"]', r'/breeder/([^/]+)/')\n",
    "\n",
    "print(f\"trainer_id: {trainer_id}\")\n",
    "print(f\"owner_id: {owner_id}\")\n",
    "print(f\"breeder_id: {breeder_id}\")\n",
    "\n",
    "# 関連リンクの確認\n",
    "trainer_links = soup.select('a[href^=\"/trainer/\"]')\n",
    "owner_links = soup.select('a[href^=\"/owner/\"]')\n",
    "breeder_links = soup.select('a[href^=\"/breeder/\"]')\n",
    "\n",
    "print(f\"調教師リンク数: {len(trainer_links)}\")\n",
    "print(f\"馬主リンク数: {len(owner_links)}\")\n",
    "print(f\"生産者リンク数: {len(breeder_links)}\")\n",
    "\n",
    "if trainer_links:\n",
    "    print(f\"調教師リンク例: {trainer_links[0].get('href')}\")\n",
    "if owner_links:\n",
    "    print(f\"馬主リンク例: {owner_links[0].get('href')}\")\n",
    "if breeder_links:\n",
    "    print(f\"生産者リンク例: {breeder_links[0].get('href')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エンコーディング問題の詳細調査（chardetなし）\n",
    "# 手動で各エンコーディングを試行\n",
    "\n",
    "encodings_to_try = ['utf-8', 'euc-jp', 'cp932', 'shift_jis', 'iso-2022-jp']\n",
    "\n",
    "successful_encoding = None\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        text_decoded = html.decode(encoding)\n",
    "        print(f\"デコード成功: {encoding}\")\n",
    "        \n",
    "        # BeautifulSoupで解析\n",
    "        soup_test = BeautifulSoup(text_decoded, 'lxml')\n",
    "        \n",
    "        # プロフィールテーブルの検索\n",
    "        prof_table_test = (\n",
    "            soup_test.find('table', class_='db_prof_table') or\n",
    "            soup_test.find('table', attrs={'summary': re.compile('プロフィール')}) or\n",
    "            soup_test.select_one('table[summary*=\"プロフィール\"]')\n",
    "        )\n",
    "        \n",
    "        if prof_table_test:\n",
    "            print(f\"  → プロフィールテーブル発見！エンコーディング: {encoding}\")\n",
    "            print(f\"  → テーブル属性: {prof_table_test.attrs}\")\n",
    "            \n",
    "            # テーブル内容を確認\n",
    "            try:\n",
    "                df_test = pd.read_html(str(prof_table_test))[0]\n",
    "                print(f\"  → DataFrame形状: {df_test.shape}\")\n",
    "                print(\"  → DataFrame内容:\")\n",
    "                display(df_test)\n",
    "                \n",
    "                # 2列形式に変換してみる\n",
    "                if df_test.shape[1] >= 2:\n",
    "                    df_test = df_test.iloc[:, :2]\n",
    "                    df_test.columns = ['項目', '値']\n",
    "                    df_info_test = df_test.set_index('項目').T\n",
    "                    print(\"  → 転置後:\")\n",
    "                    display(df_info_test)\n",
    "                    \n",
    "                successful_encoding = encoding\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  → pd.read_htmlエラー: {e}\")\n",
    "        else:\n",
    "            print(f\"  → プロフィールテーブル見つからず\")\n",
    "            \n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"デコード失敗: {encoding} - {str(e)[:100]}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"その他のエラー {encoding}: {e}\")\n",
    "        continue\n",
    "\n",
    "if successful_encoding:\n",
    "    print(f\"\\n成功したエンコーディング: {successful_encoding}\")\n",
    "    \n",
    "    # 正しいエンコーディングでの最終確認\n",
    "    text_final = html.decode(successful_encoding)\n",
    "    soup_final = BeautifulSoup(text_final, 'lxml')\n",
    "    \n",
    "    # すべてのテーブルを確認\n",
    "    tables_final = soup_final.find_all('table')\n",
    "    print(f\"\\n全テーブル数: {len(tables_final)}\")\n",
    "    for i, table in enumerate(tables_final):\n",
    "        attrs = table.attrs\n",
    "        summary = attrs.get('summary', '')\n",
    "        class_names = attrs.get('class', [])\n",
    "        print(f\"テーブル {i+1}: class={class_names}, summary='{summary}'\")\n",
    "        \n",
    "        # プロフィールらしいテーブルを詳しく確認\n",
    "        if 'db_prof_table' in class_names or 'プロフィール' in summary:\n",
    "            try:\n",
    "                df_detail = pd.read_html(str(table))[0]\n",
    "                print(f\"  → 詳細形状: {df_detail.shape}\")\n",
    "                print(f\"  → 列名: {list(df_detail.columns)}\")\n",
    "                if len(df_detail) > 0:\n",
    "                    print(f\"  → 最初の行: {df_detail.iloc[0].values}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  → 読み込みエラー: {e}\")\n",
    "                \n",
    "else:\n",
    "    print(\"\\nどのエンコーディングでもプロフィールテーブルが見つかりませんでした\")\n",
    "    \n",
    "    # HTMLの先頭部分を確認\n",
    "    print(f\"\\nHTML先頭500文字（バイナリ）:\")\n",
    "    print(html[:500])\n",
    "    \n",
    "    # UTF-8での強制解析\n",
    "    try:\n",
    "        text_force = html.decode('utf-8', errors='ignore')\n",
    "        print(f\"\\nUTF-8強制デコード後の先頭1000文字:\")\n",
    "        print(text_force[:1000])\n",
    "    except Exception as e:\n",
    "        print(f\"UTF-8強制デコードエラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_rawdata_horse_info関数の修正版を作成・テスト\n",
    "def get_rawdata_horse_info_fixed(html_path_list: list):\n",
    "    \"\"\"\n",
    "    horseページのhtmlを受け取って、馬の基本情報のDataFrameに変換する関数（修正版）。\n",
    "    - UTF-8優先でデコード\n",
    "    - プロフィールテーブルを確実に特定\n",
    "    - 調教師/馬主/生産者IDを確実に抽出\n",
    "    \"\"\"\n",
    "    print('preparing raw horse_info table (fixed version)')\n",
    "    out_rows = []\n",
    "\n",
    "    for html_path in tqdm(html_path_list):\n",
    "        try:\n",
    "            with open(html_path, 'rb') as f:\n",
    "                raw = f.read()\n",
    "\n",
    "            # 1) エンコーディング優先順位: UTF-8 → EUC-JP → CP932\n",
    "            text = None\n",
    "            for encoding in ['utf-8', 'euc-jp', 'cp932']:\n",
    "                try:\n",
    "                    text = raw.decode(encoding)\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            if text is None:\n",
    "                print(f'エンコーディング失敗: {html_path}')\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(text, 'lxml')\n",
    "\n",
    "            # 2) プロフィールテーブルの確実な特定\n",
    "            prof_table = (\n",
    "                soup.find('table', class_='db_prof_table') or\n",
    "                soup.find('table', attrs={'summary': re.compile('プロフィール')}) or\n",
    "                soup.select_one('table[summary*=\"プロフィール\"]')\n",
    "            )\n",
    "            \n",
    "            if prof_table is None:\n",
    "                print(f'プロフィールテーブル見つからず: {html_path}')\n",
    "                continue\n",
    "\n",
    "            # 3) テーブルを読み込む（StringIOを使用して警告を回避）\n",
    "            from io import StringIO\n",
    "            df = pd.read_html(StringIO(str(prof_table)))[0]\n",
    "            \n",
    "            # 左列を項目名、右列を値として転置（1行化）\n",
    "            if df.shape[1] >= 2:\n",
    "                df = df.iloc[:, :2]\n",
    "                df.columns = ['項目', '値']\n",
    "                df_info = df.set_index('項目').T\n",
    "            else:\n",
    "                print(f'プロフィールテーブルの列数が想定外: {html_path}')\n",
    "                continue\n",
    "\n",
    "            # 4) 各IDをより確実に抽出\n",
    "            def extract_id(selector, pattern):\n",
    "                a = soup.select_one(selector)\n",
    "                if a and a.has_attr('href'):\n",
    "                    m = re.search(pattern, a['href'])\n",
    "                    if m:\n",
    "                        return m.group(1)\n",
    "                return NaN\n",
    "\n",
    "            trainer_id = extract_id('a[href^=\"/trainer/\"]', r'/trainer/([^/]+)/')\n",
    "            owner_id   = extract_id('a[href^=\"/owner/\"]',   r'/owner/([^/]+)/')\n",
    "            breeder_id = extract_id('a[href^=\"/breeder/\"]', r'/breeder/([^/]+)/')\n",
    "\n",
    "            df_info['trainer_id'] = trainer_id\n",
    "            df_info['owner_id']   = owner_id\n",
    "            df_info['breeder_id'] = breeder_id\n",
    "\n",
    "            # 5) インデックスを horse_id に\n",
    "            horse_id_m = re.search(r'horse\\W(\\d+)\\.bin', html_path)\n",
    "            if horse_id_m:\n",
    "                horse_id = horse_id_m.group(1)\n",
    "                df_info.index = [horse_id]\n",
    "                out_rows.append(df_info)\n",
    "            else:\n",
    "                print(f'horse_id抽出失敗: {html_path}')\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f'処理エラー {html_path}: {e}')\n",
    "            continue\n",
    "\n",
    "    if not out_rows:\n",
    "        print('処理できたhorse_infoデータがありません')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    horse_info_df = pd.concat(out_rows, axis=0)\n",
    "    print(f'horse_info処理完了: {horse_info_df.shape}')\n",
    "    return horse_info_df\n",
    "\n",
    "# 修正版でテスト実行\n",
    "horse_info_new_fixed = get_rawdata_horse_info_fixed(html_files_horse)\n",
    "print(f\"\\n修正版の結果: {horse_info_new_fixed.shape}\")\n",
    "if len(horse_info_new_fixed) > 0:\n",
    "    print(f\"列名: {list(horse_info_new_fixed.columns[:5])}\")\n",
    "    print(f\"最初の数行:\")\n",
    "    display(horse_info_new_fixed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horse_info.pickleを読み込み\n",
    "try:\n",
    "    horse_info = pd.read_pickle(os.path.join(LocalPaths.RAW_DIR, 'horse_info.pickle'))\n",
    "    print(f\"horse_info.pickle読み込み完了: {len(horse_info)}頭の馬データ\")\n",
    "except Exception as e:\n",
    "    print(f\"horse_info.pickleの読み込みエラー: {e}\")\n",
    "    horse_info = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 馬の基本情報テーブルの更新\n",
    "preparing.update_rawdata(LocalPaths.RAW_HORSE_INFO_PATH, horse_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 馬の過去成績テーブルの作成\n",
    "horse_results_new = preparing.get_rawdata_horse_results(html_files_horse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果確認\n",
    "print(f\"horse_results_new shape: {horse_results_new.shape}\")\n",
    "print(f\"horse_results_new type: {type(horse_results_new)}\")\n",
    "if len(horse_results_new) > 0:\n",
    "    print(f\"列名: {list(horse_results_new.columns[:5])}\")\n",
    "    print(f\"最初の数行:\")\n",
    "    display(horse_results_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題診断: 1つのHTMLファイルでpd.read_htmlの動作確認\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# テスト用に1つのHTMLファイルを選択\n",
    "test_html_file = html_files_horse[0]\n",
    "print(f\"テスト対象ファイル: {test_html_file}\")\n",
    "\n",
    "# ファイルを読み込み\n",
    "with open(test_html_file, 'rb') as f:\n",
    "    html = f.read()\n",
    "\n",
    "# horse_idを取得\n",
    "horse_id = re.findall(r'horse\\W(\\d+)\\.bin', test_html_file)[0]\n",
    "print(f\"horse_id: {horse_id}\")\n",
    "\n",
    "# BeautifulSoupで解析\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "print(f\"HTMLファイルサイズ: {len(html)} bytes\")\n",
    "\n",
    "# テーブル要素の確認\n",
    "tables = soup.find_all('table')\n",
    "print(f\"テーブル数: {len(tables)}\")\n",
    "\n",
    "for i, table in enumerate(tables):\n",
    "    class_names = table.get('class', [])\n",
    "    print(f\"テーブル {i+1}: class={class_names}\")\n",
    "\n",
    "# pd.read_htmlでテーブル読み込みテスト\n",
    "try:\n",
    "    dfs = pd.read_html(html)\n",
    "    print(f\"pd.read_htmlで読み込めたテーブル数: {len(dfs)}\")\n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        print(f\"DataFrame {i+1}: 形状={df.shape}, 列名={list(df.columns[:3])}\")\n",
    "        if len(df) > 0:\n",
    "            print(f\"  最初の行: {df.iloc[0].values[:3]}\")\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"pd.read_htmlエラー: {e}\")\n",
    "\n",
    "# 過去成績テーブルの直接確認\n",
    "race_results_table = soup.find('table', class_='db_h_race_results')\n",
    "if race_results_table:\n",
    "    print(\"過去成績テーブルが見つかりました!\")\n",
    "    rows = race_results_table.find_all('tr')\n",
    "    print(f\"行数: {len(rows)}\")\n",
    "    if len(rows) > 1:  # ヘッダー行を除く\n",
    "        first_data_row = rows[1]\n",
    "        cells = first_data_row.find_all(['td', 'th'])\n",
    "        print(f\"最初のデータ行のセル数: {len(cells)}\")\n",
    "        print(f\"最初のセルの内容: {cells[0].get_text(strip=True) if cells else 'なし'}\")\n",
    "else:\n",
    "    print(\"過去成績テーブルが見つかりませんでした\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_rawdata_horse_resultsの修正版をテスト\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_rawdata_horse_results_fixed(html_path_list: list):\n",
    "    \"\"\"\n",
    "    horseページのhtmlを受け取って、馬の過去成績のDataFrameに変換する関数。\n",
    "    AJAX実装対応版: 過去成績テーブルはインデックス1（2番目）にある\n",
    "    \"\"\"\n",
    "    print('preparing raw horse_results table (fixed version)')\n",
    "    horse_results = {}\n",
    "    for html_path in tqdm(html_path_list):\n",
    "        with open(html_path, 'rb') as f:\n",
    "            try:\n",
    "                # 保存してあるbinファイルを読み込む\n",
    "                html = f.read()\n",
    "\n",
    "                # AJAX実装では、過去成績テーブルは2番目（インデックス1）\n",
    "                dfs = pd.read_html(html)\n",
    "                \n",
    "                # テーブル数の確認\n",
    "                if len(dfs) < 2:\n",
    "                    print(f'horse_results insufficient tables: {len(dfs)} tables in {html_path}')\n",
    "                    continue\n",
    "                \n",
    "                # 過去成績テーブルは2番目（インデックス1）\n",
    "                df = dfs[1]\n",
    "                \n",
    "                # 新馬の競走馬レビューが付いた場合、\n",
    "                # 列名に0が付与されるため、次のhtmlへ飛ばす\n",
    "                if df.columns[0] == 0:\n",
    "                    print('horse_results empty case1 {}'.format(html_path))\n",
    "                    continue\n",
    "\n",
    "                horse_id = re.findall(r'horse\\W(\\d+)\\.bin', html_path)[0]\n",
    "\n",
    "                df.index = [horse_id] * len(df)\n",
    "                horse_results[horse_id] = df\n",
    "                print(f'Successfully processed {horse_id}: {df.shape}')\n",
    "\n",
    "            # 競走データが無い場合（新馬）を飛ばす\n",
    "            except IndexError:\n",
    "                print('horse_results empty case2 {}'.format(html_path))\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f'horse_results error in {html_path}: {e}')\n",
    "                continue\n",
    "\n",
    "    if not horse_results:\n",
    "        print(\"警告: 処理できた過去成績データがありません\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # pd.DataFrame型にして一つのデータにまとめる\n",
    "    horse_results_df = pd.concat([horse_results[key] for key in horse_results])\n",
    "\n",
    "    # 列名に半角スペースがあれば除去する\n",
    "    horse_results_df = horse_results_df.rename(columns=lambda x: x.replace(' ', ''))\n",
    "\n",
    "    return horse_results_df\n",
    "\n",
    "# 修正版でテスト実行\n",
    "horse_results_new_fixed = get_rawdata_horse_results_fixed(html_files_horse)\n",
    "print(f\"\\n修正版の結果: {horse_results_new_fixed.shape}\")\n",
    "if len(horse_results_new_fixed) > 0:\n",
    "    print(f\"列名: {list(horse_results_new_fixed.columns[:5])}\")\n",
    "    print(f\"最初の数行:\")\n",
    "    display(horse_results_new_fixed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルの更新\n",
    "preparing.update_rawdata(LocalPaths.RAW_HORSE_RESULTS_PATH, horse_results_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(horse_info_new)\n",
    "\n",
    "# 馬の基本情報テーブルの行数を取得\n",
    "print(f\"馬の基本情報テーブルの行数: {len(horse_info_new)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(horse_results_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 特定期間の再スクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from modules import preparing\n",
    "from modules.constants import LocalPaths\n",
    "\n",
    "# 1) 対象期間\n",
    "START = \"2025-12-01\"\n",
    "END   = \"2026-01-01\"  # この日付は含めない想定（必要なら調整）\n",
    "\n",
    "# 2) 開催日(yyyymmdd) -> レースID取得\n",
    "# scrape_kaisai_date は yyyy-mm を受け取り「to_月は含まない」仕様\n",
    "kaisai_date_list = preparing.scrape_kaisai_date(\"2025-12\", \"2026-01\")\n",
    "\n",
    "# 念のため日付でフィルタ（文字列比較でOK: yyyymmdd）\n",
    "start_yyyymmdd = START.replace(\"-\", \"\")\n",
    "end_yyyymmdd   = END.replace(\"-\", \"\")\n",
    "kaisai_date_list = [d for d in kaisai_date_list if start_yyyymmdd <= d < end_yyyymmdd]\n",
    "\n",
    "race_id_list = preparing.scrape_race_id_list(kaisai_date_list)\n",
    "race_id_list = sorted(set(race_id_list))\n",
    "print(\"race_id count:\", len(race_id_list))\n",
    "\n",
    "# 3) レースHTMLを再取得（skip=False で上書き）\n",
    "race_html_paths = preparing.scrape_html_race(race_id_list, skip=False)\n",
    "print(\"race html updated:\", len(race_html_paths))\n",
    "\n",
    "# 4) レース結果→出走馬ID抽出\n",
    "results_new = preparing.get_rawdata_results(race_html_paths)\n",
    "horse_id_list = sorted(set(results_new[\"horse_id\"].dropna().astype(str).tolist()))\n",
    "print(\"horse_id count:\", len(horse_id_list))\n",
    "\n",
    "# （任意だが推奨）レース系rawも同時に更新して整合を取る\n",
    "race_info_new = preparing.get_rawdata_info(race_html_paths)\n",
    "return_new = preparing.get_rawdata_return(race_html_paths)\n",
    "\n",
    "preparing.update_rawdata(LocalPaths.RAW_RESULTS_PATH, results_new, mode=\"replace\")\n",
    "preparing.update_rawdata(LocalPaths.RAW_RACE_INFO_PATH, race_info_new, mode=\"replace\")\n",
    "preparing.update_rawdata(LocalPaths.RAW_RETURN_TABLES_PATH, return_new, mode=\"replace\")\n",
    "\n",
    "# 5) 馬ページ（戦績断片を挿入する実装）を再取得して、更新日時マスタも更新\n",
    "horse_html_paths = preparing.scrape_html_horse_with_master(horse_id_list, skip=False)\n",
    "print(\"horse html updated:\", len(horse_html_paths))\n",
    "\n",
    "# 6) horse_info / horse_results を作ってraw pickle更新\n",
    "horse_info_new = preparing.get_rawdata_horse_info(horse_html_paths)\n",
    "horse_results_new = preparing.get_rawdata_horse_results(horse_html_paths)\n",
    "\n",
    "preparing.update_rawdata(LocalPaths.RAW_HORSE_INFO_PATH, horse_info_new, mode=\"replace\")\n",
    "preparing.update_rawdata(LocalPaths.RAW_HORSE_RESULTS_PATH, horse_results_new, mode=\"replace\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. /ped/ディレクトリのデータ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_files_peds = preparing.scrape_html_ped(horse_id_list, skip=True) #htmlをスクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 血統データ完全スクレイピングの実装 ===\n",
    "import os\n",
    "from modules.preparing import scrape_html_ped\n",
    "\n",
    "# 1. 全てのHTMLファイルから完全な馬IDリストを作成\n",
    "complete_horse_id_list = [os.path.splitext(os.path.basename(f))[0] for f in html_files_horse]\n",
    "print(f\"完全な馬IDリスト作成完了: {len(complete_horse_id_list)}件\")\n",
    "\n",
    "# 2. 既存の血統データを確認\n",
    "existing_peds_ids = set([os.path.splitext(os.path.basename(f))[0] for f in html_files_peds])\n",
    "print(f\"既存の血統データ: {len(existing_peds_ids)}件\")\n",
    "\n",
    "# 3. 不足している血統データのIDを特定\n",
    "missing_ped_ids = set(complete_horse_id_list) - existing_peds_ids\n",
    "missing_ped_ids_list = sorted(list(missing_ped_ids))\n",
    "print(f\"不足している血統データ: {len(missing_ped_ids_list)}件\")\n",
    "\n",
    "# 4. 年代別の不足状況確認\n",
    "missing_by_year = {}\n",
    "for horse_id in missing_ped_ids_list:\n",
    "    year = horse_id[:4] if horse_id[:4].isdigit() else \"unknown\"\n",
    "    missing_by_year[year] = missing_by_year.get(year, 0) + 1\n",
    "\n",
    "print(f\"\\n年代別の不足血統データ:\")\n",
    "for year in sorted(missing_by_year.keys()):\n",
    "    print(f\"  {year}年: {missing_by_year[year]}件\")\n",
    "\n",
    "print(f\"\\n=== スクレイピング実行 ===\")\n",
    "print(f\"対象馬数: {len(missing_ped_ids_list)}件\")\n",
    "print(f\"推定時間: {len(missing_ped_ids_list) * 2 / 60:.1f}分\")\n",
    "\n",
    "# 5. 不足している血統データをスクレイピング実行\n",
    "# 警告: これは時間がかかる処理です（約163分）\n",
    "# バッチ処理で実行することを推奨\n",
    "print(\"\\n血統データスクレイピングを開始...\")\n",
    "new_ped_files = scrape_html_ped(missing_ped_ids_list, skip=False)\n",
    "print(f\"スクレイピング完了: {len(new_ped_files)}件の新しい血統データを取得\")\n",
    "\n",
    "# 6. 結果確認\n",
    "import glob\n",
    "updated_html_files_peds = glob.glob(os.path.join(LocalPaths.HTML_PED_DIR, \"*.bin\"))\n",
    "print(f\"更新後の血統データファイル数: {len(updated_html_files_peds)}\")\n",
    "print(f\"馬情報ファイル数: {len(html_files_horse)}\")\n",
    "print(f\"差分: {len(html_files_horse) - len(updated_html_files_peds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まず少数でテスト（最初の100件）\n",
    "test_missing_ids = missing_ped_ids_list[:100]\n",
    "print(f\"テスト対象: {len(test_missing_ids)}件\")\n",
    "test_ped_files = scrape_html_ped(test_missing_ids, skip=False)\n",
    "print(f\"テスト完了: {len(test_ped_files)}件\")\n",
    "\n",
    "# 成功を確認後、残りを実行\n",
    "remaining_ids = missing_ped_ids_list[100:]\n",
    "remaining_ped_files = scrape_html_ped(remaining_ids, skip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 血統データスクレイピングのテスト: 少数の馬でテスト実行\n",
    "# 先頭5頭の馬IDを使用してテスト\n",
    "test_horse_ids = horse_id_test_list[:5]  # 最初の5頭でテスト\n",
    "print(f\"テスト対象馬ID: {test_horse_ids}\")\n",
    "\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 血統HTMLファイルのスクレイピングテスト（5頭のみ）\n",
    "print(\"血統HTMLファイルのスクレイピングを開始...\")\n",
    "html_files_peds_test = preparing.scrape_html_ped(test_horse_ids, skip=False)\n",
    "print(f\"スクレイピング完了: {len(html_files_peds_test)}件のHTMLファイル\")\n",
    "\n",
    "# 取得されたファイルパスの確認\n",
    "if html_files_peds_test:\n",
    "    print(\"取得されたファイル:\")\n",
    "    for i, file_path in enumerate(html_files_peds_test[:3]):  # 最初の3件表示\n",
    "        print(f\"  {i+1}: {file_path}\")\n",
    "    if len(html_files_peds_test) > 3:\n",
    "        print(f\"  ... 他{len(html_files_peds_test)-3}件\")\n",
    "else:\n",
    "    print(\"取得されたHTMLファイルがありません（既存ファイルがスキップされた可能性）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 血統テーブルの作成テスト\n",
    "print(\"血統テーブルの作成を開始...\")\n",
    "peds_test = preparing.get_rawdata_peds(html_files_peds_test)\n",
    "\n",
    "print(f\"血統テーブル作成完了: {peds_test.shape}\")\n",
    "if len(peds_test) > 0:\n",
    "    print(f\"列名: {list(peds_test.columns)}\")\n",
    "    print(\"血統データサンプル:\")\n",
    "    display(peds_test.head())\n",
    "    \n",
    "    # 各列のnull値確認\n",
    "    print(\"\\n各列のnull値の数:\")\n",
    "    print(peds_test.isnull().sum())\n",
    "else:\n",
    "    print(\"血統テーブルが空です。HTMLファイルの構造を確認します。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 血統HTMLファイルの構造解析\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# テスト用に1つの血統HTMLファイルを詳しく調べる\n",
    "test_ped_file = html_files_peds_test[0]\n",
    "print(f\"解析対象ファイル: {test_ped_file}\")\n",
    "\n",
    "# ファイルを読み込み\n",
    "with open(test_ped_file, 'rb') as f:\n",
    "    ped_html = f.read()\n",
    "\n",
    "print(f\"HTMLファイルサイズ: {len(ped_html)} bytes\")\n",
    "\n",
    "# エンコーディングテスト\n",
    "encodings = ['utf-8', 'euc-jp', 'cp932']\n",
    "ped_text = None\n",
    "successful_encoding = None\n",
    "\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        ped_text = ped_html.decode(encoding)\n",
    "        successful_encoding = encoding\n",
    "        print(f\"デコード成功: {encoding}\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"デコード失敗: {encoding}\")\n",
    "        continue\n",
    "\n",
    "if ped_text:\n",
    "    # BeautifulSoupで解析\n",
    "    ped_soup = BeautifulSoup(ped_text, 'lxml')\n",
    "    \n",
    "    # HTMLの基本情報\n",
    "    title = ped_soup.find('title')\n",
    "    print(f\"ページタイトル: {title.text if title else 'なし'}\")\n",
    "    \n",
    "    # テーブル要素の確認\n",
    "    tables = ped_soup.find_all('table')\n",
    "    print(f\"テーブル数: {len(tables)}\")\n",
    "    \n",
    "    for i, table in enumerate(tables[:5]):  # 最初の5テーブルを確認\n",
    "        attrs = table.attrs\n",
    "        class_names = attrs.get('class', [])\n",
    "        summary = attrs.get('summary', '')\n",
    "        print(f\"テーブル {i+1}: class={class_names}, summary='{summary}'\")\n",
    "        \n",
    "        # テーブル内容の簡単な確認\n",
    "        try:\n",
    "            df_ped = pd.read_html(str(table))[0]\n",
    "            print(f\"  形状: {df_ped.shape}\")\n",
    "            if len(df_ped) > 0 and df_ped.shape[1] > 0:\n",
    "                print(f\"  列名: {list(df_ped.columns[:3])}\")\n",
    "                print(f\"  最初の行: {df_ped.iloc[0].values[:3]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  読み込みエラー: {e}\")\n",
    "        \n",
    "        # 血統らしいキーワードを含むテーブルかチェック\n",
    "        table_text = table.get_text()\n",
    "        if any(keyword in table_text for keyword in ['父', '母', '祖父', '血統']):\n",
    "            print(f\"  → 血統関連テーブルの可能性あり\")\n",
    "    \n",
    "    # 血統ツリー構造の確認（td要素のクラス名など）\n",
    "    print(\"\\n血統ツリー関連の要素:\")\n",
    "    bloodline_elements = ped_soup.find_all(['td', 'div'], class_=re.compile(r'(blood|ped|pedigree)', re.I))\n",
    "    print(f\"血統関連要素数: {len(bloodline_elements)}\")\n",
    "    \n",
    "    if bloodline_elements:\n",
    "        for elem in bloodline_elements[:3]:\n",
    "            print(f\"  要素: {elem.name}, class: {elem.get('class')}, text: {elem.get_text()[:50]}...\")\n",
    "            \n",
    "else:\n",
    "    print(\"すべてのエンコーディングでデコードに失敗しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_rawdata_peds関数の修正版を作成・テスト\n",
    "def get_rawdata_peds_fixed(html_path_list: list):\n",
    "    \"\"\"\n",
    "    horse/pedページのhtmlを受け取って、血統のDataFrameに変換する関数（修正版）。\n",
    "    - EUC-JP優先でデコード\n",
    "    - 血統テーブルを確実に特定\n",
    "    - 血統horse_idを確実に抽出\n",
    "    \"\"\"\n",
    "    print('preparing raw peds table (fixed version)')\n",
    "    peds = {}\n",
    "    \n",
    "    for html_path in tqdm(html_path_list):\n",
    "        try:\n",
    "            with open(html_path, 'rb') as f:\n",
    "                raw = f.read()\n",
    "\n",
    "            # 1) エンコーディング優先順位: EUC-JP → UTF-8 → CP932\n",
    "            text = None\n",
    "            for encoding in ['euc-jp', 'utf-8', 'cp932']:\n",
    "                try:\n",
    "                    text = raw.decode(encoding)\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            if text is None:\n",
    "                print(f'エンコーディング失敗: {html_path}')\n",
    "                continue\n",
    "\n",
    "            # horse_idを取得\n",
    "            horse_id = re.findall(r'ped\\W(\\d+)\\.bin', html_path)[0]\n",
    "\n",
    "            # htmlをsoupオブジェクトに変換\n",
    "            soup = BeautifulSoup(text, \"lxml\")\n",
    "\n",
    "            peds_id_list = []\n",
    "\n",
    "            # 2) 血統データからhorse_idを取得する\n",
    "            blood_table = soup.find(\"table\", attrs={\"summary\": \"5代血統表\"})\n",
    "            if blood_table is None:\n",
    "                # 代替検索\n",
    "                blood_table = soup.find(\"table\", class_=\"blood_table\")\n",
    "                \n",
    "            if blood_table is None:\n",
    "                print(f'血統テーブル見つからず: {html_path}')\n",
    "                continue\n",
    "\n",
    "            horse_a_list = blood_table.find_all(\"a\", attrs={\"href\": re.compile(r\"^/horse/\\w{10}\")})\n",
    "\n",
    "            for a in horse_a_list:\n",
    "                # 血統データのhorse_idを抜き出す\n",
    "                work_peds_id = re.findall(r'/horse/(\\w{10})', a[\"href\"])[0]\n",
    "                peds_id_list.append(work_peds_id)\n",
    "\n",
    "            peds[horse_id] = peds_id_list\n",
    "            print(f'血統ID取得成功 {horse_id}: {len(peds_id_list)}個')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'処理エラー {html_path}: {e}')\n",
    "            continue\n",
    "\n",
    "    if not peds:\n",
    "        print('処理できた血統データがありません')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # pd.DataFrame型にして一つのデータにまとめて、列名をpeds_0, ..., peds_61にする\n",
    "    peds_df = pd.DataFrame.from_dict(peds, orient='index').add_prefix('peds_')\n",
    "    print(f'血統データ処理完了: {peds_df.shape}')\n",
    "    return peds_df\n",
    "\n",
    "# 修正版でテスト実行\n",
    "peds_test_fixed = get_rawdata_peds_fixed(html_files_peds_test)\n",
    "print(f\"\\n修正版の結果: {peds_test_fixed.shape}\")\n",
    "if len(peds_test_fixed) > 0:\n",
    "    print(f\"列名サンプル: {list(peds_test_fixed.columns[:10])}\")\n",
    "    print(\"血統データサンプル:\")\n",
    "    display(peds_test_fixed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 血統テーブル内のリンク構造を詳しく調査\n",
    "test_ped_file = html_files_peds_test[0]\n",
    "\n",
    "with open(test_ped_file, 'rb') as f:\n",
    "    raw = f.read()\n",
    "\n",
    "# EUC-JPでデコード\n",
    "text = raw.decode('euc-jp')\n",
    "soup = BeautifulSoup(text, \"lxml\")\n",
    "\n",
    "# 血統テーブルを取得\n",
    "blood_table = soup.find(\"table\", attrs={\"summary\": \"5代血統表\"})\n",
    "if blood_table:\n",
    "    print(\"血統テーブル発見!\")\n",
    "    \n",
    "    # テーブル内のすべてのリンクを確認\n",
    "    all_links = blood_table.find_all(\"a\")\n",
    "    print(f\"血統テーブル内の全リンク数: {len(all_links)}\")\n",
    "    \n",
    "    # 最初の10リンクを詳しく確認\n",
    "    for i, link in enumerate(all_links[:10]):\n",
    "        href = link.get('href', '')\n",
    "        text_content = link.get_text()\n",
    "        print(f\"リンク {i+1}: href='{href}', text='{text_content}'\")\n",
    "    \n",
    "    # horse関連のリンクパターンを確認\n",
    "    horse_patterns = [\n",
    "        r\"^/horse/\\w{10}\",  # 現在のパターン\n",
    "        r\"/horse/\",         # より広いパターン\n",
    "        r\"horse\",           # 最も広いパターン\n",
    "    ]\n",
    "    \n",
    "    for pattern in horse_patterns:\n",
    "        matches = blood_table.find_all(\"a\", attrs={\"href\": re.compile(pattern)})\n",
    "        print(f\"パターン '{pattern}' にマッチするリンク数: {len(matches)}\")\n",
    "        if matches:\n",
    "            for j, match in enumerate(matches[:3]):\n",
    "                print(f\"  例{j+1}: {match.get('href')}\")\n",
    "    \n",
    "    # 実際のhorse_idの抽出テスト\n",
    "    print(\"\\n実際のhorse_id抽出テスト:\")\n",
    "    for link in all_links[:5]:\n",
    "        href = link.get('href', '')\n",
    "        if '/horse/' in href:\n",
    "            print(f\"リンク: {href}\")\n",
    "            # 異なる抽出パターンをテスト\n",
    "            patterns = [\n",
    "                r'/horse/(\\w{10})',\n",
    "                r'/horse/(\\w+)',\n",
    "                r'horse/(\\w+)',\n",
    "            ]\n",
    "            for p in patterns:\n",
    "                matches = re.findall(p, href)\n",
    "                if matches:\n",
    "                    print(f\"  パターン '{p}': {matches}\")\n",
    "else:\n",
    "    print(\"血統テーブルが見つかりません\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修正版のhorse_id抽出パターンを作成・テスト\n",
    "import re\n",
    "\n",
    "# 正しいパターンで抽出テスト\n",
    "correct_pattern = r'https://db\\.netkeiba\\.com/horse/(\\w{10})/$'\n",
    "\n",
    "test_ped_file = html_files_peds_test[0]\n",
    "with open(test_ped_file, 'rb') as f:\n",
    "    raw = f.read()\n",
    "\n",
    "text = raw.decode('euc-jp')\n",
    "soup = BeautifulSoup(text, \"lxml\")\n",
    "blood_table = soup.find(\"table\", attrs={\"summary\": \"5代血統表\"})\n",
    "\n",
    "if blood_table:\n",
    "    # 馬の詳細ページのみを抽出\n",
    "    horse_links = blood_table.find_all(\"a\", attrs={\"href\": re.compile(correct_pattern)})\n",
    "    print(f\"馬の詳細ページリンク数: {len(horse_links)}\")\n",
    "    \n",
    "    # horse_idを抽出\n",
    "    horse_ids = []\n",
    "    for link in horse_links:\n",
    "        href = link.get('href')\n",
    "        match = re.search(correct_pattern, href)\n",
    "        if match:\n",
    "            horse_id = match.group(1)\n",
    "            horse_ids.append(horse_id)\n",
    "            horse_name = link.get_text().strip()\n",
    "            print(f\"  {horse_id}: {horse_name}\")\n",
    "    \n",
    "    print(f\"\\n抽出されたhorse_id数: {len(horse_ids)}\")\n",
    "    print(f\"ユニークなhorse_id数: {len(set(horse_ids))}\")\n",
    "else:\n",
    "    print(\"血統テーブルが見つかりません\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修正版get_rawdata_peds関数を作成・テスト\n",
    "def get_rawdata_peds_fixed(horse_id_list):\n",
    "    \"\"\"\n",
    "    血統データを取得する修正版関数\n",
    "    \"\"\"\n",
    "    \n",
    "    peds_new = pd.DataFrame()\n",
    "    \n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        try:\n",
    "            # ファイルパスを作成\n",
    "            html_path = f\"data/html/ped/{horse_id}.bin\"\n",
    "            \n",
    "            # ファイルが存在するかチェック\n",
    "            if not os.path.exists(html_path):\n",
    "                print(f\"ファイルが見つかりません: {html_path}\")\n",
    "                continue\n",
    "            \n",
    "            # HTMLファイルを読み込み\n",
    "            with open(html_path, 'rb') as f:\n",
    "                raw = f.read()\n",
    "            \n",
    "            # エンコーディングを試行（UTF-8 → EUC-JP → CP932）\n",
    "            for encoding in ['utf-8', 'euc-jp', 'cp932']:\n",
    "                try:\n",
    "                    text = raw.decode(encoding)\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"デコードに失敗しました: {horse_id}\")\n",
    "                continue\n",
    "            \n",
    "            # BeautifulSoupでパース\n",
    "            soup = BeautifulSoup(text, \"lxml\")\n",
    "            \n",
    "            # 血統テーブルを検索\n",
    "            blood_table = soup.find(\"table\", attrs={\"summary\": \"5代血統表\"})\n",
    "            \n",
    "            if blood_table is None:\n",
    "                print(f\"血統テーブルが見つかりません: {horse_id}\")\n",
    "                continue\n",
    "            \n",
    "            # 修正された正規表現パターンで血統IDを抽出\n",
    "            pattern = r'https://db\\.netkeiba\\.com/horse/(\\w{10})/$'\n",
    "            horse_links = blood_table.find_all(\"a\", attrs={\"href\": re.compile(pattern)})\n",
    "            \n",
    "            # horse_idを抽出\n",
    "            peds_horse_ids = []\n",
    "            for link in horse_links:\n",
    "                href = link.get('href')\n",
    "                match = re.search(pattern, href)\n",
    "                if match:\n",
    "                    peds_horse_ids.append(match.group(1))\n",
    "            \n",
    "            # 結果をDataFrameに追加\n",
    "            if peds_horse_ids:\n",
    "                temp_df = pd.DataFrame({\n",
    "                    'horse_id': [horse_id] * len(peds_horse_ids),\n",
    "                    'peds_horse_id': peds_horse_ids\n",
    "                })\n",
    "                peds_new = pd.concat([peds_new, temp_df], ignore_index=True)\n",
    "                print(f\"血統ID取得成功 {horse_id}: {len(peds_horse_ids)}個\")\n",
    "            else:\n",
    "                print(f\"血統IDが取得できませんでした: {horse_id}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"エラーが発生しました {horse_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return peds_new\n",
    "\n",
    "# テスト用の血統HTMLファイルから馬IDを抽出\n",
    "test_horse_ids = []\n",
    "for file_path in html_files_peds_test:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    horse_id = file_name.replace('.bin', '')\n",
    "    test_horse_ids.append(horse_id)\n",
    "\n",
    "print(f\"テスト用馬ID: {test_horse_ids}\")\n",
    "\n",
    "# テスト実行\n",
    "print(\"修正版get_rawdata_peds関数をテスト中...\")\n",
    "peds_new_fixed = get_rawdata_peds_fixed(test_horse_ids)\n",
    "print(f\"\\n修正版関数の結果: {peds_new_fixed.shape}\")\n",
    "print(peds_new_fixed.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モジュールをリロードして修正版をテスト\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# モジュールをリロード\n",
    "if 'modules.preparing._get_rawdata' in sys.modules:\n",
    "    del sys.modules['modules.preparing._get_rawdata']\n",
    "\n",
    "from modules.preparing._get_rawdata import get_rawdata_peds\n",
    "\n",
    "# 修正版の関数でテスト\n",
    "print(\"元ファイルの修正版get_rawdata_peds関数をテスト中...\")\n",
    "peds_from_module = get_rawdata_peds(html_files_peds_test)\n",
    "print(f\"\\nモジュール関数の結果: {peds_from_module.shape}\")\n",
    "print(peds_from_module.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 血統データスクレイピングテストの結果確認\n",
    "print(\"=== 血統データスクレイピングテスト結果 ===\")\n",
    "print(f\"テスト馬数: {len(test_horse_ids)}頭\")\n",
    "print(f\"取得血統データ: {peds_from_module.shape[0]}頭 × {peds_from_module.shape[1]}血統ID\")\n",
    "print(f\"総血統ID数: {peds_from_module.shape[0] * peds_from_module.shape[1]}個\")\n",
    "\n",
    "# 各馬の血統ID数を確認\n",
    "print(f\"\\n各馬の血統ID数:\")\n",
    "for horse_id in test_horse_ids:\n",
    "    if horse_id in peds_from_module.index:\n",
    "        non_null_count = peds_from_module.loc[horse_id].notna().sum()\n",
    "        print(f\"  {horse_id}: {non_null_count}個\")\n",
    "\n",
    "# サンプル血統データを表示\n",
    "print(f\"\\n血統データサンプル（馬ID: {test_horse_ids[0]}）:\")\n",
    "sample_row = peds_from_module.loc[test_horse_ids[0]]\n",
    "sample_peds = sample_row.dropna().head(10)\n",
    "for i, (col, horse_id) in enumerate(sample_peds.items()):\n",
    "    print(f\"  {col}: {horse_id}\")\n",
    "\n",
    "print(\"\\n血統データテーブル作成テスト: ✅ 成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# LocalPathsからHTML血統ディレクトリを取得\n",
    "ped_html_dir = LocalPaths.HTML_PED_DIR\n",
    "print(f\"血統HTMLディレクトリ: {ped_html_dir}\")\n",
    "\n",
    "# globでbinファイルを検索\n",
    "html_files_peds = glob.glob(os.path.join(ped_html_dir, \"*.bin\"))\n",
    "print(f\"見つかったHTMLファイル数: {len(html_files_peds)}\")\n",
    "\n",
    "# 最初の5ファイルを表示\n",
    "html_files_peds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML数の詳細分析：馬の基本情報と血統データの差異調査\n",
    "import os\n",
    "\n",
    "# 馬の基本情報のIDリストを作成\n",
    "horse_ids = set([os.path.splitext(os.path.basename(f))[0] for f in html_files_horse])\n",
    "print(f\"馬の基本情報のユニークID数: {len(horse_ids)}\")\n",
    "\n",
    "# 血統データのIDリストを作成\n",
    "peds_ids = set([os.path.splitext(os.path.basename(f))[0] for f in html_files_peds])\n",
    "print(f\"血統データのユニークID数: {len(peds_ids)}\")\n",
    "\n",
    "print(f\"\\n数の違い: {len(horse_ids) - len(peds_ids)} 件\")\n",
    "\n",
    "# 血統データがない馬IDを特定\n",
    "missing_peds = horse_ids - peds_ids\n",
    "print(f\"\\n血統データがない馬IDの数: {len(missing_peds)}\")\n",
    "\n",
    "if len(missing_peds) > 0:\n",
    "    print(\"\\n血統データがない馬IDの例（最初の10件）:\")\n",
    "    missing_list = sorted(list(missing_peds))\n",
    "    for horse_id in missing_list[:10]:\n",
    "        print(f\"  {horse_id}\")\n",
    "        \n",
    "    # 年代別の分析\n",
    "    print(\"\\n年代別の分析（血統データ不足）:\")\n",
    "    year_analysis = {}\n",
    "    for horse_id in missing_peds:\n",
    "        year = horse_id[:4]  # 馬IDの最初の4文字が年\n",
    "        year_analysis[year] = year_analysis.get(year, 0) + 1\n",
    "    \n",
    "    for year in sorted(year_analysis.keys()):\n",
    "        print(f\"  {year}年: {year_analysis[year]}件\")\n",
    "\n",
    "# 逆に馬の基本情報がない血統データがあるかも確認\n",
    "missing_horse_info = peds_ids - horse_ids\n",
    "print(f\"\\n馬の基本情報がない血統データの数: {len(missing_horse_info)}\")\n",
    "if len(missing_horse_info) > 0:\n",
    "    print(\"馬の基本情報がない血統IDの例（最初の5件）:\")\n",
    "    for ped_id in list(missing_horse_info)[:5]:\n",
    "        print(f\"  {ped_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクレイピングプロセスの差異調査\n",
    "print(\"=== スクレイピングプロセス差異分析 ===\")\n",
    "\n",
    "# 1. データ収集時期の違いを確認\n",
    "print(\"\\n1. 収集データの時期分析:\")\n",
    "horse_years = {}\n",
    "peds_years = {}\n",
    "\n",
    "for f in html_files_horse:\n",
    "    horse_id = os.path.splitext(os.path.basename(f))[0]\n",
    "    year = horse_id[:4] if horse_id[:4].isdigit() else \"unknown\"\n",
    "    horse_years[year] = horse_years.get(year, 0) + 1\n",
    "\n",
    "for f in html_files_peds:\n",
    "    ped_id = os.path.splitext(os.path.basename(f))[0]\n",
    "    year = ped_id[:4] if ped_id[:4].isdigit() else \"unknown\"\n",
    "    peds_years[year] = peds_years.get(year, 0) + 1\n",
    "\n",
    "print(\"年別収集数の比較:\")\n",
    "all_years = sorted(set(horse_years.keys()) | set(peds_years.keys()))\n",
    "for year in all_years:\n",
    "    horse_count = horse_years.get(year, 0)\n",
    "    peds_count = peds_years.get(year, 0)\n",
    "    diff = horse_count - peds_count\n",
    "    print(f\"  {year}年: 馬情報{horse_count:5d}件, 血統{peds_count:5d}件, 差分{diff:5d}件\")\n",
    "\n",
    "# 2. ファイルサイズの分析（スクレイピング成功率の推定）\n",
    "print(\"\\n2. ファイルサイズ分析（サンプル）:\")\n",
    "import random\n",
    "\n",
    "sample_horse_files = random.sample(html_files_horse, min(50, len(html_files_horse)))\n",
    "sample_peds_files = random.sample(html_files_peds, min(50, len(html_files_peds)))\n",
    "\n",
    "horse_sizes = [os.path.getsize(f) for f in sample_horse_files if os.path.exists(f)]\n",
    "peds_sizes = [os.path.getsize(f) for f in sample_peds_files if os.path.exists(f)]\n",
    "\n",
    "if horse_sizes and peds_sizes:\n",
    "    print(f\"馬情報ファイル平均サイズ: {sum(horse_sizes)/len(horse_sizes):.0f} bytes\")\n",
    "    print(f\"血統ファイル平均サイズ: {sum(peds_sizes)/len(peds_sizes):.0f} bytes\")\n",
    "    print(f\"小さすぎるファイル（<1000 bytes）の割合:\")\n",
    "    small_horse = sum(1 for s in horse_sizes if s < 1000) / len(horse_sizes) * 100\n",
    "    small_peds = sum(1 for s in peds_sizes if s < 1000) / len(peds_sizes) * 100\n",
    "    print(f\"  馬情報: {small_horse:.1f}%\")\n",
    "    print(f\"  血統: {small_peds:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peds_new = preparing.get_rawdata_peds(html_files_peds) #血統テーブルの作成\n",
    "preparing.update_rawdata(LocalPaths.RAW_PEDS_PATH, peds_new) #テーブルの更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(peds_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. データ加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モジュールを更新した際、notebookに反映させるために使用。\n",
    "#すでにインポートしてあるモジュールの更新が反映される。\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#前処理\n",
    "results_processor = preprocessing.ResultsProcessor(filepath=LocalPaths.RAW_RESULTS_PATH)\n",
    "race_info_processor = preprocessing.RaceInfoProcessor(filepath=LocalPaths.RAW_RACE_INFO_PATH)\n",
    "return_processor = preprocessing.ReturnProcessor(filepath=LocalPaths.RAW_RETURN_TABLES_PATH)\n",
    "horse_info_processor = preprocessing.HorseInfoProcessor(\n",
    "    filepath=LocalPaths.RAW_HORSE_INFO_PATH)\n",
    "horse_results_processor = preprocessing.HorseResultsProcessor(\n",
    "    filepath=LocalPaths.RAW_HORSE_RESULTS_PATH)\n",
    "peds_processor = preprocessing.PedsProcessor(filepath=LocalPaths.RAW_PEDS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 騎手成績特徴量（直近10・50レース複勝率）の作成\n",
    "from modules.constants import LocalPaths\n",
    "from modules import preprocessing\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# data/tmp ディレクトリを作成\n",
    "os.makedirs(LocalPaths.TMP_DIR, exist_ok=True)\n",
    "\n",
    "# RAW_HORSE_RESULTS から騎手複勝率特徴量を作成して保存\n",
    "jockey_stats_processor = preprocessing.JockeyStatsProcessor(filepath=LocalPaths.RAW_HORSE_RESULTS_PATH)\n",
    "jockey_stats = jockey_stats_processor.preprocessed_data\n",
    "jockey_stats.to_pickle(LocalPaths.JOCKEY_STATS_PATH)\n",
    "jockey_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "馬の過去成績を集計しつつ、前処理の済みの全てのテーブルをマージする処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットエンコーディング時に「馬の成績」として扱う項目\n",
    "TARGET_COLS = [\n",
    "        HorseResultsCols.RANK,\n",
    "        HorseResultsCols.PRIZE,\n",
    "        HorseResultsCols.RANK_DIFF, \n",
    "        'first_corner',\n",
    "        'final_corner',\n",
    "        'first_to_rank',\n",
    "        'first_to_final',\n",
    "        'final_to_rank',\n",
    "        'time_seconds'\n",
    "        ]\n",
    "# horse_id列と共に、ターゲットエンコーディングの対象にする列\n",
    "GROUP_COLS = [\n",
    "        'course_len',\n",
    "        'race_type',\n",
    "        HorseResultsCols.PLACE\n",
    "        ]\n",
    "\n",
    "data_merger = preprocessing.DataMerger(\n",
    "        results_processor,\n",
    "        race_info_processor,\n",
    "        horse_results_processor,\n",
    "        horse_info_processor,\n",
    "        peds_processor,\n",
    "        target_cols=TARGET_COLS,\n",
    "        group_cols=GROUP_COLS\n",
    ")\n",
    "# 処理実行\n",
    "data_merger.merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#カテゴリ変数の処理\n",
    "feature_enginnering = preprocessing.FeatureEngineering(data_merger)\\\n",
    "    .add_interval()\\\n",
    "    .add_agedays()\\\n",
    "    .dumminize_ground_state()\\\n",
    "    .dumminize_race_type()\\\n",
    "    .dumminize_sex()\\\n",
    "    .dumminize_weather()\\\n",
    "    .encode_horse_id()\\\n",
    "    .encode_jockey_id()\\\n",
    "    .encode_trainer_id()\\\n",
    "    .encode_owner_id()\\\n",
    "    .encode_breeder_id()\\\n",
    "    .dumminize_kaisai()\\\n",
    "    .dumminize_around()\\\n",
    "    .dumminize_race_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存\n",
    "#tmpは一時保存用のディレクトリ\n",
    "feature_enginnering.featured_data.to_pickle('data/tmp/featured_data_20260103.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keiba_ai = training.KeibaAIFactory.create(feature_enginnering.featured_data) #モデル作成\n",
    "keiba_ai.train_with_tuning() #パラメータチューニングをして学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量の重要度\n",
    "keiba_ai.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ハイパーパラメータの確認\n",
    "keiba_ai.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#チューニングしないで学習\n",
    "#keiba_ai.train_without_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデル保存。models/(実行した日付)/(version_name).pickleに、モデルとデータセットが保存される。\n",
    "training.KeibaAIFactory.save(keiba_ai, version_name='basemodel_2020_2025')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルロード\n",
    "keiba_ai = training.KeibaAIFactory.load('models/20260103/basemodel_2020_2025.pickle')\n",
    "keiba_ai.set_params(keiba_ai.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. シミュレーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5章（シミュレーション）だけを実行したいときの軽量セットアップ用セル\n",
    "\n",
    "# カーネル再起動後に 3章・4章を再実行せず、\n",
    "\n",
    "# 学習済みモデル（UMABAN を特徴量から除外して再学習したもの）と払戻テーブルだけを準備する。\n",
    "\n",
    "\n",
    "\n",
    "from modules.constants import LocalPaths\n",
    "\n",
    "from modules import preprocessing, training\n",
    "\n",
    "\n",
    "\n",
    "# 払戻テーブルの前処理のみ実行（Simulator が参照）\n",
    "\n",
    "return_processor = preprocessing.ReturnProcessor(filepath=LocalPaths.RAW_RETURN_TABLES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#シミュレーターに馬券をセット\n",
    "simulator = simulation.Simulator(return_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.202428034860377e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.202428034860377e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00019328962466044669, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00019328962466044669\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.509934059998701, subsample=1.0 will be ignored. Current value: bagging_fraction=0.509934059998701\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    }
   ],
   "source": [
    "# スコアテーブルを取得\n",
    "score_table = keiba_ai.calc_score(keiba_ai.datasets.X_test, policies.StdScorePolicy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. 単一threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 単勝馬券"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>jockey_plc_rate_10_all</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>jockey_plc_rate_50_all</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jockey_id</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>jockey_has_history_flag</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>jockey_rides_10_all</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>jockey_rides_50_all</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    features  importance\n",
       "182   jockey_plc_rate_10_all         130\n",
       "184   jockey_plc_rate_50_all          95\n",
       "3                  jockey_id          28\n",
       "186  jockey_has_history_flag          17\n",
       "183      jockey_rides_10_all           0\n",
       "185      jockey_rides_50_all           0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi = keiba_ai.feature_importance(num_features=300)\n",
    "fi_j = fi[fi['features'].str.startswith('jockey_')]\n",
    "fi_j.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_enginnering' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# セルを新規で作って実行\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m feat = \u001b[43mfeature_enginnering\u001b[49m.featured_data\n\u001b[32m      3\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m feat.columns \u001b[38;5;28;01mif\u001b[39;00m c.startswith(\u001b[33m'\u001b[39m\u001b[33mjockey_\u001b[39m\u001b[33m'\u001b[39m)], feat.filter(like=\u001b[33m'\u001b[39m\u001b[33mjockey_\u001b[39m\u001b[33m'\u001b[39m).head()\n",
      "\u001b[31mNameError\u001b[39m: name 'feature_enginnering' is not defined"
     ]
    }
   ],
   "source": [
    "# セルを新規で作って実行\n",
    "feat = feature_enginnering.featured_data\n",
    "[c for c in feat.columns if c.startswith('jockey_')], feat.filter(like='jockey_').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_RANGE = [0.0, 3.5]\n",
      "score_table columns (head): ['score']\n",
      "\n",
      "score_table[jockey関連列] の例:\n",
      "[]\n",
      "\n",
      "score 分布:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    693524.000000\n",
       "mean          0.318755\n",
       "std           0.208554\n",
       "min           0.001272\n",
       "25%           0.176201\n",
       "50%           0.315512\n",
       "75%           0.460562\n",
       "max           0.955636\n",
       "Name: score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "threshold=0.0 での actions 件数:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Column not found: 馬番'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mthreshold=0.0 での actions 件数:\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolicies\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BetPolicyTansho\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m actions_debug = \u001b[43mkeiba_ai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecide_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBetPolicyTansho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mlen(actions_debug) =\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(actions_debug))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koxyg\\Documents\\GitHub\\MyKeiba-AI_v2\\modules\\training\\_keiba_ai.py:146\u001b[39m, in \u001b[36mKeibaAI.decide_action\u001b[39m\u001b[34m(self, score_table, bet_policy, **params)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecide_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, score_table: pd.DataFrame,\n\u001b[32m    142\u001b[39m     bet_policy: AbstractBetPolicy, **params) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m    143\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m    bet_policyを元に、賭ける馬券を決定する。paramsにthresholdを入れる。\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     actions = \u001b[43mbet_policy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjudge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m actions\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koxyg\\Documents\\GitHub\\MyKeiba-AI_v2\\modules\\policies\\_bet_policy.py:31\u001b[39m, in \u001b[36mBetPolicyTansho.judge\u001b[39m\u001b[34m(score_table, threshold)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjudge\u001b[39m(score_table: pd.DataFrame, threshold: \u001b[38;5;28mfloat\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m     30\u001b[39m     filtered_table = score_table[score_table[\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m] >= threshold]\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     bet_df = \u001b[43mfiltered_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResultsCols\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUMABAN\u001b[49m\u001b[43m]\u001b[49m.apply(\u001b[38;5;28mlist\u001b[39m).to_frame()\n\u001b[32m     32\u001b[39m     bet_dict = bet_df.rename(columns={ResultsCols.UMABAN: \u001b[33m'\u001b[39m\u001b[33mtansho\u001b[39m\u001b[33m'\u001b[39m}).T.to_dict()\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m bet_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koxyg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:1951\u001b[39m, in \u001b[36mDataFrameGroupBy.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1944\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) > \u001b[32m1\u001b[39m:\n\u001b[32m   1945\u001b[39m     \u001b[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[32m   1946\u001b[39m     \u001b[38;5;66;03m# valid syntax, so don't raise\u001b[39;00m\n\u001b[32m   1947\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1948\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1949\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse a list instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1950\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koxyg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:244\u001b[39m, in \u001b[36mSelectionMixin.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    245\u001b[39m     ndim = \u001b[38;5;28mself\u001b[39m.obj[key].ndim\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gotitem(key, ndim=ndim)\n",
      "\u001b[31mKeyError\u001b[39m: 'Column not found: 馬番'"
     ]
    }
   ],
   "source": [
    "# 単勝シミュレーション: T_RANGE と actions サイズ、騎手特徴量の確認用デバッグ\n",
    "\n",
    "T_RANGE = [0.0, 3.5]\n",
    "print('T_RANGE =', T_RANGE)\n",
    "\n",
    "print('score_table columns (head):', score_table.columns[:10].tolist())\n",
    "\n",
    "print('\\nscore_table[jockey関連列] の例:')\n",
    "\n",
    "j_cols = [c for c in score_table.columns if c.startswith('jockey_')]\n",
    "\n",
    "print(j_cols[:10])\n",
    "\n",
    "if j_cols:\n",
    "\n",
    "    display(score_table[j_cols].describe())\n",
    "\n",
    "\n",
    "\n",
    "print('\\nscore 分布:')\n",
    "\n",
    "display(score_table['score'].describe())\n",
    "\n",
    "\n",
    "\n",
    "print('\\nthreshold=0.0 での actions 件数:')\n",
    "\n",
    "from modules.policies import BetPolicyTansho\n",
    "\n",
    "actions_debug = keiba_ai.decide_action(score_table, BetPolicyTansho, threshold=0.0)\n",
    "\n",
    "print('len(actions_debug) =', len(actions_debug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.202428034860377e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.202428034860377e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00019328962466044669, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00019328962466044669\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.509934059998701, subsample=1.0 will be ignored. Current value: bagging_fraction=0.509934059998701\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2bd52d4a7a4c36a183ba532a7b0483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\koxyg\\AppData\\Local\\Temp\\ipykernel_15096\\3173261058.py\", line 19, in <module>\n",
      "    actions = keiba_ai.decide_action(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\koxyg\\Documents\\GitHub\\MyKeiba-AI_v2\\modules\\training\\_keiba_ai.py\", line 146, in decide_action\n",
      "    actions = bet_policy.judge(score_table, **params)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\koxyg\\Documents\\GitHub\\MyKeiba-AI_v2\\modules\\policies\\_bet_policy.py\", line 31, in judge\n",
      "    bet_df = filtered_table.groupby(level=0)[ResultsCols.UMABAN].apply(list).to_frame()\n",
      "             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\koxyg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py\", line 1951, in __getitem__\n",
      "    return super().__getitem__(key)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\koxyg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py\", line 244, in __getitem__\n",
      "    raise KeyError(f\"Column not found: {key}\")\n",
      "KeyError: 'Column not found: 馬番'\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "T_RANGE = [0.0, 3.5]\n",
    "N_SAMPLES = 100\n",
    "returns = {}\n",
    "\n",
    "# スコアテーブルを一度だけ計算しておく\n",
    "score_table = keiba_ai.calc_score(keiba_ai.datasets.X_test, policies.StdScorePolicy)\n",
    "\n",
    "# 「馬の勝ちやすさスコア」の閾値を変化させた時の成績を計算\n",
    "for i in tqdm(range(N_SAMPLES)):\n",
    "    # T_RANGE の範囲を N_SAMPLES 個に分割し、0.0〜3.5 を両端含めてスイープ\n",
    "    if N_SAMPLES > 1:\n",
    "        threshold = T_RANGE[0] + (T_RANGE[1] - T_RANGE[0]) * i / (N_SAMPLES - 1)\n",
    "    else:\n",
    "        threshold = T_RANGE[0]\n",
    "    try:\n",
    "        # 賭ける馬券を決定\n",
    "        actions = keiba_ai.decide_action(\n",
    "            score_table,              # スコアテーブル\n",
    "            policies.BetPolicyTansho, # 賭け方の方針\n",
    "            threshold=threshold       # 「馬の勝ちやすさスコア」の閾値\n",
    "        )\n",
    "        returns[threshold] = simulator.calc_returns(actions)\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        break\n",
    "\n",
    "returns_df = pd.DataFrame.from_dict(returns, orient='index').sort_index()\n",
    "returns_df.index.name = 'threshold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#シミュレーション結果も、models/に保存しておくとわかりやすい。\n",
    "returns_df.to_pickle('models/20260103/tansho.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#回収率をプロット\n",
    "simulation.plot_single_threshold(returns_df, N_SAMPLES, label='tansho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_single_threshold_compare(old_returns_df, returns_df, N_SAMPLES, label1='old_tansho', label2='new_tansho'):\n",
    "    plt.figure(dpi=100)\n",
    "    # old_returns_dfの標準偏差で幅をつけて薄くプロット\n",
    "    plt.fill_between(\n",
    "        old_returns_df.index,\n",
    "        y1=old_returns_df['return_rate']-old_returns_df['std'],\n",
    "        y2=old_returns_df['return_rate']+old_returns_df['std'],\n",
    "        alpha=0.3\n",
    "        )\n",
    "    # old_returns_dfの回収率を実線でプロット\n",
    "    plt.plot(old_returns_df.index, old_returns_df['return_rate'], label=label1)\n",
    "\n",
    "    # returns_dfの標準偏差で幅をつけて薄くプロット\n",
    "    plt.fill_between(\n",
    "        returns_df.index,\n",
    "        y1=returns_df['return_rate']-returns_df['std'],\n",
    "        y2=returns_df['return_rate']+returns_df['std'],\n",
    "        alpha=0.3\n",
    "        )\n",
    "    # returns_dfの回収率を実線でプロット\n",
    "    plt.plot(returns_df.index, returns_df['return_rate'], label=label2)\n",
    "\n",
    "    # labelで設定した凡例を表示させる\n",
    "    plt.legend()\n",
    "    # グリッドをつける\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('threshold')\n",
    "    plt.ylabel('return_rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_returns_df = pd.read_pickle('models/20251223/tansho.pickle')\n",
    "\n",
    "#old_returns_dfとreturns_dfの結果を重ねてプロットして比較\n",
    "plot_single_threshold_compare(\n",
    "    old_returns_df, returns_df, N_SAMPLES,\n",
    "    label1='old_tansho', label2='new_tansho'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_table['score'].describe() の出力とreturns_df.index.min()/max() と len(returns_df)を貼るコード\n",
    "print(\"score_table['score'] の統計情報:\")\n",
    "display(score_table['score'].describe())\n",
    "print(f\"returns_df index min: {returns_df.index.min()}\")\n",
    "print(f\"returns_df index max: {returns_df.index.max()}\")\n",
    "print(f\"returns_df length: {len(returns_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 複勝馬券"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_RANGE = [0.5, 3.5]\n",
    "N_SAMPLES = 100\n",
    "returns = {}\n",
    "\n",
    "# 「馬の勝ちやすさスコア」の閾値を変化させた時の成績を計算\n",
    "for i in tqdm(range(N_SAMPLES)):\n",
    "    # T_RANGEの範囲を、N_SAMPLES等分して、thresholdをfor分で回す\n",
    "    threshold = T_RANGE[1] * i / N_SAMPLES + T_RANGE[0] * (1 - (i / N_SAMPLES))\n",
    "    try:\n",
    "        # 賭ける馬券を決定\n",
    "        actions = keiba_ai.decide_action(\n",
    "                score_table, # スコアテーブル\n",
    "                policies.BetPolicyFukusho, # 賭け方の方針\n",
    "                threshold=threshold # 「馬の勝ちやすさスコア」の閾値\n",
    "                )\n",
    "        returns[threshold] = simulator.calc_returns(actions)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break\n",
    "returns_df = pd.DataFrame.from_dict(returns, orient='index')\n",
    "returns_df.index.name = 'threshold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シミュレーション結果も、models/YYYYMMDD/に保存しておくとわかりやすい。\n",
    "returns_df.to_pickle('models/20260103/fukusho.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回収率をプロット\n",
    "simulation.plot_single_threshold(returns_df, N_SAMPLES, label='fukusho')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 馬連BOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_RANGE = [0.5, 3.5]\n",
    "N_SAMPLES = 100\n",
    "returns = {}\n",
    "\n",
    "# 「馬の勝ちやすさスコア」の閾値を変化させた時の成績を計算\n",
    "for i in tqdm(range(N_SAMPLES)):\n",
    "    # T_RANGEの範囲を、N_SAMPLES等分して、thresholdをfor分で回す\n",
    "    threshold = T_RANGE[1] * i / N_SAMPLES + T_RANGE[0] * (1 - (i / N_SAMPLES))\n",
    "    try:\n",
    "        # 賭ける馬券を決定\n",
    "        actions = keiba_ai.decide_action(\n",
    "                score_table, # スコアテーブル\n",
    "                policies.BetPolicyUmarenBox, # 賭け方の方針\n",
    "                threshold=threshold # 「馬の勝ちやすさスコア」の閾値\n",
    "                )\n",
    "        returns[threshold] = simulator.calc_returns(actions)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break\n",
    "returns_df = pd.DataFrame.from_dict(returns, orient='index')\n",
    "returns_df.index.name = 'threshold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シミュレーション結果も、models/YYYYMMDD/に保存しておくとわかりやすい。\n",
    "returns_df.to_pickle('models/20260103/umarenbox.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回収率をプロット\n",
    "simulation.plot_single_threshold(returns_df, N_SAMPLES, label='umarenbox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4 馬単BOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_RANGE = [0.5, 3.5]\n",
    "N_SAMPLES = 100\n",
    "returns = {}\n",
    "\n",
    "# 「馬の勝ちやすさスコア」の閾値を変化させた時の成績を計算\n",
    "for i in tqdm(range(N_SAMPLES)):\n",
    "    # T_RANGEの範囲を、N_SAMPLES等分して、thresholdをfor分で回す\n",
    "    threshold = T_RANGE[1] * i / N_SAMPLES + T_RANGE[0] * (1 - (i / N_SAMPLES))\n",
    "    try:\n",
    "        # 賭ける馬券を決定\n",
    "        actions = keiba_ai.decide_action(\n",
    "                score_table, # スコアテーブル\n",
    "                policies.BetPolicyUmatanBox, # 賭け方の方針\n",
    "                threshold=threshold # 「馬の勝ちやすさスコア」の閾値\n",
    "                )\n",
    "        returns[threshold] = simulator.calc_returns(actions)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break\n",
    "returns_df = pd.DataFrame.from_dict(returns, orient='index')\n",
    "returns_df.index.name = 'threshold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シミュレーション結果も、models/YYYYMMDD/に保存しておくとわかりやすい。\n",
    "returns_df.to_pickle('models/20260103/umatanbox.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回収率をプロット\n",
    "simulation.plot_single_threshold(returns_df, N_SAMPLES, label='umatanbox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.5 ワイドBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_RANGE = [0.5, 3.5]\n",
    "N_SAMPLES = 100\n",
    "returns = {}\n",
    "\n",
    "# 「馬の勝ちやすさスコア」の閾値を変化させた時の成績を計算\n",
    "for i in tqdm(range(N_SAMPLES)):\n",
    "    # T_RANGEの範囲を、N_SAMPLES等分して、thresholdをfor分で回す\n",
    "    threshold = T_RANGE[1] * i / N_SAMPLES + T_RANGE[0] * (1 - (i / N_SAMPLES))\n",
    "    try:\n",
    "        # 賭ける馬券を決定\n",
    "        actions = keiba_ai.decide_action(\n",
    "                score_table, # スコアテーブル\n",
    "                policies.BetPolicyWideBox, # 賭け方の方針\n",
    "                threshold=threshold # 「馬の勝ちやすさスコア」の閾値\n",
    "                )\n",
    "        returns[threshold] = simulator.calc_returns(actions)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break\n",
    "returns_df = pd.DataFrame.from_dict(returns, orient='index')\n",
    "returns_df.index.name = 'threshold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シミュレーション結果も、models/YYYYMMDD/に保存しておくとわかりやすい。\n",
    "returns_df.to_pickle('models/20260103/widebox.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回収率をプロット\n",
    "simulation.plot_single_threshold(returns_df, N_SAMPLES, label='widebox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.6 三連複BOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_RANGE = [0.5, 3.5]\n",
    "N_SAMPLES = 100\n",
    "returns = {}\n",
    "\n",
    "# 「馬の勝ちやすさスコア」の閾値を変化させた時の成績を計算\n",
    "for i in tqdm(range(N_SAMPLES)):\n",
    "    # T_RANGEの範囲を、N_SAMPLES等分して、thresholdをfor分で回す\n",
    "    threshold = T_RANGE[1] * i / N_SAMPLES + T_RANGE[0] * (1 - (i / N_SAMPLES))\n",
    "    try:\n",
    "        # 賭ける馬券を決定\n",
    "        actions = keiba_ai.decide_action(\n",
    "                score_table, # スコアテーブル\n",
    "                policies.BetPolicySanrenpukuBox, # 賭け方の方針\n",
    "                threshold=threshold # 「馬の勝ちやすさスコア」の閾値\n",
    "                )\n",
    "        returns[threshold] = simulator.calc_returns(actions)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break\n",
    "returns_df = pd.DataFrame.from_dict(returns, orient='index')\n",
    "returns_df.index.name = 'threshold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シミュレーション結果も、models/YYYYMMDD/に保存しておくとわかりやすい。\n",
    "returns_df.to_pickle('models/20260103/sanrenpukubox.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回収率をプロット\n",
    "simulation.plot_single_threshold(returns_df, N_SAMPLES, label='sanrenpukubox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.7 三連単BOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_RANGE = [0.5, 3.5]\n",
    "N_SAMPLES = 100\n",
    "returns = {}\n",
    "\n",
    "# 「馬の勝ちやすさスコア」の閾値を変化させた時の成績を計算\n",
    "for i in tqdm(range(N_SAMPLES)):\n",
    "    # T_RANGEの範囲を、N_SAMPLES等分して、thresholdをfor分で回す\n",
    "    threshold = T_RANGE[1] * i / N_SAMPLES + T_RANGE[0] * (1 - (i / N_SAMPLES))\n",
    "    try:\n",
    "        # 賭ける馬券を決定\n",
    "        actions = keiba_ai.decide_action(\n",
    "                score_table, # スコアテーブル\n",
    "                policies.BetPolicySanrentanBox, # 賭け方の方針\n",
    "                threshold=threshold # 「馬の勝ちやすさスコア」の閾値\n",
    "                )\n",
    "        returns[threshold] = simulator.calc_returns(actions)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break\n",
    "returns_df = pd.DataFrame.from_dict(returns, orient='index')\n",
    "returns_df.index.name = 'threshold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シミュレーション結果も、models/YYYYMMDD/に保存しておくとわかりやすい。\n",
    "returns_df.to_pickle('models/20260103/sanrentanbox.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回収率をプロット\n",
    "simulation.plot_single_threshold(returns_df, N_SAMPLES, label='sanrentanbox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. 複数馬券\n",
    "未実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. 複数threshold\n",
    "未実装だが、以下のようなコードになる予定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1_RANGE = [2.5, 3.5]\n",
    "MIN_T2 = 1\n",
    "N_SAMPLES = 10\n",
    "\n",
    "returns = {}\n",
    "#「馬の勝ちやすさスコア」の閾値を変化させた時の成績を計算\n",
    "idx = 0\n",
    "for i in tqdm(range(N_SAMPLES)):\n",
    "    #T_RANGEの範囲を、N_SAMPLES等分して、thresholdをfor分で回す\n",
    "    threshold1 = T1_RANGE[1] * i / N_SAMPLES + T1_RANGE[0] * (1-(i/N_SAMPLES))\n",
    "    for j in range(N_SAMPLES):\n",
    "        #MIN_T2からthreshold1までをN_SAMPLES等分\n",
    "        threshold2 = threshold1 * j / N_SAMPLES + MIN_T2 * (1-(j/N_SAMPLES))\n",
    "        try:\n",
    "            #print(threshold1, threshold2)\n",
    "            #賭ける馬券を決定\n",
    "            actions = keiba_ai.decide_action(\n",
    "                    score_table, # スコアテーブル\n",
    "                    policies.BetPolicyTanshoFukusho, # 賭け方の方針(未実装)\n",
    "                    threshold1=threshold1, #「馬の勝ちやすさスコア」の閾値\n",
    "                    threshold2=threshold2\n",
    "                    )\n",
    "            returns[idx] = simulator.calc_returns(actions)\n",
    "            idx += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "returns_df = pd.DataFrame.from_dict(returns, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation.plot_single_threshold(returns_df.reset_index(), 100, label='tansho_fukusho')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 当日の予想\n",
    "例として2022年1月8日のレースを実際に予想する場合を考える。  \n",
    "https://race.netkeiba.com/top/race_list.html?kaisai_date=20220108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. 前日準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_id_list = preparing.scrape_race_id_list(['20260105']) #レースidを取得\n",
    "len(race_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モジュール変更をNotebookに反映（スクレイピング周りの修正後はここを実行）\n",
    "import importlib\n",
    "import modules.preparing._scrape_shutuba_table as _sst\n",
    "import modules.preparing._scrape_html as _sh\n",
    "import modules.preparing as preparing\n",
    "importlib.reload(_sst)\n",
    "importlib.reload(_sh)\n",
    "importlib.reload(preparing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#出走するhorse_idの取得\n",
    "horse_id_list = preparing.scrape_horse_id_list(race_id_list)\n",
    "len(horse_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#horseページのhtmlをスクレイピング\n",
    "#直近レースが更新されている可能性があるので、skip=Falseにして上書きする\n",
    "html_files_horse = preparing.scrape_html_horse_with_master(horse_id_list, skip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#horse_infoテーブルの更新\n",
    "horse_info_20250920 = preparing.get_rawdata_horse_info(html_files_horse)\n",
    "preparing.update_rawdata(LocalPaths.RAW_HORSE_INFO_PATH, horse_info_20250920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#horse_resultsテーブルの更新\n",
    "horse_results_20250920 = preparing.get_rawdata_horse_results(html_files_horse)\n",
    "preparing.update_rawdata(LocalPaths.RAW_HORSE_RESULTS_PATH, horse_results_20250920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#更新後のhorse_infoテーブルの確認\n",
    "horse_info_processor = preprocessing.HorseInfoProcessor(filepath=LocalPaths.RAW_HORSE_INFO_PATH)\n",
    "display(horse_info_processor.raw_data.tail())\n",
    "len(horse_info_processor.raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#更新後のhorse_resultsテーブルの確認\n",
    "horse_results_processor = preprocessing.HorseResultsProcessor(filepath=LocalPaths.RAW_HORSE_RESULTS_PATH)\n",
    "display(horse_results_processor.raw_data.tail())\n",
    "len(horse_results_processor.raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pedsテーブルの更新\n",
    "html_files_peds = preparing.scrape_html_ped(horse_id_list, skip=False)\n",
    "peds_20250920 = preparing.get_rawdata_peds(html_files_peds)\n",
    "preparing.update_rawdata(LocalPaths.RAW_PEDS_PATH, peds_20250920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koxyg\\Documents\\GitHub\\MyKeiba-AI_v2\\modules\\preprocessing\\_horse_results_processor.py:33: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[Cols.PRIZE].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#processorの更新\n",
    "horse_info_processor = preprocessing.HorseInfoProcessor(\n",
    "    filepath=LocalPaths.RAW_HORSE_INFO_PATH)\n",
    "horse_results_processor = preprocessing.HorseResultsProcessor(\n",
    "    filepath=LocalPaths.RAW_HORSE_RESULTS_PATH)\n",
    "peds_processor = preprocessing.PedsProcessor(filepath=LocalPaths.RAW_PEDS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルの準備\n",
    "keiba_ai = training.KeibaAIFactory.load('models/20260103/basemodel_2020_2025.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected_n_features: 276 276\n",
      "feature_name_ type: <class 'list'>\n",
      "len(feature_name_): 276\n",
      "feature_name_ head: ['Column_0', 'Column_1', 'Column_2', 'Column_3', 'Column_4', 'Column_5', 'Column_6', 'Column_7', 'Column_8', 'Column_9']\n",
      "datasets.X_train n_features: 276\n",
      "X_train head: ['枠番', '斤量', 'horse_id', 'jockey_id', 'trainer_id', 'owner_id', '年齢', '体重', '体重変化', 'n_horses']\n",
      "manually set feature_name_ failed: AttributeError(\"property 'feature_name_' of 'LGBMClassifier' object has no setter\")\n"
     ]
    }
   ],
   "source": [
    "# --- 診断: モデルのfeature_name_状態を確認 ---\n",
    "import numpy as np\n",
    "model = keiba_ai._KeibaAI__model_wrapper.lgb_model\n",
    "expected_from_attr = getattr(model, 'n_features_in_', None)\n",
    "expected_from_booster = None\n",
    "try:\n",
    "    expected_from_booster = model.booster_.num_feature()\n",
    "except Exception:\n",
    "    pass\n",
    "print('expected_n_features:', expected_from_attr, expected_from_booster)\n",
    "fn = getattr(model, 'feature_name_', None)\n",
    "print('feature_name_ type:', type(fn))\n",
    "try:\n",
    "    print('len(feature_name_):', None if fn is None else len(fn))\n",
    "except Exception as e:\n",
    "    print('len(feature_name_) failed:', e, 'value:', fn)\n",
    "try:\n",
    "    print('feature_name_ head:', None if fn is None else list(fn)[:10])\n",
    "except Exception as e:\n",
    "    print('list(feature_name_) failed:', e)\n",
    "\n",
    "print('datasets.X_train n_features:', keiba_ai.datasets.X_train.shape[1])\n",
    "print('X_train head:', list(keiba_ai.datasets.X_train.columns)[:10])\n",
    "\n",
    "# 手動でfeature_name_を書き換えできるかテスト\n",
    "try:\n",
    "    model.feature_name_ = list(keiba_ai.datasets.X_train.columns)\n",
    "    print('manually set feature_name_ ok')\n",
    "    print('feature_name_ head (after set):', list(model.feature_name_)[:10])\n",
    "except Exception as e:\n",
    "    print('manually set feature_name_ failed:', repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. 前日全レース予想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reloaded policies/training (explicit)\n"
     ]
    }
   ],
   "source": [
    "# 変更点: %autoreload がWindowsの既定エンコーディング(cp932)で読み込み失敗することがあるため、明示的にreload\n",
    "import importlib\n",
    "import modules.policies._score_policy as _score_policy_mod\n",
    "import modules.policies as _policies_mod\n",
    "import modules.training._model_wrapper as _model_wrapper_mod\n",
    "import modules.training._keiba_ai as _keiba_ai_mod\n",
    "import modules.training._keiba_ai_factory as _keiba_ai_factory_mod\n",
    "import modules.training as _training_mod\n",
    "\n",
    "importlib.reload(_score_policy_mod)\n",
    "importlib.reload(_policies_mod)\n",
    "importlib.reload(_model_wrapper_mod)\n",
    "importlib.reload(_keiba_ai_mod)\n",
    "importlib.reload(_keiba_ai_factory_mod)\n",
    "importlib.reload(_training_mod)\n",
    "\n",
    "from modules import policies as policies\n",
    "from modules import training as training\n",
    "score_policy = policies.StdScorePolicy\n",
    "print('reloaded policies/training (explicit)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting race_id_list\n",
      "scraping: https://race.netkeiba.com/top/race_list.html?kaisai_date=20260105\n",
      "24\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# 前日全レース予想用のレースidとレース発走時刻を取得\n",
    "target_race_id_list, target_race_time_list = preparing.scrape_race_id_race_time_list('20260105')\n",
    "print(len(target_race_id_list))\n",
    "print(len(target_race_time_list))\n",
    "yesterday = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットエンコーディング時に「馬の成績」として扱う項目\n",
    "TARGET_COLS = [\n",
    "        HorseResultsCols.RANK,\n",
    "        HorseResultsCols.PRIZE,\n",
    "        HorseResultsCols.RANK_DIFF, \n",
    "        'first_corner',\n",
    "        'final_corner',\n",
    "        'first_to_rank',\n",
    "        'first_to_final',\n",
    "        'final_to_rank',\n",
    "        'time_seconds'\n",
    "        ]\n",
    "# horse_id列と共に、ターゲットエンコーディングの対象にする列\n",
    "GROUP_COLS = [\n",
    "        'course_len',\n",
    "        'race_type',\n",
    "        HorseResultsCols.PLACE\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "スクレイピング完了 - レース202606010211: 16頭立て\n",
      "生データの列数: 18\n",
      "馬番列（index=1）の値: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', 'ショウナンラスボス', '']\n",
      "クリーンアップ前の馬番: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', 'ショウナンラスボス', '']\n",
      "scrape_shutuba_table: 2件の不正な馬番レコードを除去しました\n",
      "除去された馬番: ['ショウナンラスボス', '']\n",
      "  除去レコード 202606010211: 馬番='ショウナンラスボス', 体重='nan'\n",
      "  除去レコード 202606010211: 馬番='', 体重='nan'\n",
      "ShutubaTableProcessor: 馬番クリーンアップ開始（14件のレコード）\n",
      "ShutubaTableProcessor: すべての馬番が有効です\n",
      "separating horse results by date\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035833be5474cdb9e950d12f6e757dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging horse_results\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e092648c6c543959fafd168fc533085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.42, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.42\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.202428034860377e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.202428034860377e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00019328962466044669, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00019328962466044669\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.509934059998701, subsample=1.0 will be ignored. Current value: bagging_fraction=0.509934059998701\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "debug race_id: 202606010211 time: 15:45\n",
      "score nunique: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koxyg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] 指定されたファイルが見つかりません。\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\koxyg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\koxyg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\koxyg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\koxyg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>馬番</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.544244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.537783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.520173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.513145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.420229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.379463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.375325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.354268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.337002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.308253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.282381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.193016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.097093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.074810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    馬番     score\n",
       "5    6  0.544244\n",
       "10  11  0.537783\n",
       "13  14  0.520173\n",
       "2    3  0.513145\n",
       "9   10  0.420229\n",
       "4    5  0.379463\n",
       "12  13  0.375325\n",
       "7    8  0.354268\n",
       "8    9  0.337002\n",
       "0    1  0.308253\n",
       "11  12  0.282381\n",
       "3    4  0.193016\n",
       "1    2  0.097093\n",
       "6    7  0.074810"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 診断: 特徴量の一致率とスコアの分散を1レースで確認 ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 例として最初のレースを対象（必要なら race_id をここで上書き）\n",
    "race_id_debug = target_race_id_list[10]\n",
    "race_time_debug = target_race_time_list[10]\n",
    "filepath = 'data/tmp/shutuba_debug.pickle'\n",
    "today = '2025/12/27'  # 前日予測の取得日（実運用に合わせる）\n",
    "\n",
    "preparing.scrape_shutuba_table(race_id_debug, today, filepath)\n",
    "\n",
    "if yesterday:\n",
    "    pd2 = pd.read_pickle(filepath)\n",
    "    pd2[ResultsCols.WEIGHT_AND_DIFF] = '0(0)'\n",
    "    if 'weather' not in pd2.columns or pd2['weather'].isnull().all():\n",
    "        pd2['weather'] = '晴'\n",
    "    if 'ground_state' not in pd2.columns or pd2['ground_state'].isnull().all():\n",
    "        pd2['ground_state'] = '良'\n",
    "    pd2.to_pickle(filepath)\n",
    "\n",
    "shutuba_table_processor = preprocessing.ShutubaTableProcessor(filepath)\n",
    "shutuba_data_merger = preprocessing.ShutubaDataMerger(\n",
    "    shutuba_table_processor,\n",
    "    horse_results_processor,\n",
    "    horse_info_processor,\n",
    "    peds_processor,\n",
    "    target_cols=TARGET_COLS,\n",
    "    group_cols=GROUP_COLS,\n",
    " )\n",
    "shutuba_data_merger.merge()\n",
    "\n",
    "feature_enginnering_shutuba = preprocessing.FeatureEngineering(shutuba_data_merger) \\\n",
    "    .add_interval()\\\n",
    "    .add_agedays()\\\n",
    "    .dumminize_ground_state()\\\n",
    "    .dumminize_race_type()\\\n",
    "    .dumminize_sex()\\\n",
    "    .dumminize_weather()\\\n",
    "    .encode_horse_id()\\\n",
    "    .encode_jockey_id()\\\n",
    "    .encode_trainer_id()\\\n",
    "    .encode_owner_id()\\\n",
    "    .encode_breeder_id()\\\n",
    "    .dumminize_kaisai()\\\n",
    "    .dumminize_around()\\\n",
    "    .dumminize_race_class()\n",
    "\n",
    "X_debug = feature_enginnering_shutuba.featured_data.drop(['date'], axis=1)\n",
    "\n",
    "# 推論時の列数不一致対策：学習時列に合わせて0埋め整列してからpredict\n",
    "train_cols = list(keiba_ai.datasets.X_train.columns)\n",
    "X_debug_aligned = X_debug.reindex(columns=train_cols, fill_value=0)\n",
    "if ResultsCols.UMABAN in X_debug.columns:\n",
    "    X_debug_aligned[ResultsCols.UMABAN] = X_debug[ResultsCols.UMABAN]\n",
    "\n",
    "model = keiba_ai._KeibaAI__model_wrapper.lgb_model\n",
    "score_debug = score_policy.calc(model, X_debug_aligned)\n",
    "print('debug race_id:', race_id_debug, 'time:', race_time_debug)\n",
    "print('score nunique:', score_debug['score'].nunique())\n",
    "display(score_debug.sort_values('score', ascending=False).head(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一時的に出馬表を保存するパスを指定\n",
    "filepath = 'data/tmp/shutuba.pickle'\n",
    "today = '2022/12/27'\n",
    "\n",
    "for race_id, race_time in zip(target_race_id_list, target_race_time_list):\n",
    "    # 出馬表の取得\n",
    "    preparing.scrape_shutuba_table(race_id, today, filepath)\n",
    "\n",
    "    # 前日予想の場合\n",
    "    if yesterday:\n",
    "        # 前日予想の場合、馬体重を0（0）に補正\n",
    "        pd2 = pd.read_pickle(filepath)\n",
    "        pd2[ResultsCols.WEIGHT_AND_DIFF] = '0(0)'\n",
    "        # 前日予想の場合、天候と馬場状態が公開されていない場合はこちらを有効にする\n",
    "        #pd2['weather'] = '晴'\n",
    "        #pd2['ground_state'] = '良'\n",
    "        pd2.to_pickle(filepath)\n",
    "\n",
    "    # 出馬表の加工\n",
    "    shutuba_table_processor = preprocessing.ShutubaTableProcessor(filepath)\n",
    "\n",
    "    # テーブルのマージ\n",
    "    shutuba_data_merger = preprocessing.ShutubaDataMerger(\n",
    "        shutuba_table_processor,\n",
    "        horse_results_processor,\n",
    "        horse_info_processor,\n",
    "        peds_processor,\n",
    "        target_cols=TARGET_COLS,\n",
    "        group_cols=GROUP_COLS\n",
    "    )\n",
    "    shutuba_data_merger.merge()\n",
    "\n",
    "    # 特徴量エンジニアリング\n",
    "    feature_enginnering_shutuba = preprocessing.FeatureEngineering(shutuba_data_merger) \\\n",
    "        .add_interval()\\\n",
    "        .add_agedays()\\\n",
    "        .dumminize_ground_state()\\\n",
    "        .dumminize_race_type()\\\n",
    "        .dumminize_sex()\\\n",
    "        .dumminize_weather()\\\n",
    "        .encode_horse_id()\\\n",
    "        .encode_jockey_id()\\\n",
    "        .encode_trainer_id()\\\n",
    "        .encode_owner_id()\\\n",
    "        .encode_breeder_id()\\\n",
    "        .dumminize_kaisai()\\\n",
    "        .dumminize_around()\\\n",
    "        .dumminize_race_class()\n",
    "\n",
    "    # 予測\n",
    "    X = feature_enginnering_shutuba.featured_data.drop(['date'], axis=1)\n",
    "\n",
    "    # 当日の出走情報テーブル（前処理前）\n",
    "    df_tmp = shutuba_table_processor.raw_data[:1]\n",
    "\n",
    "    i = 0\n",
    "    for num in list(Master.PLACE_DICT.values()):\n",
    "        if num == race_id[4:6]:\n",
    "            print(list(Master.PLACE_DICT)[i] + race_id[10:12] + 'R ' + race_time + '発走 ' + str(df_tmp.iat[0, 12])\n",
    "                + str(df_tmp.iat[0, 10]) + 'm ' + str(df_tmp.iat[0, 13]) + ' ' + str(df_tmp.iat[0, 15]))\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    print(keiba_ai.calc_score(X, policies.StdScorePolicy).sort_values('score', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 全レース前日予測（特徴量整列は keiba_ai.calc_score に委譲）\n",
    "# ============================================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== 全レース前日予測開始 ===\")\n",
    "print(f\"対象レース数: {len(target_race_id_list)}\")\n",
    "print(f\"前日予想モード: {'ON' if yesterday else 'OFF'}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 一時的に出馬表を保存するパスを指定\n",
    "filepath = 'data/tmp/shutuba.pickle'\n",
    "today = '2025/12/26'\n",
    "\n",
    "# 全レースの予測結果を格納\n",
    "all_predictions = {}\n",
    "error_count = 0\n",
    "\n",
    "for idx, (race_id, race_time) in enumerate(zip(target_race_id_list, target_race_time_list), 1):\n",
    "    try:\n",
    "        print(f\"\\n[{idx}/{len(target_race_id_list)}] レース処理中: {race_id}\")\n",
    "        \n",
    "        # サーバー負荷軽減（必須）\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # 出馬表の取得\n",
    "        preparing.scrape_shutuba_table(race_id, today, filepath)\n",
    "\n",
    "        # 前日予想の場合\n",
    "        if yesterday:\n",
    "            # 前日予想の場合、馬体重を0（0）に補正\n",
    "            pd2 = pd.read_pickle(filepath)\n",
    "            pd2[ResultsCols.WEIGHT_AND_DIFF] = '0(0)'\n",
    "            # 前日予想の場合、天候と馬場状態が公開されていない場合はデフォルト値を設定\n",
    "            if 'weather' not in pd2.columns or pd2['weather'].isnull().all():\n",
    "                pd2['weather'] = '晴'\n",
    "            if 'ground_state' not in pd2.columns or pd2['ground_state'].isnull().all():\n",
    "                pd2['ground_state'] = '良'\n",
    "            pd2.to_pickle(filepath)\n",
    "\n",
    "        # 出馬表の加工\n",
    "        shutuba_table_processor = preprocessing.ShutubaTableProcessor(filepath)\n",
    "\n",
    "        # テーブルのマージ\n",
    "        shutuba_data_merger = preprocessing.ShutubaDataMerger(\n",
    "            shutuba_table_processor,\n",
    "            horse_results_processor,\n",
    "            horse_info_processor,\n",
    "            peds_processor,\n",
    "            target_cols=TARGET_COLS,\n",
    "            group_cols=GROUP_COLS\n",
    ")\n",
    "        shutuba_data_merger.merge()\n",
    "\n",
    "        # 特徴量エンジニアリング\n",
    "        feature_enginnering_shutuba = (\n",
    "            preprocessing.FeatureEngineering(shutuba_data_merger)\n",
    "            .add_interval()\n",
    "            .add_agedays()\n",
    "            .dumminize_ground_state()\n",
    "            .dumminize_race_type()\n",
    "            .dumminize_sex()\n",
    "            .dumminize_weather()\n",
    "            .encode_horse_id()\n",
    "            .encode_jockey_id()\n",
    "            .encode_trainer_id()\n",
    "            .encode_owner_id()\n",
    "            .encode_breeder_id()\n",
    "            .dumminize_kaisai()\n",
    "            .dumminize_around()\n",
    "            .dumminize_race_class()\n",
    "        )\n",
    "\n",
    "        # 予測（整列・型変換は calc_score 側のポリシーで吸収）\n",
    "        X = feature_enginnering_shutuba.featured_data.drop(['date'], axis=1, errors='ignore')\n",
    "\n",
    "        # 当日の出走情報テーブル（前処理前）\n",
    "        df_tmp = shutuba_table_processor.raw_data[:1]\n",
    "\n",
    "        # レース情報の表示\n",
    "        race_info = \"\"\n",
    "        for place_name, num in Master.PLACE_DICT.items():\n",
    "            if num == race_id[4:6]:\n",
    "                race_info = (\n",
    "                    f\"{place_name}{race_id[10:12]}R {race_time}発走 \"\n",
    "                    f\"{df_tmp.iat[0, 12]}{df_tmp.iat[0, 10]}m \"\n",
    "                    f\"{df_tmp.iat[0, 13]} {df_tmp.iat[0, 15]}\"\n",
    "                )\n",
    "                print(race_info)\n",
    "                break\n",
    "\n",
    "        # 予測実行\n",
    "        score_result = keiba_ai.calc_score(X, score_policy).sort_values('score', ascending=False)\n",
    "        print(\"score nunique:\", score_result['score'].nunique())\n",
    "\n",
    "        # 上位馬のみを表示（簡潔化）\n",
    "        top_horses = score_result.head(5)\n",
    "        print(\"TOP5予想:\")\n",
    "        for rank, (_, row) in enumerate(top_horses.iterrows(), 1):\n",
    "            print(f\"  {rank}位: {row['馬番']}番 (スコア: {row['score']:.3f})\")\n",
    "\n",
    "        # 結果を保存\n",
    "        all_predictions[race_id] = {\n",
    "            'race_info': race_info,\n",
    "            'predictions': score_result,\n",
    "            'race_time': race_time\n",
    "        }\n",
    "\n",
    "        print(f\"✅ {race_id} 予測完了\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        print(f\"❌ {race_id} 予測エラー: {str(e)}\")\n",
    "        # エラーが発生したレースをスキップして続行\n",
    "        continue\n",
    "\n",
    "print(f\"\\n=== 全レース予測完了 ===\")\n",
    "print(f\"成功: {len(all_predictions)}/{len(target_race_id_list)} レース\")\n",
    "print(f\"エラー: {error_count} レース\")\n",
    "\n",
    "# 最終結果のサマリー表示\n",
    "if all_predictions:\n",
    "    print(f\"\\n=== 本日の予想結果一覧 ===\")\n",
    "    for race_id, result in all_predictions.items():\n",
    "        print(f\"\\n{result['race_info']}\")\n",
    "        top3 = result['predictions'].head(3)\n",
    "\n",
    "        for rank, (_, row) in enumerate(top3.iterrows(), 1):\n",
    "            print(f\"  {rank}位予想: {row['馬番']}番 (スコア: {row['score']:.3f})\")\n",
    "\n",
    "    print(f\"\\n🎯 {len(all_predictions)}レースの予測が完了しました！\")\n",
    "else:\n",
    "    print(\"❌ 予測に成功したレースがありません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. レース直前データ処理（当日レース予想）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 馬体重の発表されたレースID、レース時刻を取得（レース当日用）\n",
    "target_race_id_list, target_race_time_list = preparing.create_active_race_id_list()\n",
    "\n",
    "# レース時刻順にソート\n",
    "race_data = list(zip(target_race_id_list, target_race_time_list))\n",
    "race_data_sorted = sorted(race_data, key=lambda x: x[1])  # 時刻でソート\n",
    "target_race_id_list = [race_id for race_id, race_time in race_data_sorted]\n",
    "target_race_time_list = [race_time for race_id, race_time in race_data_sorted]\n",
    "\n",
    "print(\"ソート後のレースID:\", target_race_id_list)\n",
    "print(\"ソート後のレース時刻:\", target_race_time_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一時的に出馬表を保存するパスを指定\n",
    "filepath = 'data/tmp/shutuba.pickle'\n",
    "#today = '2022/10/01'\n",
    "today = datetime.datetime.now().date().strftime('%Y/%m/%d')\n",
    "\n",
    "for race_id, race_time in zip(target_race_id_list, target_race_time_list):\n",
    "    # 出馬表の取得\n",
    "    preparing.scrape_shutuba_table(race_id, today, filepath)\n",
    "\n",
    "    # 出馬表の加工\n",
    "    shutuba_table_processor = preprocessing.ShutubaTableProcessor(filepath)\n",
    "    # 馬番クリーンアップを含む前処理を実行\n",
    "    # shutuba_table_processor.process()\n",
    "\n",
    "    # テーブルのマージ\n",
    "    shutuba_data_merger = preprocessing.ShutubaDataMerger(\n",
    "        shutuba_table_processor,\n",
    "        horse_results_processor,\n",
    "        horse_info_processor,\n",
    "        peds_processor,\n",
    "        target_cols=TARGET_COLS,\n",
    "        group_cols=GROUP_COLS\n",
    "    )\n",
    "    shutuba_data_merger.merge()\n",
    "\n",
    "    # 特徴量エンジニアリング\n",
    "    feature_enginnering_shutuba = preprocessing.FeatureEngineering(shutuba_data_merger) \\\n",
    "        .add_interval()\\\n",
    "        .add_agedays()\\\n",
    "        .dumminize_ground_state()\\\n",
    "        .dumminize_race_type()\\\n",
    "        .dumminize_sex()\\\n",
    "        .dumminize_weather()\\\n",
    "        .encode_horse_id()\\\n",
    "        .encode_jockey_id()\\\n",
    "        .encode_trainer_id()\\\n",
    "        .encode_owner_id()\\\n",
    "        .encode_breeder_id()\\\n",
    "        .dumminize_kaisai()\\\n",
    "        .dumminize_around()\\\n",
    "        .dumminize_race_class()\n",
    "\n",
    "    # 予測\n",
    "    X = feature_enginnering_shutuba.featured_data.drop(['date'], axis=1)\n",
    "\n",
    "    # 当日の出走情報テーブル（前処理前）\n",
    "    df_tmp = shutuba_table_processor.raw_data[:1]\n",
    "    #df_tmp['race_type'] tmp.iat[0, 12]\n",
    "    #df_tmp['around'] 13\n",
    "    #df_tmp['weather'] 14\n",
    "    #df_tmp['ground_state'] 15\n",
    "    #df_tmp['race_class']16\n",
    "\n",
    "    i = 0\n",
    "    for num in list(Master.PLACE_DICT.values()):\n",
    "        if num == race_id[4:6]:\n",
    "            print(list(Master.PLACE_DICT)[i] + race_id[10:12] + 'R ' + race_time + '発走 ' + str(df_tmp.iat[0, 12])\n",
    "                + str(df_tmp.iat[0, 10]) + 'm ' + str(df_tmp.iat[0, 13]) + ' ' + str(df_tmp.iat[0, 15]))\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    print(keiba_ai.calc_score(X, policies.StdScorePolicy).sort_values('score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. レース直前データ処理（旧方式）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'data/tmp/shutuba.pickle' #一時的に出馬表を保存するパスを指定\n",
    "preparing.scrape_shutuba_table(race_id_list[0], '2025/9/21', filepath) #馬体重が発表されたら、出馬表を取得\n",
    "shutuba_table_processor = preprocessing.ShutubaTableProcessor(filepath) #出馬表の加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テーブルのマージ\n",
    "shutuba_data_merger = preprocessing.ShutubaDataMerger(\n",
    "    shutuba_table_processor,\n",
    "    horse_results_processor,\n",
    "    horse_info_processor,\n",
    "    peds_processor,\n",
    "    target_cols=TARGET_COLS,\n",
    "    group_cols=GROUP_COLS\n",
    ")\n",
    "\n",
    "shutuba_data_merger.merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量エンジニアリング\n",
    "feature_enginnering_shutuba = preprocessing.FeatureEngineering(shutuba_data_merger)\\\n",
    "    .add_interval()\\\n",
    "    .add_agedays()\\\n",
    "    .dumminize_ground_state()\\\n",
    "    .dumminize_race_type()\\\n",
    "    .dumminize_sex()\\\n",
    "    .dumminize_weather()\\\n",
    "    .encode_horse_id()\\\n",
    "    .encode_jockey_id()\\\n",
    "    .encode_trainer_id()\\\n",
    "    .encode_owner_id()\\\n",
    "    .encode_breeder_id()\\\n",
    "    .dumminize_kaisai()\\\n",
    "    .dumminize_around()\\\n",
    "    .dumminize_race_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測（学習列に完全整列＋NaN防止）\n",
    "from modules.constants import ResultsCols\n",
    "import numpy as np\n",
    "\n",
    "# 1) 学習で実際に使った列を取得\n",
    "train_cols = keiba_ai.datasets.X_train.columns\n",
    "\n",
    "# 2) 予測用特徴量（date/rankは除外）\n",
    "X_feat = feature_enginnering_shutuba.featured_data.drop(['date', 'rank'], axis=1, errors='ignore')\n",
    "\n",
    "# 3) 学習列に揃える（不足は0、余剰は落とす）\n",
    "X_feat = X_feat.reindex(columns=train_cols, fill_value=0)\n",
    "\n",
    "# 4) 数値化とNaN/inf対策\n",
    "for c in X_feat.columns:\n",
    "    if getattr(X_feat[c].dtype, 'name', '') == 'category':\n",
    "        X_feat[c] = X_feat[c].cat.codes\n",
    "X_feat = X_feat.astype(float).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "# 5) 表示用に馬番を付与（ポリシー側で自動除外）\n",
    "X_for_policy = X_feat.copy()\n",
    "if ResultsCols.UMABAN in feature_enginnering_shutuba.featured_data.columns:\n",
    "    X_for_policy[ResultsCols.UMABAN] = feature_enginnering_shutuba.featured_data[ResultsCols.UMABAN].values\n",
    "\n",
    "# 6) 予測\n",
    "score_result = keiba_ai.calc_score(X_for_policy, policies.StdScorePolicy).sort_values('score', ascending=False)\n",
    "score_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 付録\n",
    "騎手勝率無し VS 有りの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_returns_df = pd.read_pickle('models/20251226/tansho_no_jockey_std_0_3p5.pickle')\n",
    "\n",
    "\n",
    "\n",
    "# old_returns_df と returns_df の結果を重ねてプロットして比較\n",
    "\n",
    "plot_single_threshold_compare(\n",
    "\n",
    "    old_returns_df, returns_df, N_SAMPLES,\n",
    "\n",
    "    label1='no_jockey(std,0-3.5)', label2='with_jockey(std,0-3.5)'\n",
    "\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_bets / n_races がどの閾値から崩れるか確認（特徴量なし vs あり）\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def _plot_counts(df, label, ax_bets, ax_races):\n",
    "\n",
    "    ax_bets.plot(df.index, df['n_bets'], label=label)\n",
    "\n",
    "    ax_races.plot(df.index, df['n_races'], label=label)\n",
    "\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6), dpi=100, sharex=True)\n",
    "\n",
    "\n",
    "\n",
    "_plot_counts(old_returns_df, 'no_jockey', ax1, ax2)\n",
    "\n",
    "_plot_counts(returns_df, 'with_jockey', ax1, ax2)\n",
    "\n",
    "\n",
    "\n",
    "ax1.set_ylabel('n_bets')\n",
    "\n",
    "ax1.grid(True)\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "\n",
    "ax2.set_ylabel('n_races')\n",
    "\n",
    "ax2.set_xlabel('threshold')\n",
    "\n",
    "ax2.grid(True)\n",
    "\n",
    "ax2.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print('--- tail(10): no_jockey ---')\n",
    "\n",
    "display(old_returns_df.tail(10)[['n_bets','n_races','return_rate']])\n",
    "\n",
    "print('--- tail(10): with_jockey ---')\n",
    "\n",
    "display(returns_df.tail(10)[['n_bets','n_races','return_rate']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_races >= 100 に限定した return_rate の最大値（特徴量なし vs あり）\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "MIN_RACES = 100\n",
    "\n",
    "\n",
    "\n",
    "def best_under_constraint(df, min_races: int):\n",
    "\n",
    "    d = df[df['n_races'] >= min_races].copy()\n",
    "\n",
    "    if len(d) == 0:\n",
    "\n",
    "        return None, None, d\n",
    "\n",
    "    best_thr = float(d['return_rate'].idxmax())\n",
    "\n",
    "    best_rr = float(d.loc[best_thr, 'return_rate'])\n",
    "\n",
    "    return best_thr, best_rr, d\n",
    "\n",
    "\n",
    "\n",
    "thr0, rr0, d0 = best_under_constraint(old_returns_df, MIN_RACES)\n",
    "\n",
    "thr1, rr1, d1 = best_under_constraint(returns_df, MIN_RACES)\n",
    "\n",
    "\n",
    "\n",
    "print(f'MIN_RACES = {MIN_RACES}')\n",
    "\n",
    "print('--- no_jockey ---')\n",
    "\n",
    "if thr0 is None:\n",
    "\n",
    "    print('条件を満たすthresholdがありません')\n",
    "\n",
    "else:\n",
    "\n",
    "    print('best threshold:', thr0)\n",
    "\n",
    "    print('best return_rate:', rr0)\n",
    "\n",
    "    display(old_returns_df.loc[[thr0], ['n_bets','n_races','return_rate','std']])\n",
    "\n",
    "\n",
    "\n",
    "print('--- with_jockey ---')\n",
    "\n",
    "if thr1 is None:\n",
    "\n",
    "    print('条件を満たすthresholdがありません')\n",
    "\n",
    "else:\n",
    "\n",
    "    print('best threshold:', thr1)\n",
    "\n",
    "    print('best return_rate:', rr1)\n",
    "\n",
    "    display(returns_df.loc[[thr1], ['n_bets','n_races','return_rate','std']])\n",
    "\n",
    "\n",
    "\n",
    "# 参考: 上位5件も表示\n",
    "\n",
    "if len(d0) > 0:\n",
    "\n",
    "    print('top5(no_jockey)')\n",
    "\n",
    "    display(d0.sort_values('return_rate', ascending=False).head(5)[['n_bets','n_races','return_rate','std']])\n",
    "\n",
    "if len(d1) > 0:\n",
    "\n",
    "    print('top5(with_jockey)')\n",
    "\n",
    "    display(d1.sort_values('return_rate', ascending=False).head(5)[['n_bets','n_races','return_rate','std']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上位帯の安定性チェック: MIN_RACES を変えてベストを比較\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "MIN_RACES_LIST = [100, 200, 500]\n",
    "\n",
    "\n",
    "\n",
    "def best_row(df: pd.DataFrame, min_races: int, label: str) -> dict:\n",
    "\n",
    "    d = df[df['n_races'] >= min_races]\n",
    "\n",
    "    if len(d) == 0:\n",
    "\n",
    "        return {\n",
    "\n",
    "            'model': label,\n",
    "\n",
    "            'min_races': min_races,\n",
    "\n",
    "            'best_threshold': None,\n",
    "\n",
    "            'best_return_rate': None,\n",
    "\n",
    "            'n_races': 0,\n",
    "\n",
    "            'n_bets': 0,\n",
    "\n",
    "            'std': None,\n",
    "\n",
    "        }\n",
    "\n",
    "    thr = float(d['return_rate'].idxmax())\n",
    "\n",
    "    row = df.loc[thr]\n",
    "\n",
    "    return {\n",
    "\n",
    "        'model': label,\n",
    "\n",
    "        'min_races': min_races,\n",
    "\n",
    "        'best_threshold': thr,\n",
    "\n",
    "        'best_return_rate': float(row['return_rate']),\n",
    "\n",
    "        'n_races': int(row['n_races']),\n",
    "\n",
    "        'n_bets': int(row['n_bets']),\n",
    "\n",
    "        'std': float(row['std']),\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for m in MIN_RACES_LIST:\n",
    "\n",
    "    rows.append(best_row(old_returns_df, m, 'no_jockey'))\n",
    "\n",
    "    rows.append(best_row(returns_df, m, 'with_jockey'))\n",
    "\n",
    "\n",
    "\n",
    "stability_df = pd.DataFrame(rows).sort_values(['min_races', 'model']).reset_index(drop=True)\n",
    "\n",
    "display(stability_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 運用向けチェック: min_races=500 の範囲で return_rate > 1.0 は存在するか\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "MIN_RACES_OPS = 500\n",
    "\n",
    "RR_TARGET = 1.0\n",
    "\n",
    "\n",
    "\n",
    "def points_over_1(df: pd.DataFrame, label: str, min_races: int, rr_target: float):\n",
    "\n",
    "    d = df[df['n_races'] >= min_races].copy()\n",
    "\n",
    "    over = d[d['return_rate'] > rr_target].copy()\n",
    "\n",
    "    print(f'[{label}] min_races>={min_races} の点数: {len(d)}')\n",
    "\n",
    "    print(f'[{label}] return_rate>{rr_target} の点数: {len(over)}')\n",
    "\n",
    "    if len(over) == 0:\n",
    "\n",
    "        return\n",
    "\n",
    "    print(f'[{label}] threshold 範囲: {float(over.index.min())} 〜 {float(over.index.max())}')\n",
    "\n",
    "    display(over.sort_values('return_rate', ascending=False).head(10)[['n_bets','n_races','return_rate','std']])\n",
    "\n",
    "\n",
    "\n",
    "points_over_1(old_returns_df, 'no_jockey', MIN_RACES_OPS, RR_TARGET)\n",
    "\n",
    "points_over_1(returns_df, 'with_jockey', MIN_RACES_OPS, RR_TARGET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 旧モデル(特徴量なし)を新仕様(StdScorePolicy)で再計算して保存 ===\n",
    "\n",
    "import os\n",
    "\n",
    "import traceback\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from modules import training, policies, preprocessing, simulation\n",
    "\n",
    "from modules.constants import LocalPaths\n",
    "\n",
    "\n",
    "\n",
    "# 旧モデル（特徴量なし）をロード\n",
    "\n",
    "keiba_ai_no_jockey = training.KeibaAIFactory.load('models/20251223/basemodel_2020_2025.pickle')\n",
    "\n",
    "keiba_ai_no_jockey.set_params(keiba_ai_no_jockey.get_params())\n",
    "\n",
    "\n",
    "\n",
    "# Simulator / ReturnProcessor が未準備なら用意\n",
    "\n",
    "try:\n",
    "\n",
    "    simulator\n",
    "\n",
    "except NameError:\n",
    "\n",
    "    return_processor = preprocessing.ReturnProcessor(filepath=LocalPaths.RAW_RETURN_TABLES_PATH)\n",
    "\n",
    "    simulator = simulation.Simulator(return_processor)\n",
    "\n",
    "\n",
    "\n",
    "# 0.0〜3.5 を両端含めてスイープ\n",
    "\n",
    "T_RANGE_OLD = [0.0, 3.5]\n",
    "\n",
    "N_SAMPLES_OLD = 100\n",
    "\n",
    "\n",
    "\n",
    "score_table_old = keiba_ai_no_jockey.calc_score(keiba_ai_no_jockey.datasets.X_test, policies.StdScorePolicy)\n",
    "\n",
    "\n",
    "\n",
    "returns_old = {}\n",
    "\n",
    "for i in tqdm(range(N_SAMPLES_OLD)):\n",
    "\n",
    "    if N_SAMPLES_OLD > 1:\n",
    "\n",
    "        threshold = T_RANGE_OLD[0] + (T_RANGE_OLD[1] - T_RANGE_OLD[0]) * i / (N_SAMPLES_OLD - 1)\n",
    "\n",
    "    else:\n",
    "\n",
    "        threshold = T_RANGE_OLD[0]\n",
    "\n",
    "    try:\n",
    "\n",
    "        actions_old = keiba_ai_no_jockey.decide_action(\n",
    "\n",
    "            score_table_old,\n",
    "\n",
    "            policies.BetPolicyTansho,\n",
    "\n",
    "            threshold=threshold,\n",
    "\n",
    "        )\n",
    "\n",
    "        returns_old[threshold] = simulator.calc_returns(actions_old)\n",
    "\n",
    "    except Exception:\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "returns_old_df = pd.DataFrame.from_dict(returns_old, orient='index').sort_index()\n",
    "\n",
    "returns_old_df.index.name = 'threshold'\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs('models/20251226', exist_ok=True)\n",
    "\n",
    "returns_old_path = 'models/20251226/tansho_no_jockey_std_0_3p5.pickle'\n",
    "\n",
    "returns_old_df.to_pickle(returns_old_path)\n",
    "\n",
    "\n",
    "\n",
    "print('saved:', returns_old_path)\n",
    "\n",
    "print('index min/max/len:', float(returns_old_df.index.min()), float(returns_old_df.index.max()), len(returns_old_df))\n",
    "\n",
    "returns_old_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. 過去日（2025/12/21）の当日予想→券種別回収率シミュレーション\n",
    "\n",
    "このセクションは 6.3 の「当日予想」セルと同じ処理（出馬表→結合→特徴量→スコア）を、**過去日**の指定 race_id に対して実行し、指定ルールで馬券を買ったと仮定した回収率を計算します。\n",
    "\n",
    "注意:\n",
    "- `DataMerger` 側の `date < 対象日` フィルタ（馬の過去成績集計）に依存してリークを避けます。\n",
    "- ただし **使用モデルが対象日のデータを学習に含んでいる場合**、評価は楽観的になり得ます。\n",
    "- 払戻テーブル（return_tables）に race_id が無いと、そのレースは集計から除外されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from modules import preparing, preprocessing, policies, training, simulation\n",
    "from modules.constants import LocalPaths, ResultsCols\n",
    "\n",
    "# --- 対象レース（2025/12/21 全12R x 3開催 = 36レース） ---\n",
    "SIM_DATE_STR = '2025/12/21'  # scrape_shutuba_table の date 引数（yyyy/mm/dd）\n",
    "BASE_RACE_IDS = [\n",
    "    '202506050601',\n",
    "    '202509050601',\n",
    "    '202507050601',\n",
    "]\n",
    "\n",
    "race_id_list = []\n",
    "for base in BASE_RACE_IDS:\n",
    "    prefix = base[:-2]  # 末尾\"01\"を除いた部分\n",
    "    race_id_list.extend([prefix + f'{i:02d}' for i in range(1, 13)])\n",
    "race_id_list = sorted(set(race_id_list))\n",
    "print('race_id_list size:', len(race_id_list))\n",
    "\n",
    "# --- 使用モデル ---\n",
    "MODEL_PATH = 'models/20251226/basemodel_2020_2025.pickle'\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    # 実在する basemodel を自動選択（ファイル名が変わっても動くようにする）\n",
    "    candidates = [\n",
    "        os.path.join('models', '20251226', f)\n",
    "        for f in os.listdir(os.path.join('models', '20251226'))\n",
    "        if f.startswith('basemodel_') and f.endswith('.pickle')\n",
    "    ]\n",
    "    if len(candidates) == 0:\n",
    "        raise FileNotFoundError('basemodel_*.pickle が models/20251226 に見つかりません')\n",
    "    MODEL_PATH = sorted(candidates)[-1]\n",
    "    print('[WARN] 指定モデルが無いため自動選択:', MODEL_PATH)\n",
    "\n",
    "keiba_ai = training.KeibaAIFactory.load(MODEL_PATH)\n",
    "score_policy = policies.StdScorePolicy\n",
    "\n",
    "# --- 前処理済みテーブル（最新 raw を使用） ---\n",
    "horse_results_processor = preprocessing.HorseResultsProcessor(filepath=LocalPaths.RAW_HORSE_RESULTS_PATH)\n",
    "horse_info_processor   = preprocessing.HorseInfoProcessor(filepath=LocalPaths.RAW_HORSE_INFO_PATH)\n",
    "peds_processor         = preprocessing.PedsProcessor(filepath=LocalPaths.RAW_PEDS_PATH)\n",
    "return_processor       = preprocessing.ReturnProcessor(filepath=LocalPaths.RAW_RETURN_TABLES_PATH)\n",
    "simulator              = simulation.Simulator(return_processor)\n",
    "\n",
    "# 6章の既存変数が無い場合は最低限の空で進める（特徴量が減るだけ）\n",
    "if 'TARGET_COLS' not in globals():\n",
    "    TARGET_COLS = []\n",
    "    print('[WARN] TARGET_COLS が未定義なので空で進めます（特徴量が減ります）。')\n",
    "if 'GROUP_COLS' not in globals():\n",
    "    GROUP_COLS = []\n",
    "    print('[WARN] GROUP_COLS が未定義なので空で進めます（特徴量が減ります）。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) 出馬表スクレイピング（36レース） ---\n",
    "out_dir = os.path.join(LocalPaths.TMP_DIR, 'shutuba_20251221')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "created = 0\n",
    "skipped = 0\n",
    "for rid in race_id_list:\n",
    "    out_path = os.path.join(out_dir, f'{rid}.pickle')\n",
    "    if os.path.exists(out_path):\n",
    "        skipped += 1\n",
    "        continue\n",
    "    time.sleep(1)  # サーバー負荷軽減\n",
    "    preparing.scrape_shutuba_table(rid, SIM_DATE_STR, out_path)\n",
    "    created += 1\n",
    "\n",
    "print('scrape done. created=', created, 'skipped(existing)=', skipped, 'dir=', out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) 出馬表pickleを結合 → 前処理 → マージ → 特徴量 ---\n",
    "paths = [os.path.join(out_dir, f'{rid}.pickle') for rid in race_id_list if os.path.exists(os.path.join(out_dir, f'{rid}.pickle'))]\n",
    "print('available shutuba pickles:', len(paths), '/', len(race_id_list))\n",
    "\n",
    "if len(paths) == 0:\n",
    "    raise RuntimeError('出馬表pickleが1件もありません。先にスクレイピングセルを実行してください。')\n",
    "\n",
    "raw_list = [pd.read_pickle(p) for p in paths]\n",
    "shutuba_raw = pd.concat(raw_list, axis=0, ignore_index=False)\n",
    "shutuba_all_path = os.path.join(LocalPaths.TMP_DIR, 'shutuba_20251221_all.pickle')\n",
    "shutuba_raw.to_pickle(shutuba_all_path)\n",
    "print('saved:', shutuba_all_path, 'rows=', len(shutuba_raw))\n",
    "\n",
    "# 出馬表の加工（race_idメタ列を保持するようにProcessor側は修正済み）\n",
    "shutuba_table_processor = preprocessing.ShutubaTableProcessor(shutuba_all_path)\n",
    "\n",
    "# テーブルのマージ\n",
    "shutuba_data_merger = preprocessing.ShutubaDataMerger(\n",
    "    shutuba_table_processor,\n",
    "    horse_results_processor,\n",
    "    horse_info_processor,\n",
    "    peds_processor,\n",
    "    target_cols=TARGET_COLS,\n",
    "    group_cols=GROUP_COLS,\n",
    " )\n",
    "shutuba_data_merger.merge()\n",
    "\n",
    "# 特徴量エンジニアリング\n",
    "feature_enginnering_shutuba = preprocessing.FeatureEngineering(shutuba_data_merger)\\\n",
    "    .add_interval()\\\n",
    "    .add_agedays()\\\n",
    "    .dumminize_ground_state()\\\n",
    "    .dumminize_race_type()\\\n",
    "    .dumminize_sex()\\\n",
    "    .dumminize_weather()\\\n",
    "    .encode_horse_id()\\\n",
    "    .encode_jockey_id()\\\n",
    "    .encode_trainer_id()\\\n",
    "    .encode_owner_id()\\\n",
    "    .encode_breeder_id()\\\n",
    "    .dumminize_kaisai()\\\n",
    "    .dumminize_around()\\\n",
    "    .dumminize_race_class()\n",
    "\n",
    "X_shutuba = feature_enginnering_shutuba.featured_data\n",
    "print('X_shutuba shape:', X_shutuba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.5) コード修正を反映（モジュールreload） ---\n",
    "import importlib\n",
    "import modules.policies._score_policy as _score_policy_mod\n",
    "import modules.policies as _policies_mod\n",
    "\n",
    "importlib.reload(_score_policy_mod)\n",
    "importlib.reload(_policies_mod)\n",
    "\n",
    "from modules import policies as policies  # 再import\n",
    "score_policy = policies.StdScorePolicy\n",
    "print('reloaded policies._score_policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) スコア算出（レース内標準化） ---\n",
    "score_table_20251221 = keiba_ai.calc_score(X_shutuba, score_policy)\n",
    "print('score_table shape:', score_table_20251221.shape)\n",
    "display(score_table_20251221.head())\n",
    "\n",
    "# 便利カラム（型）を整える\n",
    "if ResultsCols.UMABAN in score_table_20251221.columns:\n",
    "    score_table_20251221[ResultsCols.UMABAN] = pd.to_numeric(score_table_20251221[ResultsCols.UMABAN], errors='coerce').astype('Int64')\n",
    "score_table_20251221['race_id'] = score_table_20251221['race_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4) 券種別ルールで actions を作る ---\n",
    "def _top_umaban(df_1race: pd.DataFrame, n: int) -> list[int]:\n",
    "    df = df_1race.sort_values('score', ascending=False)\n",
    "    uma = df[ResultsCols.UMABAN].dropna().astype(int).tolist()\n",
    "    return uma[:n]\n",
    "\n",
    "def build_actions_by_ticket(score_table: pd.DataFrame) -> dict[str, dict]:\n",
    "    actions_by_ticket: dict[str, dict] = {\n",
    "        'tansho': {},\n",
    "        'fukusho': {},\n",
    "        'umaren': {},\n",
    "        'umatan': {},\n",
    "        'wide': {},\n",
    "        'sanrenpuku': {},\n",
    "        'sanrentan': {},\n",
    "    }\n",
    "    for rid, df_r in score_table.groupby('race_id'):\n",
    "        n_horses = len(df_r)\n",
    "        # ルールに従う（買い目が成立しない場合は空にする）\n",
    "        top2 = _top_umaban(df_r, 2) if n_horses >= 1 else []\n",
    "        top3 = _top_umaban(df_r, 3) if n_horses >= 1 else []\n",
    "        top4 = _top_umaban(df_r, 4) if n_horses >= 1 else []\n",
    "        if n_horses <= 13:\n",
    "            topN_tri = max(3, (n_horses + 1) // 2)  # 上位半分（切り上げ）\n",
    "        else:\n",
    "            topN_tri = 7\n",
    "        top_tri = _top_umaban(df_r, topN_tri) if n_horses >= 1 else []\n",
    "\n",
    "        actions_by_ticket['tansho'][rid] = {'tansho': top2}\n",
    "        actions_by_ticket['fukusho'][rid] = {'fukusho': top3}\n",
    "        actions_by_ticket['umaren'][rid] = {'umaren': top4 if len(top4) >= 2 else []}\n",
    "        actions_by_ticket['umatan'][rid] = {'umatan': top4 if len(top4) >= 2 else []}\n",
    "        actions_by_ticket['wide'][rid] = {'wide': top4 if len(top4) >= 2 else []}\n",
    "        actions_by_ticket['sanrenpuku'][rid] = {'sanrenpuku': top_tri if len(top_tri) >= 3 else []}\n",
    "        actions_by_ticket['sanrentan'][rid] = {'sanrentan': top_tri if len(top_tri) >= 3 else []}\n",
    "    return actions_by_ticket\n",
    "\n",
    "actions_by_ticket = build_actions_by_ticket(score_table_20251221)\n",
    "print('tickets:', list(actions_by_ticket.keys()))\n",
    "print('races in actions:', len(actions_by_ticket['tansho']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5) 券種別に回収率を集計（払戻テーブルに無いrace_idは自動スキップ） ---\n",
    "rows = []\n",
    "detail_by_ticket = {}\n",
    "for ticket, actions_ticket in actions_by_ticket.items():\n",
    "    returns_per_race = simulator.calc_returns_per_race(actions_ticket)\n",
    "    returns = simulator.calc_returns(actions_ticket)\n",
    "    detail_by_ticket[ticket] = returns_per_race.sort_index()\n",
    "    skipped_races = len(actions_ticket) - returns_per_race.index.nunique()\n",
    "    rows.append({\n",
    "        'ticket': ticket,\n",
    "        'n_races_target': len(actions_ticket),\n",
    "        'n_races_in_return_tables': returns_per_race.index.nunique(),\n",
    "        'n_races_skipped': skipped_races,\n",
    "        **returns,\n",
    "    })\n",
    "\n",
    "summary_20251221 = pd.DataFrame(rows).sort_values('ticket').reset_index(drop=True)\n",
    "display(summary_20251221)\n",
    "\n",
    "# 例: 単勝のレース別明細\n",
    "display(detail_by_ticket['tansho'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6.5) 払戻（return_tables）欠損の補完（欠損race_idのみ取得） ---\n",
    "import pandas as pd\n",
    "\n",
    "from modules.constants import LocalPaths\n",
    "from modules.preparing._scrape_html import scrape_html_race\n",
    "from modules.preparing._get_rawdata import get_rawdata_return, update_rawdata\n",
    "from modules.preprocessing._return_processor import ReturnProcessor\n",
    "from modules.simulation._simulator import Simulator\n",
    "\n",
    "# return_tables に存在する race_id を抽出（MultiIndexにも対応）\n",
    "raw_return_tables = return_processor.raw_data\n",
    "if getattr(raw_return_tables.index, 'nlevels', 1) > 1:\n",
    "    existing_race_ids = set(raw_return_tables.index.get_level_values(0).astype(str))\n",
    "else:\n",
    "    existing_race_ids = set(raw_return_tables.index.astype(str))\n",
    "\n",
    "missing_race_ids = sorted(set(map(str, race_id_list)) - existing_race_ids)\n",
    "print(f'missing race_id in return_tables: {len(missing_race_ids)}')\n",
    "if len(missing_race_ids) > 0:\n",
    "    display(pd.Series(missing_race_ids, name='missing_race_id').head(20))\n",
    "\n",
    "    # 1) race html 取得（欠損分のみ）\n",
    "    updated_html_paths = scrape_html_race(missing_race_ids, skip=False)\n",
    "\n",
    "    # 2) raw return_tables 作成→既存pickleへ追記\n",
    "    new_return_df = get_rawdata_return(updated_html_paths)\n",
    "    _ = update_rawdata(LocalPaths.RAW_RETURN_TABLES_PATH, new_return_df, mode='update')\n",
    "\n",
    "    # 3) ReturnProcessor/Simulator を作り直し\n",
    "    return_processor = ReturnProcessor(LocalPaths.RAW_RETURN_TABLES_PATH)\n",
    "    simulator = Simulator(return_processor)\n",
    "\n",
    "    print('return_tables updated. 再集計したい場合は、直前の回収率集計セル（6.5）を再実行してください。')\n",
    "else:\n",
    "    print('欠損はありません（このまま回収率集計結果を採用できます）。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6. 過去日（2025/12/20）の当日予想→券種別回収率シミュレーション\n",
    "\n",
    "- 対象: 2025/12/20（土）\n",
    "- race_id: 202506050501~12 / 202509050501~12 / 202507050501~12（合計36R）\n",
    "- 6.5 と同じルールで actions を生成して回収率を集計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0) 対象レース設定（2025/12/20 全12R x 3開催 = 36レース） ---\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from modules import preparing, preprocessing, policies, training, simulation\n",
    "from modules.constants import LocalPaths, ResultsCols\n",
    "\n",
    "SIM_DATE_STR_20251220 = '2025/12/20'  # scrape_shutuba_table の date 引数（yyyy/mm/dd）\n",
    "BASE_RACE_IDS_20251220 = [\n",
    "    '202506050501',\n",
    "    '202509050501',\n",
    "    '202507050501',\n",
    "]\n",
    "\n",
    "race_id_list_20251220: list[str] = []\n",
    "for base in BASE_RACE_IDS_20251220:\n",
    "    prefix = base[:-2]\n",
    "    race_id_list_20251220.extend([prefix + f'{i:02d}' for i in range(1, 13)])\n",
    "race_id_list_20251220 = sorted(set(race_id_list_20251220))\n",
    "print('race_id_list_20251220 size:', len(race_id_list_20251220))\n",
    "\n",
    "# 6.5を実行していない環境でも動くように最低限を初期化\n",
    "if 'keiba_ai' not in globals():\n",
    "    MODEL_PATH = 'models/20251226/basemodel_2020_2025.pickle'\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        candidates = [\n",
    "            os.path.join('models', '20251226', f)\n",
    "            for f in os.listdir(os.path.join('models', '20251226'))\n",
    "            if f.startswith('basemodel_') and f.endswith('.pickle')\n",
    "        ]\n",
    "        if len(candidates) == 0:\n",
    "            raise FileNotFoundError('basemodel_*.pickle が models/20251226 に見つかりません')\n",
    "        MODEL_PATH = sorted(candidates)[-1]\n",
    "        print('[WARN] 指定モデルが無いため自動選択:', MODEL_PATH)\n",
    "\n",
    "    keiba_ai = training.KeibaAIFactory.load(MODEL_PATH)\n",
    "\n",
    "if 'score_policy' not in globals():\n",
    "    score_policy = policies.StdScorePolicy\n",
    "\n",
    "# raw processors / simulator\n",
    "horse_results_processor = preprocessing.HorseResultsProcessor(filepath=LocalPaths.RAW_HORSE_RESULTS_PATH)\n",
    "horse_info_processor   = preprocessing.HorseInfoProcessor(filepath=LocalPaths.RAW_HORSE_INFO_PATH)\n",
    "peds_processor         = preprocessing.PedsProcessor(filepath=LocalPaths.RAW_PEDS_PATH)\n",
    "return_processor       = preprocessing.ReturnProcessor(filepath=LocalPaths.RAW_RETURN_TABLES_PATH)\n",
    "simulator              = simulation.Simulator(return_processor)\n",
    "\n",
    "if 'TARGET_COLS' not in globals():\n",
    "    TARGET_COLS = []\n",
    "    print('[WARN] TARGET_COLS が未定義なので空で進めます（特徴量が減ります）。')\n",
    "if 'GROUP_COLS' not in globals():\n",
    "    GROUP_COLS = []\n",
    "    print('[WARN] GROUP_COLS が未定義なので空で進めます（特徴量が減ります）。')\n",
    "\n",
    "out_dir_20251220 = os.path.join(LocalPaths.TMP_DIR, 'shutuba_20251220')\n",
    "os.makedirs(out_dir_20251220, exist_ok=True)\n",
    "print('out_dir_20251220:', out_dir_20251220)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) 出馬表スクレイピング（36レース） ---\n",
    "created = 0\n",
    "skipped = 0\n",
    "for rid in race_id_list_20251220:\n",
    "    out_path = os.path.join(out_dir_20251220, f'{rid}.pickle')\n",
    "    if os.path.exists(out_path):\n",
    "        skipped += 1\n",
    "        continue\n",
    "    time.sleep(1)  # サーバー負荷軽減\n",
    "    preparing.scrape_shutuba_table(rid, SIM_DATE_STR_20251220, out_path)\n",
    "    created += 1\n",
    "\n",
    "print('scrape done. created=', created, 'skipped(existing)=', skipped, 'dir=', out_dir_20251220)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) 出馬表pickleを結合 → 前処理 → マージ → 特徴量 ---\n",
    "paths = [\n",
    "    os.path.join(out_dir_20251220, f'{rid}.pickle')\n",
    "    for rid in race_id_list_20251220\n",
    "    if os.path.exists(os.path.join(out_dir_20251220, f'{rid}.pickle'))\n",
    "]\n",
    "print('available shutuba pickles:', len(paths), '/', len(race_id_list_20251220))\n",
    "\n",
    "if len(paths) == 0:\n",
    "    raise RuntimeError('出馬表pickleが1件もありません。先にスクレイピングセルを実行してください。')\n",
    "\n",
    "raw_list = [pd.read_pickle(p) for p in paths]\n",
    "shutuba_raw_20251220 = pd.concat(raw_list, axis=0, ignore_index=False)\n",
    "shutuba_all_path_20251220 = os.path.join(LocalPaths.TMP_DIR, 'shutuba_20251220_all.pickle')\n",
    "shutuba_raw_20251220.to_pickle(shutuba_all_path_20251220)\n",
    "print('saved:', shutuba_all_path_20251220, 'rows=', len(shutuba_raw_20251220))\n",
    "\n",
    "shutuba_table_processor_20251220 = preprocessing.ShutubaTableProcessor(shutuba_all_path_20251220)\n",
    "\n",
    "shutuba_data_merger_20251220 = preprocessing.ShutubaDataMerger(\n",
    "    shutuba_table_processor_20251220,\n",
    "    horse_results_processor,\n",
    "    horse_info_processor,\n",
    "    peds_processor,\n",
    "    target_cols=TARGET_COLS,\n",
    "    group_cols=GROUP_COLS,\n",
    ")\n",
    "shutuba_data_merger_20251220.merge()\n",
    "\n",
    "feature_enginnering_shutuba_20251220 = preprocessing.FeatureEngineering(shutuba_data_merger_20251220)\\\n",
    "    .add_interval()\\\n",
    "    .add_agedays()\\\n",
    "    .dumminize_ground_state()\\\n",
    "    .dumminize_race_type()\\\n",
    "    .dumminize_sex()\\\n",
    "    .dumminize_weather()\\\n",
    "    .encode_horse_id()\\\n",
    "    .encode_jockey_id()\\\n",
    "    .encode_trainer_id()\\\n",
    "    .encode_owner_id()\\\n",
    "    .encode_breeder_id()\\\n",
    "    .dumminize_kaisai()\\\n",
    "    .dumminize_around()\\\n",
    "    .dumminize_race_class()\n",
    "\n",
    "X_shutuba_20251220 = feature_enginnering_shutuba_20251220.featured_data\n",
    "print('X_shutuba_20251220 shape:', X_shutuba_20251220.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) スコア算出（レース内標準化） ---\n",
    "score_table_20251220 = keiba_ai.calc_score(X_shutuba_20251220, score_policy)\n",
    "print('score_table_20251220 shape:', score_table_20251220.shape)\n",
    "display(score_table_20251220.head())\n",
    "\n",
    "if ResultsCols.UMABAN in score_table_20251220.columns:\n",
    "    score_table_20251220[ResultsCols.UMABAN] = pd.to_numeric(score_table_20251220[ResultsCols.UMABAN], errors='coerce').astype('Int64')\n",
    "score_table_20251220['race_id'] = score_table_20251220['race_id'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4) 券種別ルールで actions を作る → 回収率を集計 ---\n",
    "# 6.5で定義済みなら再利用、無ければここで定義\n",
    "if 'build_actions_by_ticket' not in globals():\n",
    "    def _top_umaban(df_1race: pd.DataFrame, n: int) -> list[int]:\n",
    "        df = df_1race.sort_values('score', ascending=False)\n",
    "        uma = df[ResultsCols.UMABAN].dropna().astype(int).tolist()\n",
    "        return uma[:n]\n",
    "\n",
    "    def build_actions_by_ticket(score_table: pd.DataFrame) -> dict[str, dict]:\n",
    "        actions_by_ticket: dict[str, dict] = {\n",
    "            'tansho': {},\n",
    "            'fukusho': {},\n",
    "            'umaren': {},\n",
    "            'umatan': {},\n",
    "            'wide': {},\n",
    "            'sanrenpuku': {},\n",
    "            'sanrentan': {},\n",
    "        }\n",
    "        for rid, df_r in score_table.groupby('race_id'):\n",
    "            n_horses = len(df_r)\n",
    "            top2 = _top_umaban(df_r, 2) if n_horses >= 1 else []\n",
    "            top3 = _top_umaban(df_r, 3) if n_horses >= 1 else []\n",
    "            top4 = _top_umaban(df_r, 4) if n_horses >= 1 else []\n",
    "            if n_horses <= 13:\n",
    "                topN_tri = max(3, (n_horses + 1) // 2)\n",
    "            else:\n",
    "                topN_tri = 7\n",
    "            top_tri = _top_umaban(df_r, topN_tri) if n_horses >= 1 else []\n",
    "\n",
    "            actions_by_ticket['tansho'][rid] = {'tansho': top2}\n",
    "            actions_by_ticket['fukusho'][rid] = {'fukusho': top3}\n",
    "            actions_by_ticket['umaren'][rid] = {'umaren': top4 if len(top4) >= 2 else []}\n",
    "            actions_by_ticket['umatan'][rid] = {'umatan': top4 if len(top4) >= 2 else []}\n",
    "            actions_by_ticket['wide'][rid] = {'wide': top4 if len(top4) >= 2 else []}\n",
    "            actions_by_ticket['sanrenpuku'][rid] = {'sanrenpuku': top_tri if len(top_tri) >= 3 else []}\n",
    "            actions_by_ticket['sanrentan'][rid] = {'sanrentan': top_tri if len(top_tri) >= 3 else []}\n",
    "        return actions_by_ticket\n",
    "\n",
    "actions_by_ticket_20251220 = build_actions_by_ticket(score_table_20251220)\n",
    "print('tickets:', list(actions_by_ticket_20251220.keys()))\n",
    "print('races in actions:', len(actions_by_ticket_20251220['tansho']))\n",
    "\n",
    "rows = []\n",
    "detail_by_ticket_20251220 = {}\n",
    "for ticket, actions_ticket in actions_by_ticket_20251220.items():\n",
    "    returns_per_race = simulator.calc_returns_per_race(actions_ticket)\n",
    "    returns = simulator.calc_returns(actions_ticket)\n",
    "    detail_by_ticket_20251220[ticket] = returns_per_race.sort_index()\n",
    "    skipped_races = len(actions_ticket) - returns_per_race.index.nunique()\n",
    "    rows.append({\n",
    "        'ticket': ticket,\n",
    "        'n_races_target': len(actions_ticket),\n",
    "        'n_races_in_return_tables': returns_per_race.index.nunique(),\n",
    "        'n_races_skipped': skipped_races,\n",
    "        **returns,\n",
    "    })\n",
    "\n",
    "summary_20251220 = pd.DataFrame(rows).sort_values('ticket').reset_index(drop=True)\n",
    "display(summary_20251220)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5) 払戻（return_tables）欠損の確認＆必要なら補完 ---\n",
    "import pandas as pd\n",
    "\n",
    "from modules.constants import LocalPaths\n",
    "from modules.preparing._scrape_html import scrape_html_race\n",
    "from modules.preparing._get_rawdata import get_rawdata_return, update_rawdata\n",
    "from modules.preprocessing._return_processor import ReturnProcessor\n",
    "from modules.simulation._simulator import Simulator\n",
    "\n",
    "raw_return_tables = return_processor.raw_data\n",
    "if getattr(raw_return_tables.index, 'nlevels', 1) > 1:\n",
    "    existing_race_ids = set(raw_return_tables.index.get_level_values(0).astype(str))\n",
    "else:\n",
    "    existing_race_ids = set(raw_return_tables.index.astype(str))\n",
    "\n",
    "missing_race_ids_20251220 = sorted(set(map(str, race_id_list_20251220)) - existing_race_ids)\n",
    "print(f'missing race_id in return_tables (20251220): {len(missing_race_ids_20251220)}')\n",
    "if len(missing_race_ids_20251220) > 0:\n",
    "    display(pd.Series(missing_race_ids_20251220, name='missing_race_id').head(20))\n",
    "\n",
    "    updated_html_paths = scrape_html_race(missing_race_ids_20251220, skip=False)\n",
    "    new_return_df = get_rawdata_return(updated_html_paths)\n",
    "    _ = update_rawdata(LocalPaths.RAW_RETURN_TABLES_PATH, new_return_df, mode='update')\n",
    "\n",
    "    return_processor = ReturnProcessor(LocalPaths.RAW_RETURN_TABLES_PATH)\n",
    "    simulator = Simulator(return_processor)\n",
    "\n",
    "    print('return_tables updated. 必要なら、上の回収率集計セルを再実行してください。')\n",
    "else:\n",
    "    print('欠損はありません（このまま回収率集計結果を採用できます）。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature diff ===\n",
      "train cols: 276\n",
      "inference cols: 273\n",
      "missing_in_inference (train - inference): 5\n",
      "extra_in_inference (inference - train): 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_in_inference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jockey_plc_rate_10_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jockey_rides_10_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jockey_plc_rate_50_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jockey_rides_50_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jockey_has_history_flag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      missing_in_inference\n",
       "0   jockey_plc_rate_10_all\n",
       "1      jockey_rides_10_all\n",
       "2   jockey_plc_rate_50_all\n",
       "3      jockey_rides_50_all\n",
       "4  jockey_has_history_flag"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extra_in_inference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>馬番</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>単勝</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  extra_in_inference\n",
       "0                 馬番\n",
       "1                 単勝"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model feature names ===\n",
      "feature_name_ exists: True len: 276\n",
      "feature_name_ head: ['Column_0', 'Column_1', 'Column_2', 'Column_3', 'Column_4', 'Column_5', 'Column_6', 'Column_7', 'Column_8', 'Column_9']\n",
      "booster_.feature_name() exists: True len: 276\n",
      "booster feature_name head: ['Column_0', 'Column_1', 'Column_2', 'Column_3', 'Column_4', 'Column_5', 'Column_6', 'Column_7', 'Column_8', 'Column_9']\n",
      "\n",
      "[NOTE] 欠けている列は推論時に0埋めして整列可能です。\n"
     ]
    }
   ],
   "source": [
    "# --- 診断: 学習時特徴量(train_cols) vs 推論特徴量(X_debug) の差分一覧 ---\n",
    "import pandas as pd\n",
    "from modules.constants import ResultsCols\n",
    "\n",
    "# 比較対象のXを決める（基本は X_debug）\n",
    "if 'X_debug' in globals():\n",
    "    X_cmp = X_debug.copy()\n",
    "elif 'X_shutuba' in globals():\n",
    "    X_cmp = X_shutuba.copy()\n",
    "else:\n",
    "    raise NameError('X_debug も X_shutuba も見つかりません')\n",
    "\n",
    "# 学習時の列（keiba_aiが持つデータセット由来）\n",
    "train_cols = list(keiba_ai.datasets.X_train.columns)\n",
    "x_cols = list(X_cmp.columns)\n",
    "\n",
    "missing_in_inference = [c for c in train_cols if c not in x_cols]\n",
    "extra_in_inference = [c for c in x_cols if c not in train_cols]\n",
    "\n",
    "print('=== Feature diff ===')\n",
    "print('train cols:', len(train_cols))\n",
    "print('inference cols:', len(x_cols))\n",
    "print('missing_in_inference (train - inference):', len(missing_in_inference))\n",
    "print('extra_in_inference (inference - train):', len(extra_in_inference))\n",
    "\n",
    "if len(missing_in_inference) > 0:\n",
    "    display(pd.DataFrame({'missing_in_inference': missing_in_inference}))\n",
    "if len(extra_in_inference) > 0:\n",
    "    display(pd.DataFrame({'extra_in_inference': extra_in_inference}))\n",
    "\n",
    "# 参考: LightGBMモデルが保持しているfeature名の状態\n",
    "if 'model' not in globals():\n",
    "    try:\n",
    "        model = getattr(keiba_ai, '_KeibaAI__model_wrapper').lgb_model\n",
    "    except Exception:\n",
    "        model = None\n",
    "if model is not None:\n",
    "    fn_attr = getattr(model, 'feature_name_', None)\n",
    "    fn_booster = None\n",
    "    try:\n",
    "        fn_booster = model.booster_.feature_name()\n",
    "    except Exception:\n",
    "        pass\n",
    "    print('\\n=== Model feature names ===')\n",
    "    print('feature_name_ exists:', fn_attr is not None, 'len:', (len(fn_attr) if fn_attr is not None else None))\n",
    "    if fn_attr is not None:\n",
    "        print('feature_name_ head:', list(fn_attr)[:10])\n",
    "    print('booster_.feature_name() exists:', fn_booster is not None, 'len:', (len(fn_booster) if fn_booster is not None else None))\n",
    "    if fn_booster is not None:\n",
    "        print('booster feature_name head:', list(fn_booster)[:10])\n",
    "\n",
    "# 参考: 推論側で欠けている列が全て0埋めで問題ないかざっくり確認（存在する列のみ）\n",
    "if len(missing_in_inference) > 0:\n",
    "    print('\\n[NOTE] 欠けている列は推論時に0埋めして整列可能です。')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
