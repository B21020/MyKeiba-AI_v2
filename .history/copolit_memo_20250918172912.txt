## 使い方
horse_ids = ["2012100683", "2013101904"]
paths = save_horse_page_with_results_bin(horse_ids)
print(paths)

## 実装コード本体＋AJAX断片をマージして .bin 保存）
```
import os, time, random, re, json
import requests
from bs4 import BeautifulSoup

def _build_session():
    s = requests.Session()
    s.headers.update({
        "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                       "AppleWebKit/537.36 (KHTML, like Gecko) "
                       "Chrome/122.0 Safari/537.36"),
        "Accept-Language": "ja,en;q=0.9",
    })
    return s

def _decode_best(bytes_body, enc_candidates=("euc-jp","cp932","utf-8")):
    for enc in enc_candidates:
        try:
            return bytes_body.decode(enc), enc
        except Exception:
            pass
    # 最後はignoreで
    return bytes_body.decode(enc_candidates[-1], errors="ignore"), enc_candidates[-1]

def _merge_results_into_base_html(base_html_text: str, results_fragment_html: str) -> str:
    """
    base_html_text（EUC-JPのことが多い）に、results_fragment_html（UTF-8想定）を挿入。
    #horse_results_box があれば置換、無ければ末尾にセクションを追加。
    返り値はUTF-8の文字列（保存時に .encode('utf-8') する）。
    """
    soup = BeautifulSoup(base_html_text, "lxml")

    target = soup.select_one("#horse_results_box")
    frag_soup = BeautifulSoup(results_fragment_html, "lxml")

    if target:
        # 既存コンテナの中身を差し替え
        target.clear()
        for child in frag_soup.contents:
            target.append(child)
    else:
        # 念のため末尾に追加（構造変更時の保険）
        wrap = soup.new_tag("div", id="horse_results_box")
        for child in frag_soup.contents:
            wrap.append(child)
        if soup.body:
            soup.body.append(wrap)
        else:
            soup.append(wrap)

    # meta charset をUTF-8に差し替え（後工程が扱いやすいように）
    head = soup.head or soup.new_tag("head")
    soup.html.insert(0, head) if not soup.head else None
    # 既存のcontent-typeを削除
    for m in soup.find_all("meta", attrs={"http-equiv": re.compile("^content-type$", re.I)}):
        m.decompose()
    meta = soup.new_tag("meta")
    meta.attrs["charset"] = "utf-8"
    head.insert(0, meta)

    return str(soup)

def save_horse_page_with_results_bin(horse_id_list, out_dir="data/html/horse", skip=True,
                                     sleep_sec=2.0, max_retries=3, backoff=1.5):
    """
    1) 本体HTML（馬ページ）を取得（EUC-JPなど）
    2) AJAX（/horse/ajax_horse_results.html?id=...）で過去成績のHTML断片を取得
    3) 断片を本体に挿入してUTF-8で .bin 保存
    """
    os.makedirs(out_dir, exist_ok=True)
    s = _build_session()
    saved = []

    for horse_id in horse_id_list:
        path = os.path.join(out_dir, f"{horse_id}.bin")
        if skip and os.path.exists(path):
            print(f"horse_id {horse_id} skipped (exists)")
            continue

        # --- 1) 本体HTML ---
        base_url = f"https://db.netkeiba.com/horse/{horse_id}/"
        try:
            r = s.get(base_url, timeout=20)
            r.raise_for_status()
        except Exception as e:
            print(f"[ERROR] base GET {horse_id}: {e}")
            continue

        base_text, used_enc = _decode_best(r.content)

        # --- 2) AJAX（過去成績） ---
        ajax_url = "https://db.netkeiba.com/horse/ajax_horse_results.html"
        params = {"id": horse_id, "input": "UTF-8", "output": "json"}
        headers = {"Referer": base_url}
        frag_html = ""
        ok = False
        for attempt in range(1, max_retries+1):
            try:
                rr = s.get(ajax_url, params=params, headers=headers, timeout=20)
                rr.raise_for_status()
                js = rr.json()
                if js.get("status") == "OK":
                    frag_html = js.get("data", "")
                    ok = True
                    break
            except Exception as e:
                if attempt == max_retries:
                    print(f"[ERROR] ajax GET {horse_id} attempt={attempt}: {e}")
                time.sleep(backoff ** attempt)

        if not ok or not frag_html:
            # 成績が無い or 取得失敗 → 本体のみUTF-8で保存（ログは出す）
            print(f"[WARN] results fragment missing for horse_id {horse_id}; saving base only.")
            merged = _merge_results_into_base_html(base_text, "")  # 空でもコンテナは整える
        else:
            merged = _merge_results_into_base_html(base_text, frag_html)

        # --- 3) 保存（UTF-8バイト） ---
        with open(path, "wb") as f:
            f.write(merged.encode("utf-8", errors="ignore"))

        saved.append(path)
        time.sleep(sleep_sec + random.random())
    return saved
```

## 要件
・上記コードをもとに_scrape_html.pyを、馬のデータをスクレイピングする処理のところで、AJAXを直たたきしてスクレイピングする方針に切り替えてください
・mainでの実行を一貫させるためにmainでpreparing.scrape_html_horse_with_masterを実行→preparing.scrape_html_horse_with_master内でpreparing.scrape_html_horseを実行してスクレイピングする構造を維持してください。
・目標は過去成績が含まれる馬のHTMLファイルをbin形式で保存することです。