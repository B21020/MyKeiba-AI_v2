# -*- coding: utf-8 -*-
import datetime
import re
import pandas as pd
import time
import os
from tqdm.auto import tqdm
from urllib.request import urlopen, Request
from bs4 import BeautifulSoup
import random
import requests
import json
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait


from modules.constants import UrlPaths, LocalPaths

# è¿½åŠ ï¼šUser-Agentä¸€è¦§
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:115.0) Gecko/20100101 Firefox/115.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:115.0) Gecko/20100101 Firefox/115.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Edg/115.0.0.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 OPR/85.0.4341.72",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 OPR/85.0.4341.72",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Vivaldi/5.3.2679.55",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Vivaldi/5.3.2679.55",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Brave/1.40.107",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Brave/1.40.107",
]

def _build_driver(headless: bool = True, user_agent: str | None = None):
    """Selenium WebDriver æ§‹ç¯‰ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼"""
    opts = Options()
    if headless:
        # Chrome 109 ä»¥é™æ¨å¥¨ã®æ–°ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹
        opts.add_argument("--headless=new")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--window-size=1280,2000")
    if user_agent:
        opts.add_argument(f"--user-agent={user_agent}")
    # è¿½åŠ ã§å®‰å®šåŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    opts.add_argument("--disable-blink-features=AutomationControlled")
    return webdriver.Chrome(options=opts)


def scrape_html_race(
    race_id_list: list,
    skip: bool = True,
    headless: bool = True,
    wait_sec: int = 20,
    per_request_sleep: tuple[float, float] = (1.5, 2.0),
):
    """Selenium ã§ JS å®Ÿè¡Œå¾Œã®ãƒ¬ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ HTML ã‚’å–å¾—ã—ä¿å­˜ã™ã‚‹ã€‚

    å¼•æ•°:
        race_id_list: å–å¾—å¯¾è±¡ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆ (æ–‡å­—åˆ—)
        skip: True ã®å ´åˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã¯å†å–å¾—ã—ãªã„
        headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•ã™ã‚‹ã‹
        wait_sec: å„ãƒšãƒ¼ã‚¸ã§ã®æœ€å¤§å¾…æ©Ÿç§’æ•°
        per_request_sleep: (min,max) ãƒ©ãƒ³ãƒ€ãƒ ã‚¹ãƒªãƒ¼ãƒ—ç§’ãƒ¬ãƒ³ã‚¸

    æˆ»ã‚Šå€¤:
        ä¿å­˜ (æ–°è¦) ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
    """
    os.makedirs(LocalPaths.HTML_RACE_DIR, exist_ok=True)
    updated_html_path_list: list[str] = []

    driver = _build_driver(headless=headless, user_agent=random.choice(USER_AGENTS))

    def _tables_populated(d):
        # ä»£è¡¨çš„ãªçµæœãƒ†ãƒ¼ãƒ–ãƒ« / æ‰•æˆ»ãƒ†ãƒ¼ãƒ–ãƒ«ã® td ãŒ 1 å€‹ä»¥ä¸Šå‡ºç¾ã™ã‚‹ã®ã‚’å¾…æ©Ÿæ¡ä»¶ã«ã™ã‚‹
        return d.execute_script(
            """
            const sel = [
              '.result_info .result_table_02 td',
              '.result_info .race_table_01 td',
              '.PayBack_Table td'
            ];
            return sel.some(s => document.querySelectorAll(s).length >= 1);
            """
        ) is True

    try:
        for race_id in tqdm(race_id_list, desc="race HTML (selenium)"):
            filename = os.path.join(LocalPaths.HTML_RACE_DIR, f"{race_id}.bin")
            if skip and os.path.isfile(filename):
                print(f"{race_id} skipped (exists)")
                continue

            url = UrlPaths.RACE_URL + race_id + "/"
            try:
                driver.get(url)
            except Exception as e:
                print(f"{race_id} driver.get error: {e}")
                continue

            # DOM å®Œå…¨ãƒ­ãƒ¼ãƒ‰å¾…æ©Ÿ
            try:
                WebDriverWait(driver, wait_sec).until(
                    lambda d: d.execute_script("return document.readyState") == "complete"
                )
            except Exception:
                print(f"{race_id} page readyState timeout")

            # ãƒ†ãƒ¼ãƒ–ãƒ« or æ‰•æˆ»ãƒ†ãƒ¼ãƒ–ãƒ«å‡ºç¾å¾…æ©Ÿ
            try:
                WebDriverWait(driver, wait_sec).until(_tables_populated)
            except Exception:
                # å¤šå°‘é…å»¶ã®ä½™åœ°
                time.sleep(3)

            # æœ‰åŠ¹ãƒšãƒ¼ã‚¸ç¢ºèª (data_intro ãŒç„¡ã„=å­˜åœ¨ã—ãªã„/æº–å‚™ä¸­ãƒšãƒ¼ã‚¸æƒ³å®š)
            try:
                valid = driver.execute_script(
                    "return !!document.querySelector('div.data_intro');"
                )
            except Exception:
                valid = False

            if not valid:
                print(f"{race_id} invalid (no data_intro)")
                continue

            # HTML ä¿å­˜
            try:
                html = driver.page_source
                with open(filename, "wb") as f:
                    f.write(html.encode("utf-8", errors="ignore"))
                updated_html_path_list.append(filename)
            except Exception as e:
                print(f"{race_id} save error: {e}")
                continue

            # ã‚¢ã‚¯ã‚»ã‚¹é–“éš” (è² è·/ãƒ–ãƒ­ãƒƒã‚¯å›é¿)
            sleep_time = random.uniform(*per_request_sleep)
            time.sleep(sleep_time)

    finally:
        try:
            driver.quit()
        except Exception:
            pass

    return updated_html_path_list

def scrape_html_horse(horse_id_list: list, skip: bool = True):
    """
    netkeiba.comã®horseãƒšãƒ¼ã‚¸ã®htmlã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦data/html/horseã«ä¿å­˜ã™ã‚‹é–¢æ•°ã€‚
    skip=Trueã«ã™ã‚‹ã¨ã€ã™ã§ã«htmlãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã€Falseã«ã™ã‚‹ã¨ä¸Šæ›¸ãã•ã‚Œã‚‹ã€‚
    è¿”ã‚Šå€¤ï¼šæ–°ã—ãã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸhtmlã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    updated_html_path_list = []
    for horse_id in tqdm(horse_id_list):
        # ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å
        filename = os.path.join(LocalPaths.HTML_HORSE_DIR, horse_id+'.bin')
        # skipãŒTrueã§ã€ã‹ã¤binãƒ•ã‚¡ã‚¤ãƒ«ãŒã™ã§ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯é£›ã°ã™
        if skip and os.path.isfile(filename):
            print('horse_id {} skipped'.format(horse_id))
        else:
            # horse_idã‹ã‚‰urlã‚’ä½œã‚‹
            url = UrlPaths.HORSE_URL + horse_id
            time.sleep(2)
            agent = random.choice(USER_AGENTS)
            req = Request(url, headers={'User-Agent': agent})
            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
            html = urlopen(req).read()
            # ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š
            with open(filename, 'wb') as f:
                # ä¿å­˜
                f.write(html)
            updated_html_path_list.append(filename)
    return updated_html_path_list

def fetch_horse_results_html(horse_id: str, retries: int = 3, backoff: float = 1.0) -> str | None:
    """
    AJAX ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ç«¶èµ°æˆç¸¾ã® HTML æ–­ç‰‡ã‚’å–å¾—ã™ã‚‹ã€‚
    æˆåŠŸã™ã‚‹ã¨ HTML éƒ¨åˆ†æ–‡å­—åˆ—ã‚’è¿”ã—ã€å¤±æ•—ã™ã‚‹ã¨ None ã‚’è¿”ã™ã€‚
    """
    url = "https://db.netkeiba.com/horse/ajax_horse_results.html"
    params = {
        "input": "UTF-8",
        "output": "json",
        "id": horse_id,
    }
    headers = {
        "User-Agent": random.choice(USER_AGENTS),
        "Referer": f"https://db.netkeiba.com/horse/{horse_id}/",  # å‚ç…§å…ƒã‚’ä»˜ã‘ã¦ãŠãã¨å®‰å®šã—ã‚„ã™ã„
    }

    for attempt in range(1, retries + 1):
        try:
            resp = requests.get(url, params=params, headers=headers, timeout=10)
            # æˆåŠŸã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèª
            if resp.status_code != 200:
                print(f"[{horse_id}] HTTP {resp.status_code} (attempt {attempt})")
                time.sleep(backoff * attempt)
                continue

            # JSON ã‚’ãƒ‘ãƒ¼ã‚¹
            json_data = resp.json()
            if json_data.get("status") != "OK":
                print(f"[{horse_id}] ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ OK ã§ãªã„: {json_data.get('status')} (attempt {attempt})")
                time.sleep(backoff * attempt)
                continue

            # æˆç¸¾ HTML æ–­ç‰‡ã‚’å–å¾—ï¼ˆå†…éƒ¨ã¯å¤šãã®å ´åˆ EUC-JP ã®ãƒ†ã‚­ã‚¹ãƒˆã ãŒ JSON ãŒ UTF-8 ã§æ¥ã‚‹ï¼‰
            fragment = json_data.get("data", "")
            if not fragment:
                print(f"[{horse_id}] data field ãŒç©ºã§ã™ (attempt {attempt})")
                time.sleep(backoff * attempt)
                continue

            return fragment  # ã“ã“ã‚’ BeautifulSoup ã«æµã™ãªã©æ¬¡ã®å‡¦ç†ã¸

        except Exception as e:
            print(f"[{horse_id}] ä¾‹å¤–: {e} (attempt {attempt})")
            time.sleep(backoff * attempt)

    print(f"[{horse_id}] ç«¶èµ°æˆç¸¾ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    return None

def parse_horse_results_deprecated(horse_id: str):
    """
    é¦¬IDã‹ã‚‰ç«¶èµ°æˆç¸¾ã‚’DataFrameå½¢å¼ã§å–å¾—ã™ã‚‹ã€‚ï¼ˆéæ¨å¥¨ï¼šparse_html_fragment_to_dataframeã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼‰
    """
    html_fragment = fetch_horse_results_html(horse_id)
    if html_fragment is None:
        return None  # å–å¾—å¤±æ•—

    soup = BeautifulSoup(html_fragment, "html.parser")
    
    # æˆç¸¾ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™ï¼ˆè¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™ï¼‰
    table_selectors = [
        "table.db_h_race_results",
        "table[summary*='ãƒ¬ãƒ¼ã‚¹çµæœ']",
        "table[summary*='å‡ºèµ°å±¥æ­´']",
        "table.race_table",
        "table"  # æœ€å¾Œã®æ‰‹æ®µ
    ]
    
    table = None
    for selector in table_selectors:
        table = soup.select_one(selector)
        if table:
            break
    
    if not table:
        print(f"[{horse_id}] æˆç¸¾ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    try:
        # pandas ã«å¤‰æ›
        df = pd.read_html(str(table))[0]
        df["horse_id"] = horse_id
        return df
    except Exception as e:
        print(f"[{horse_id}] ãƒ†ãƒ¼ãƒ–ãƒ«è§£æã‚¨ãƒ©ãƒ¼: {e}")
        return None

def scrape_html_horse_results_ajax(horse_id_list: list, skip: bool = True):
    """
    AJAXã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦é¦¬ã®ç«¶èµ°æˆç¸¾HTMLã‚’å–å¾—ã—ã€binãƒ•ã‚¡ã‚¤ãƒ«ã§ä¿å­˜ã—ã¦DataFrameã¨ã—ã¦è¿”ã™ã€‚
    skip=Trueã«ã™ã‚‹ã¨ã€ã™ã§ã«HTMLãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã€Falseã«ã™ã‚‹ã¨ä¸Šæ›¸ãã•ã‚Œã‚‹ã€‚
    """
    print('fetching horse results via AJAX and saving HTML as bin files')
    horse_results = {}
    saved_html_files = []
    
    for horse_id in tqdm(horse_id_list):
        # æˆç¸¾HTMLã¯ horse_results ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ï¼ˆãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã¨åˆ†é›¢ï¼‰
        os.makedirs(LocalPaths.HTML_HORSE_RESULTS_DIR, exist_ok=True)
        filename = os.path.join(LocalPaths.HTML_HORSE_RESULTS_DIR, f"{horse_id}.bin")
        
        # skipãŒTrueã§ã€ã‹ã¤binãƒ•ã‚¡ã‚¤ãƒ«ãŒã™ã§ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        if skip and os.path.isfile(filename):
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç©ºãƒ•ã‚¡ã‚¤ãƒ«ã§ãªã„ã‹ç¢ºèªï¼‰
                if os.path.getsize(filename) > 0:
                    with open(filename, 'r', encoding='utf-8') as f:
                        html_fragment = f.read()
                    # æ—¢å­˜HTMLã‹ã‚‰DataFrameã‚’ä½œæˆ
                    df = parse_html_fragment_to_dataframe(horse_id, html_fragment)
                    if df is not None and not df.empty:
                        horse_results[horse_id] = df
                        print(f'horse_id {horse_id} loaded from existing file')
                        continue
                    else:
                        print(f'horse_id {horse_id} existing file has no valid data, re-fetching')
                else:
                    print(f'horse_id {horse_id} existing file is empty, re-fetching')
            except Exception as e:
                print(f'Error loading existing file for {horse_id}: {e}, re-fetching')
                # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸå ´åˆã¯æ–°è¦å–å¾—ã™ã‚‹
        
        try:
            # AJAXã‹ã‚‰HTMLã‚’å–å¾—
            html_fragment = fetch_horse_results_html(horse_id)
            if html_fragment:
                # HTMLã‚’binãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(html_fragment)
                saved_html_files.append(filename)
                
                # DataFrameã‚’ä½œæˆ
                df = parse_html_fragment_to_dataframe(horse_id, html_fragment)
                if df is not None and not df.empty:
                    horse_results[horse_id] = df
                    print(f'horse_id {horse_id} HTML saved and processed')
                else:
                    print(f'horse_results empty for {horse_id}')
                
                # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆ1.5ï½2.0ç§’ï¼‰
                sleep_time = random.uniform(1.5, 2.0)
                time.sleep(sleep_time)
            else:
                print(f'Failed to fetch HTML for {horse_id}')
        except Exception as e:
            print(f'Error processing {horse_id}: {e}')
            continue

    # pd.DataFrameå‹ã«ã—ã¦ä¸€ã¤ã®ãƒ‡ãƒ¼ã‚¿ã«ã¾ã¨ã‚ã‚‹
    if not horse_results:
        print('è­¦å‘Š: ã™ã¹ã¦ã®é¦¬ã®æˆç¸¾ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ç©ºã®DataFrameã‚’è¿”ã—ã¾ã™ã€‚')
        return pd.DataFrame()
    
    horse_results_df = pd.concat([horse_results[key] for key in horse_results])

    # åˆ—åã«åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ãŒã‚ã‚Œã°é™¤å»ã™ã‚‹
    horse_results_df = horse_results_df.rename(columns=lambda x: x.replace(' ', ''))

    print(f'ä¿å­˜ã•ã‚ŒãŸHTMLãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(saved_html_files)}')
    return horse_results_df

def parse_html_fragment_to_dataframe(horse_id: str, html_fragment: str):
    """
    HTMLãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰DataFrameã‚’ä½œæˆã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    """
    if html_fragment is None:
        return None

    soup = BeautifulSoup(html_fragment, "html.parser")
    
    # æˆç¸¾ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™ï¼ˆè¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™ï¼‰
    table_selectors = [
        "table.db_h_race_results",
        "table[summary*='ãƒ¬ãƒ¼ã‚¹çµæœ']",
        "table[summary*='å‡ºèµ°å±¥æ­´']",
        "table.race_table",
        "table"  # æœ€å¾Œã®æ‰‹æ®µ
    ]
    
    table = None
    for selector in table_selectors:
        table = soup.select_one(selector)
        if table:
            break
    
    if not table:
        print(f"[{horse_id}] æˆç¸¾ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    try:
        # pandas ã«å¤‰æ›
        df = pd.read_html(str(table))[0]
        df["horse_id"] = horse_id
        return df
    except Exception as e:
        print(f"[{horse_id}] ãƒ†ãƒ¼ãƒ–ãƒ«è§£æã‚¨ãƒ©ãƒ¼: {e}")
        return None

def scrape_html_ped(horse_id_list: list, skip: bool = False):
    """
    netkeiba.comã®horse/pedãƒšãƒ¼ã‚¸ã®htmlã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦data/html/pedã«ä¿å­˜ã™ã‚‹é–¢æ•°ã€‚
    skip=Trueã«ã™ã‚‹ã¨ã€ã™ã§ã«htmlãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã€Falseã«ã™ã‚‹ã¨ä¸Šæ›¸ãã•ã‚Œã‚‹ã€‚
    è¿”ã‚Šå€¤ï¼šæ–°ã—ãã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸhtmlã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    updated_html_path_list = []
    for horse_id in tqdm(horse_id_list):
        # ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å
        filename = os.path.join(LocalPaths.HTML_PED_DIR, horse_id+'.bin')
        # skipãŒTrueã§ã€ã‹ã¤binãƒ•ã‚¡ã‚¤ãƒ«ãŒã™ã§ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯é£›ã°ã™
        if skip and os.path.isfile(filename):
            print('horse_id {} skipped'.format(horse_id))
        else:
            # horse_idã‹ã‚‰urlã‚’ä½œã‚‹
            url = UrlPaths.PED_URL + horse_id
            # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆ1.0ï½2.0ç§’ï¼‰
            sleep_time = random.uniform(1.0, 2.0)
            time.sleep(sleep_time)
            agent = random.choice(USER_AGENTS)
            req = Request(url, headers={'User-Agent': agent})
            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
            html = urlopen(req).read()
            # ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š
            with open(filename, 'wb') as f:
                # ä¿å­˜
                f.write(html)
            updated_html_path_list.append(filename)
    return updated_html_path_list

def scrape_html_horse_with_master(horse_id_list: list, skip: bool = True):
    """
    netkeiba.comã®horseãƒšãƒ¼ã‚¸ã®htmlã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦data/html/horseã«ä¿å­˜ã™ã‚‹é–¢æ•°ã€‚
    skip=Trueã«ã™ã‚‹ã¨ã€ã™ã§ã«htmlãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã€Falseã«ã™ã‚‹ã¨ä¸Šæ›¸ãã•ã‚Œã‚‹ã€‚
    è¿”ã‚Šå€¤ï¼šæ–°ã—ãã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸhtmlã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    ã¾ãŸã€horse_idã”ã¨ã«ã€æœ€å¾Œã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸæ—¥ä»˜ã‚’è¨˜éŒ²ã—ã€data/master/horse_results_updated_at.csvã«ä¿å­˜ã™ã‚‹ã€‚
    """
    ### ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œï¼ˆé™çš„HTMLï¼‰ ###
    print('scraping (static horse page)')
    updated_html_path_list = scrape_html_horse(horse_id_list, skip)

    # ãƒ‘ã‚¹ã‹ã‚‰å®‰å…¨ã«horse_idã‚’æŠ½å‡º
    horse_id_extracted = []
    for html_path in updated_html_path_list:
        base = os.path.basename(html_path)
        m = re.match(r'(\d+)\.bin$', base)
        if m:
            horse_id_extracted.append(m.group(1))
        else:
            print(f"WARNING: could not parse horse_id from path {html_path}")
    # DataFrameã«ã—ã¦ãŠã
    horse_id_df = pd.DataFrame({'horse_id': horse_id_extracted})

    ### è¿½åŠ : AJAXã§ç«¶èµ°æˆç¸¾ã‚‚å–å¾—ãƒ»ä¿å­˜ ###
    print('scraping (AJAX horse results)')
    from tqdm.auto import tqdm
    import random
    import time
    import requests
    # æ—¢å­˜ã®ç«¶èµ°æˆç¸¾ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã‘ã‚Œã°ä½œæˆ
    for horse_id in tqdm(horse_id_list, desc='AJAX horse results'):
        # æˆç¸¾ãƒ•ã‚¡ã‚¤ãƒ«ã¯ horse_results ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸
        os.makedirs(LocalPaths.HTML_HORSE_RESULTS_DIR, exist_ok=True)
        results_filename = os.path.join(LocalPaths.HTML_HORSE_RESULTS_DIR, f"{horse_id}.bin")
        # skipãŒTrueã§æ—¢ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
        if skip and os.path.isfile(results_filename):
            print(f'horse_id {horse_id} skipped (already exists)')
            continue
        # AJAXå–å¾—
        url = "https://db.netkeiba.com/horse/ajax_horse_results.html"
        params = {"input": "UTF-8", "output": "json", "id": horse_id}
        headers = {
            "User-Agent": random.choice(USER_AGENTS),
            "Referer": f"https://db.netkeiba.com/horse/{horse_id}/",
        }
        try:
            resp = requests.get(url, params=params, headers=headers, timeout=10)
            if resp.status_code == 200:
                # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰HTMLãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã¦ä¿å­˜
                json_data = resp.json()
                if json_data.get("status") == "OK":
                    html_fragment = json_data.get("data", "")
                    with open(results_filename, 'w', encoding='utf-8') as f:
                        f.write(html_fragment)
                    print(f'horse_id {horse_id} HTML saved via AJAX')
                else:
                    print(f"[AJAX] {horse_id} status not OK: {json_data.get('status')}")
            else:
                print(f"[AJAX] {horse_id} HTTP {resp.status_code}")
        except Exception as e:
            print(f"[AJAX] {horse_id} error: {e}")
        time.sleep(random.uniform(1.0, 2.0))

    ### å–å¾—æ—¥ãƒã‚¹ã‚¿ã®æ›´æ–° ###
    print('updating master')
    now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    if not os.path.isfile(LocalPaths.MASTER_RAW_HORSE_RESULTS_PATH):
        pd.DataFrame(columns=['horse_id', 'updated_at']).to_csv(LocalPaths.MASTER_RAW_HORSE_RESULTS_PATH, index=None)
    master = pd.read_csv(LocalPaths.MASTER_RAW_HORSE_RESULTS_PATH, dtype=object)
    new_master = master.merge(horse_id_df, on='horse_id', how='outer')
    new_master.loc[new_master['horse_id'].isin(horse_id_extracted), 'updated_at'] = now
    new_master[['horse_id', 'updated_at']].to_csv(LocalPaths.MASTER_RAW_HORSE_RESULTS_PATH, index=None)
    return updated_html_path_list
#TODO: scrape_html_horse_with_updated_atã®ãƒ†ã‚¹ãƒˆ

def fetch_pedigree_data_optimized(horse_id):
    """æœ€é©åŒ–ã•ã‚ŒãŸè¡€çµ±ãƒ‡ãƒ¼ã‚¿å–å¾—"""
    
    # èª¿æŸ»çµæœã§æœ€ã‚‚ç¢ºå®Ÿã ã£ãŸã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    url = f"https://db.netkeiba.com/horse/ped/{horse_id}/"
    
    headers = {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
    }
    
    try:
        time.sleep(0.8)  # é©åº¦ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            content = response.content.decode('euc-jp', errors='ignore')
            return content
        else:
            return None
            
    except Exception as e:
        return None

def parse_pedigree_comprehensive(html_content):
    """è¡€çµ±HTMLã‹ã‚‰åŒ…æ‹¬çš„ã«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    
    if not html_content:
        return {}
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        pedigree_data = {}
        
        # è¡€çµ±ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¤œç´¢
        blood_table = soup.find('table', class_='blood_table')
        
        if blood_table:
            # è¡€çµ±è¡¨ã®æ§‹é€ ã‚’è§£æ
            rows = blood_table.find_all('tr')
            
            for i, row in enumerate(rows):
                cells = row.find_all(['td', 'th'])
                
                # ã‚»ãƒ«ã®å†…å®¹ã‚’æŠ½å‡º
                cell_data = []
                for cell in cells:
                    # ãƒªãƒ³ã‚¯æƒ…å ±ã‚‚ä¿æŒ
                    links = cell.find_all('a', href=True)
                    cell_info = {
                        'text': cell.get_text(strip=True),
                        'links': []
                    }
                    
                    for link in links:
                        href = link.get('href', '')
                        if '/horse/' in href:
                            # é¦¬IDã‚’æŠ½å‡º
                            horse_id_match = href.split('/horse/')[-1].split('/')[0]
                            if horse_id_match.isdigit():
                                cell_info['links'].append({
                                    'name': link.get_text(strip=True),
                                    'horse_id': horse_id_match,
                                    'url': href
                                })
                    
                    cell_data.append(cell_info)
                
                if cell_data:
                    pedigree_data[f'row_{i}'] = cell_data
        
        # åŸºæœ¬æƒ…å ±ã‚‚æŠ½å‡º
        horse_title = soup.find('h1')
        if horse_title:
            pedigree_data['horse_name'] = horse_title.get_text(strip=True)
        
        # è¡€çµ±æƒ…å ±ã®ã‚µãƒãƒªãƒ¼ä½œæˆ
        summary = {
            'total_rows': len([k for k in pedigree_data.keys() if k.startswith('row_')]),
            'total_horses': 0,
            'generation_depth': 0
        }
        
        # é–¢é€£é¦¬ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        for key, value in pedigree_data.items():
            if key.startswith('row_'):
                for cell in value:
                    summary['total_horses'] += len(cell['links'])
        
        pedigree_data['summary'] = summary
        
        return pedigree_data
        
    except Exception as e:
        print(f"âš ï¸  è¡€çµ±è§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {}

def scrape_pedigree_batch_optimized(horse_ids, max_horses=None, show_progress=True, skip_existing=True):
    """æœ€é©åŒ–ã•ã‚ŒãŸè¡€çµ±ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å–å¾—"""
    
    print(f"ğŸ“Š å¯¾è±¡é¦¬æ•°: {len(horse_ids)} é ­")
    
    # æ—¢å­˜ã®è¡€çµ±ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
    if skip_existing:
        print("ğŸ” æ—¢å­˜ã®è¡€çµ±ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")
        existing_horse_ids = set()
        missing_horse_ids = []
        
        for horse_id in horse_ids:
            ped_file_path = os.path.join(LocalPaths.HTML_PED_DIR, f"{horse_id}.bin")
            if os.path.exists(ped_file_path):
                existing_horse_ids.add(horse_id)
            else:
                missing_horse_ids.append(horse_id)
        
        print(f"âœ… æ—¢å­˜è¡€çµ±ãƒ•ã‚¡ã‚¤ãƒ«: {len(existing_horse_ids)} é ­")
        print(f"âŒ ä¸è¶³ã—ã¦ã„ã‚‹è¡€çµ±ãƒ•ã‚¡ã‚¤ãƒ«: {len(missing_horse_ids)} é ­")
        
        # ä¸è¶³ã—ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
        horse_ids = missing_horse_ids
        
        if len(horse_ids) == 0:
            print("ğŸ‰ ã™ã¹ã¦ã®è¡€çµ±ãƒ‡ãƒ¼ã‚¿ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™ï¼")
            return {}
    
    if max_horses:
        horse_ids = horse_ids[:max_horses]
        print(f"ğŸ¯ å®Ÿè¡Œå¯¾è±¡: {len(horse_ids)} é ­ï¼ˆåˆ¶é™é©ç”¨ï¼‰")
    
    all_pedigree_data = {}
    success_count = 0
    error_count = 0
    
    # tqdmã§ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’è¡¨ç¤º
    progress_bar = tqdm(horse_ids, desc="ğŸ§¬ è¡€çµ±ãƒ‡ãƒ¼ã‚¿å–å¾—", unit="é ­") if show_progress else horse_ids

    for i, horse_id in enumerate(progress_bar, 1):
        # è¡€çµ±ãƒ‡ãƒ¼ã‚¿å–å¾—
        html_content = fetch_pedigree_data_optimized(horse_id)
        
        if html_content:
            # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆæ—¢å­˜ã®pedä¿å­˜å½¢å¼ã¨çµ±ä¸€ï¼‰
            ped_file_path = os.path.join(LocalPaths.HTML_PED_DIR, f"{horse_id}.bin")
            try:
                with open(ped_file_path, 'w', encoding='utf-8') as f:
                    f.write(html_content)
            except Exception as e:
                print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ {horse_id}: {e}")
            
            pedigree_data = parse_pedigree_comprehensive(html_content)
            
            if pedigree_data and 'summary' in pedigree_data:
                all_pedigree_data[horse_id] = pedigree_data
                success_count += 1
                
                # tqdmã®èª¬æ˜ã‚’å‹•çš„ã«æ›´æ–°
                if show_progress:
                    progress_bar.set_postfix({
                        'æˆåŠŸ': success_count,
                        'ã‚¨ãƒ©ãƒ¼': error_count,
                        'æˆåŠŸç‡': f"{success_count/(success_count+error_count)*100:.1f}%" if (success_count+error_count) > 0 else "0%"
                    })
                
                if show_progress and i % 100 == 0:
                    print(f"\n  âœ… æˆåŠŸ: {success_count} | âŒ ã‚¨ãƒ©ãƒ¼: {error_count}")
            else:
                error_count += 1
        else:
            error_count += 1
        
        # éåº¦ãªã‚¢ã‚¯ã‚»ã‚¹ã‚’é¿ã‘ã‚‹
        sleep_time = random.uniform(1.0, 2.0)
        time.sleep(sleep_time)
    
    print(f"\n\n=== ğŸ§¬ è¡€çµ±ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ===")
    print(f"âœ… æˆåŠŸ: {success_count} é ­")
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {error_count} é ­")
    if success_count + error_count > 0:
        print(f"ğŸ“Š æˆåŠŸç‡: {success_count/(success_count+error_count)*100:.1f}%")
    
    return all_pedigree_data

def normalize_numeric_id_from_path(path_or_id: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«å/IDæ–‡å­—åˆ—ã‹ã‚‰æ•°å­—ã ã‘æŠœãå‡ºã—ã¦è¿”ã™"""
    s = os.path.basename(str(path_or_id))
    m = re.search(r'(\d+)', s)
    return m.group(1) if m else s

def scrape_jockey_html(jockey_id_list: list, skip: bool = True):
    """
    é¨æ‰‹ãƒšãƒ¼ã‚¸ã®HTMLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆIDæ­£è¦åŒ–å¯¾å¿œç‰ˆï¼‰
    """
    print('scraping jockey HTML pages (improved)')
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèªãƒ»ä½œæˆ
    jockey_html_dir = os.path.join(LocalPaths.HTML_DIR, 'jockey')
    os.makedirs(jockey_html_dir, exist_ok=True)
    
    updated_html_path_list = []
    
    for jockey_id in tqdm(jockey_id_list, desc="é¨æ‰‹HTMLã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"):
        # IDã‚’æ­£è¦åŒ–
        jid = normalize_numeric_id_from_path(jockey_id)
        
        # ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        file_path = os.path.join(jockey_html_dir, f"jockey_{jid}.bin")
        
        # skipãƒã‚§ãƒƒã‚¯
        if skip and os.path.exists(file_path):
            updated_html_path_list.append(file_path)
            continue
        
        try:
            # é¨æ‰‹ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸URL
            url = f"https://db.netkeiba.com/jockey/{jid}/"
            
            # ãƒ©ãƒ³ãƒ€ãƒ ãªUser-Agent
            agent = random.choice(USER_AGENTS)
            headers = {
                'User-Agent': agent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
                'Connection': 'keep-alive',
            }
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # pickleã§ä¿å­˜ï¼ˆæ—¢å­˜å½¢å¼ã«åˆã‚ã›ã‚‹ï¼‰
            import pickle
            with open(file_path, 'wb') as f:
                pickle.dump(response.text, f)
            
            updated_html_path_list.append(file_path)
            
            # é©åº¦ãªé–“éš”
            sleep_time = random.uniform(1.5, 2.5)
            time.sleep(sleep_time)
            
        except Exception as e:
            print(f"âŒ é¨æ‰‹HTMLå–å¾—ã‚¨ãƒ©ãƒ¼ {jockey_id}: {e}")
            continue
    
    return updated_html_path_list

def scrape_jockey_result_html(jockey_id_list: list, skip: bool = True, wait_time: float = 1.0):
    """
    é¨æ‰‹ã®çµæœãƒšãƒ¼ã‚¸ (/jockey/result/{id}/) ã‚’ä¿å­˜ã—ã€ãƒ‘ã‚¹ã‚’è¿”ã™
    """
    print('scraping jockey result pages')
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèªãƒ»ä½œæˆ
    jockey_html_dir = os.path.join(LocalPaths.HTML_DIR, 'jockey')
    os.makedirs(jockey_html_dir, exist_ok=True)
    
    paths = []

    for raw_id in tqdm(jockey_id_list, desc="é¨æ‰‹RESULTãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"):
        # IDã‚’æ­£è¦åŒ–
        jid = normalize_numeric_id_from_path(raw_id)
        
        # ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        file_path = os.path.join(jockey_html_dir, f"jockey_result_{jid}.bin")
        
        # skipãƒã‚§ãƒƒã‚¯
        if skip and os.path.exists(file_path):
            paths.append(file_path)
            continue
        
        try:
            # é¨æ‰‹çµæœãƒšãƒ¼ã‚¸URL
            url = f"https://db.netkeiba.com/jockey/result/{jid}/"
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®š
            headers = {
                'User-Agent': random.choice(USER_AGENTS),
                'Referer': f"https://db.netkeiba.com/jockey/{jid}/",
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            }
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # pickleã§ä¿å­˜ï¼ˆæ—¢å­˜å½¢å¼ã«åˆã‚ã›ã‚‹ï¼‰
            import pickle
            with open(file_path, 'wb') as f:
                pickle.dump(response.text, f)
            
            paths.append(file_path)
            
            # é–“éš”èª¿æ•´
            time.sleep(wait_time + random.uniform(0, 0.5))
            
        except Exception as e:
            print(f"âŒ é¨æ‰‹çµæœãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼ {raw_id}: {e}")
            continue
    
    return paths